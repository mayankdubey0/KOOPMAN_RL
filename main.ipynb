{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16f8007",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Episode 1/1000 | Return: -6.82 | Avg(10): -6.82 | Length: 100\n",
      "Episode 2/1000 | Return: -100.67 | Avg(10): -53.75 | Length: 99\n",
      "Episode 3/1000 | Return: -101.37 | Avg(10): -69.62 | Length: 78\n",
      "Episode 4/1000 | Return: -10.02 | Avg(10): -54.72 | Length: 100\n",
      "Episode 5/1000 | Return: -6.89 | Avg(10): -45.15 | Length: 100\n",
      "Episode 6/1000 | Return: -16.58 | Avg(10): -40.39 | Length: 100\n",
      "Episode 7/1000 | Return: -8.53 | Avg(10): -35.84 | Length: 100\n",
      "Episode 8/1000 | Return: -12.06 | Avg(10): -32.87 | Length: 100\n",
      "Episode 9/1000 | Return: -3.47 | Avg(10): -29.60 | Length: 100\n",
      "Episode 10/1000 | Return: -98.03 | Avg(10): -36.44 | Length: 84\n",
      "  Losses - World: 76.252, Q1: 77.880, Policy: -0.870, Alpha: 0.197\n",
      "  Losses - World: 68.928, Q1: 73.207, Policy: -1.018, Alpha: 0.194\n",
      "Episode 11/1000 | Return: -12.22 | Avg(10): -36.98 | Length: 100\n",
      "Episode 12/1000 | Return: -106.02 | Avg(10): -37.52 | Length: 80\n",
      "Episode 13/1000 | Return: -12.85 | Avg(10): -28.67 | Length: 100\n",
      "Episode 14/1000 | Return: -5.70 | Avg(10): -28.23 | Length: 100\n",
      "Episode 15/1000 | Return: -126.64 | Avg(10): -40.21 | Length: 91\n",
      "Episode 16/1000 | Return: -107.38 | Avg(10): -49.29 | Length: 59\n",
      "Episode 17/1000 | Return: -8.29 | Avg(10): -49.27 | Length: 100\n",
      "Episode 18/1000 | Return: -110.83 | Avg(10): -59.14 | Length: 94\n",
      "Episode 19/1000 | Return: -16.03 | Avg(10): -60.40 | Length: 100\n",
      "Episode 20/1000 | Return: -122.65 | Avg(10): -62.86 | Length: 69\n",
      "  Losses - World: 3.559, Q1: 8.346, Policy: -1.769, Alpha: 0.151\n",
      "Episode 21/1000 | Return: -104.22 | Avg(10): -72.06 | Length: 66\n",
      "Episode 22/1000 | Return: -105.79 | Avg(10): -72.04 | Length: 55\n",
      "Episode 23/1000 | Return: -104.85 | Avg(10): -81.24 | Length: 65\n",
      "Episode 24/1000 | Return: -98.55 | Avg(10): -90.52 | Length: 86\n",
      "Episode 25/1000 | Return: -102.73 | Avg(10): -88.13 | Length: 80\n",
      "Episode 26/1000 | Return: -99.05 | Avg(10): -87.30 | Length: 66\n",
      "Episode 27/1000 | Return: -5.65 | Avg(10): -87.03 | Length: 100\n",
      "Episode 28/1000 | Return: -124.35 | Avg(10): -88.39 | Length: 92\n",
      "Episode 29/1000 | Return: -9.64 | Avg(10): -87.75 | Length: 100\n",
      "Episode 30/1000 | Return: -11.05 | Avg(10): -76.59 | Length: 100\n",
      "  Losses - World: 57.340, Q1: 91.194, Policy: -5.970, Alpha: 0.121\n",
      "Episode 31/1000 | Return: -102.18 | Avg(10): -76.38 | Length: 75\n",
      "Episode 32/1000 | Return: -7.45 | Avg(10): -66.55 | Length: 100\n",
      "Episode 33/1000 | Return: -98.52 | Avg(10): -65.92 | Length: 72\n",
      "Episode 34/1000 | Return: -99.21 | Avg(10): -65.98 | Length: 63\n",
      "Episode 35/1000 | Return: -101.95 | Avg(10): -65.90 | Length: 52\n",
      "Episode 36/1000 | Return: -98.98 | Avg(10): -65.90 | Length: 91\n",
      "Episode 37/1000 | Return: -100.48 | Avg(10): -75.38 | Length: 96\n",
      "Episode 38/1000 | Return: 0.88 | Avg(10): -62.86 | Length: 100\n",
      "Episode 39/1000 | Return: -18.90 | Avg(10): -63.78 | Length: 100\n",
      "Episode 40/1000 | Return: -101.97 | Avg(10): -72.88 | Length: 71\n",
      "  Losses - World: 8.665, Q1: 12.195, Policy: -7.458, Alpha: 0.098\n",
      "  Losses - World: 0.077, Q1: 15.686, Policy: -7.618, Alpha: 0.097\n",
      "Episode 41/1000 | Return: -101.91 | Avg(10): -72.85 | Length: 76\n",
      "Episode 42/1000 | Return: -102.02 | Avg(10): -82.30 | Length: 54\n",
      "Episode 43/1000 | Return: -99.95 | Avg(10): -82.45 | Length: 57\n",
      "Episode 44/1000 | Return: -7.47 | Avg(10): -73.27 | Length: 100\n",
      "Episode 45/1000 | Return: -102.24 | Avg(10): -73.30 | Length: 76\n",
      "Episode 46/1000 | Return: -102.64 | Avg(10): -73.67 | Length: 53\n",
      "Episode 47/1000 | Return: -104.99 | Avg(10): -74.12 | Length: 80\n",
      "Episode 48/1000 | Return: -101.07 | Avg(10): -84.32 | Length: 73\n",
      "Episode 49/1000 | Return: -103.19 | Avg(10): -92.74 | Length: 86\n",
      "Episode 50/1000 | Return: -103.10 | Avg(10): -92.86 | Length: 91\n",
      "  Losses - World: 0.058, Q1: 28.407, Policy: -5.527, Alpha: 0.080\n",
      "Episode 51/1000 | Return: -106.00 | Avg(10): -93.27 | Length: 47\n",
      "Episode 52/1000 | Return: -106.13 | Avg(10): -93.68 | Length: 47\n",
      "Episode 53/1000 | Return: -104.51 | Avg(10): -94.13 | Length: 58\n",
      "Episode 54/1000 | Return: -24.49 | Avg(10): -95.83 | Length: 100\n",
      "Episode 55/1000 | Return: -101.17 | Avg(10): -95.73 | Length: 72\n",
      "Episode 56/1000 | Return: -101.90 | Avg(10): -95.65 | Length: 69\n",
      "Episode 57/1000 | Return: -105.33 | Avg(10): -95.69 | Length: 51\n",
      "Episode 58/1000 | Return: -100.33 | Avg(10): -95.61 | Length: 75\n",
      "Episode 59/1000 | Return: -100.73 | Avg(10): -95.37 | Length: 63\n",
      "Episode 60/1000 | Return: -102.10 | Avg(10): -95.27 | Length: 84\n",
      "  Losses - World: 0.623, Q1: 8.289, Policy: -3.538, Alpha: 0.067\n",
      "Episode 61/1000 | Return: -106.53 | Avg(10): -95.32 | Length: 49\n",
      "Episode 62/1000 | Return: -101.35 | Avg(10): -94.84 | Length: 68\n",
      "Episode 63/1000 | Return: -106.11 | Avg(10): -95.00 | Length: 49\n",
      "Episode 64/1000 | Return: -12.39 | Avg(10): -93.80 | Length: 100\n",
      "Episode 65/1000 | Return: -101.80 | Avg(10): -93.86 | Length: 61\n",
      "Episode 66/1000 | Return: -101.16 | Avg(10): -93.78 | Length: 92\n",
      "Episode 67/1000 | Return: -100.76 | Avg(10): -93.33 | Length: 87\n",
      "Episode 68/1000 | Return: -105.15 | Avg(10): -93.81 | Length: 92\n",
      "Episode 69/1000 | Return: -21.36 | Avg(10): -85.87 | Length: 100\n",
      "Episode 70/1000 | Return: -11.63 | Avg(10): -76.82 | Length: 100\n",
      "  Losses - World: 0.464, Q1: 15.376, Policy: -0.774, Alpha: 0.054\n",
      "  Losses - World: 1.199, Q1: 26.450, Policy: -3.633, Alpha: 0.053\n",
      "Episode 71/1000 | Return: -14.15 | Avg(10): -67.59 | Length: 100\n",
      "Episode 72/1000 | Return: -19.25 | Avg(10): -59.38 | Length: 100\n",
      "Episode 73/1000 | Return: -108.44 | Avg(10): -59.61 | Length: 62\n",
      "Episode 74/1000 | Return: -20.95 | Avg(10): -60.46 | Length: 100\n",
      "Episode 75/1000 | Return: -10.97 | Avg(10): -51.38 | Length: 100\n",
      "Episode 76/1000 | Return: -110.49 | Avg(10): -52.31 | Length: 45\n",
      "Episode 77/1000 | Return: -10.19 | Avg(10): -43.26 | Length: 100\n",
      "Episode 78/1000 | Return: -9.31 | Avg(10): -33.67 | Length: 100\n",
      "Episode 79/1000 | Return: -18.83 | Avg(10): -33.42 | Length: 100\n",
      "Episode 80/1000 | Return: -11.67 | Avg(10): -33.42 | Length: 100\n",
      "  Losses - World: 0.052, Q1: 31.700, Policy: -2.467, Alpha: 0.043\n",
      "  Losses - World: 0.195, Q1: 13.005, Policy: -2.087, Alpha: 0.042\n",
      "Episode 81/1000 | Return: -0.79 | Avg(10): -32.09 | Length: 100\n",
      "Episode 82/1000 | Return: -12.17 | Avg(10): -31.38 | Length: 100\n",
      "Episode 83/1000 | Return: -100.28 | Avg(10): -30.56 | Length: 57\n",
      "Episode 84/1000 | Return: -103.05 | Avg(10): -38.77 | Length: 70\n",
      "Episode 85/1000 | Return: -6.55 | Avg(10): -38.33 | Length: 100\n",
      "Episode 86/1000 | Return: -10.11 | Avg(10): -28.29 | Length: 100\n",
      "Episode 87/1000 | Return: -15.15 | Avg(10): -28.79 | Length: 100\n",
      "Episode 88/1000 | Return: -100.94 | Avg(10): -37.95 | Length: 88\n",
      "Episode 89/1000 | Return: -114.38 | Avg(10): -47.51 | Length: 44\n",
      "Episode 90/1000 | Return: -103.59 | Avg(10): -56.70 | Length: 81\n",
      "  Losses - World: 0.972, Q1: 12.774, Policy: -0.426, Alpha: 0.035\n",
      "  Losses - World: 0.266, Q1: 10.654, Policy: -3.192, Alpha: 0.034\n",
      "Episode 91/1000 | Return: -18.84 | Avg(10): -58.51 | Length: 100\n",
      "Episode 92/1000 | Return: -109.03 | Avg(10): -68.19 | Length: 48\n",
      "Episode 93/1000 | Return: -10.77 | Avg(10): -59.24 | Length: 100\n",
      "Episode 94/1000 | Return: -104.20 | Avg(10): -59.36 | Length: 46\n",
      "Episode 95/1000 | Return: -16.34 | Avg(10): -60.33 | Length: 100\n",
      "Episode 96/1000 | Return: -111.40 | Avg(10): -70.46 | Length: 72\n",
      "Episode 97/1000 | Return: -110.16 | Avg(10): -79.97 | Length: 89\n",
      "Episode 98/1000 | Return: -20.09 | Avg(10): -71.88 | Length: 100\n",
      "Episode 99/1000 | Return: -103.14 | Avg(10): -70.76 | Length: 85\n",
      "Episode 100/1000 | Return: -10.35 | Avg(10): -61.43 | Length: 100\n",
      "  Losses - World: 0.030, Q1: 16.514, Policy: -5.741, Alpha: 0.028\n",
      "  Losses - World: 0.151, Q1: 13.116, Policy: -5.653, Alpha: 0.028\n",
      "Episode 101/1000 | Return: -7.97 | Avg(10): -60.35 | Length: 100\n",
      "Episode 102/1000 | Return: -7.26 | Avg(10): -50.17 | Length: 100\n",
      "Episode 103/1000 | Return: -101.37 | Avg(10): -59.23 | Length: 79\n",
      "Episode 104/1000 | Return: -10.51 | Avg(10): -49.86 | Length: 100\n",
      "Episode 105/1000 | Return: -12.83 | Avg(10): -49.51 | Length: 100\n",
      "Episode 106/1000 | Return: -105.74 | Avg(10): -48.94 | Length: 94\n",
      "Episode 107/1000 | Return: -6.14 | Avg(10): -38.54 | Length: 100\n",
      "Episode 108/1000 | Return: -104.29 | Avg(10): -46.96 | Length: 53\n",
      "Episode 109/1000 | Return: -10.74 | Avg(10): -37.72 | Length: 100\n",
      "Episode 110/1000 | Return: -103.03 | Avg(10): -46.99 | Length: 46\n",
      "  Losses - World: 12.727, Q1: 10.495, Policy: -2.806, Alpha: 0.022\n",
      "Episode 111/1000 | Return: -106.09 | Avg(10): -56.80 | Length: 95\n",
      "Episode 112/1000 | Return: -99.83 | Avg(10): -66.06 | Length: 86\n",
      "Episode 113/1000 | Return: -103.28 | Avg(10): -66.25 | Length: 63\n",
      "Episode 114/1000 | Return: -10.95 | Avg(10): -66.29 | Length: 100\n",
      "Episode 115/1000 | Return: -9.15 | Avg(10): -65.93 | Length: 100\n",
      "Episode 116/1000 | Return: -3.74 | Avg(10): -55.73 | Length: 100\n",
      "Episode 117/1000 | Return: -120.41 | Avg(10): -67.15 | Length: 75\n",
      "Episode 118/1000 | Return: -4.13 | Avg(10): -57.14 | Length: 100\n",
      "Episode 119/1000 | Return: -110.10 | Avg(10): -67.07 | Length: 57\n",
      "Episode 120/1000 | Return: -6.78 | Avg(10): -57.45 | Length: 100\n",
      "  Losses - World: 3.252, Q1: 22.139, Policy: -4.217, Alpha: 0.018\n",
      "Episode 121/1000 | Return: -107.09 | Avg(10): -57.55 | Length: 54\n",
      "Episode 122/1000 | Return: -7.38 | Avg(10): -48.30 | Length: 100\n",
      "Episode 123/1000 | Return: -0.67 | Avg(10): -38.04 | Length: 100\n",
      "Episode 124/1000 | Return: -15.25 | Avg(10): -38.47 | Length: 100\n",
      "Episode 125/1000 | Return: -3.23 | Avg(10): -37.88 | Length: 100\n",
      "Episode 126/1000 | Return: -5.32 | Avg(10): -38.04 | Length: 100\n",
      "Episode 127/1000 | Return: -18.70 | Avg(10): -27.86 | Length: 100\n",
      "Episode 128/1000 | Return: -8.53 | Avg(10): -28.30 | Length: 100\n",
      "Episode 129/1000 | Return: -4.69 | Avg(10): -17.76 | Length: 100\n",
      "Episode 130/1000 | Return: -4.86 | Avg(10): -17.57 | Length: 100\n",
      "  Losses - World: 14.293, Q1: 15.184, Policy: -3.564, Alpha: 0.015\n",
      "  Losses - World: 0.062, Q1: 32.088, Policy: -2.532, Alpha: 0.015\n",
      "Episode 131/1000 | Return: -5.71 | Avg(10): -7.43 | Length: 100\n",
      "Episode 132/1000 | Return: -1.18 | Avg(10): -6.81 | Length: 100\n",
      "Episode 133/1000 | Return: -9.27 | Avg(10): -7.67 | Length: 100\n",
      "Episode 134/1000 | Return: -2.87 | Avg(10): -6.43 | Length: 100\n",
      "Episode 135/1000 | Return: -1.46 | Avg(10): -6.26 | Length: 100\n",
      "Episode 136/1000 | Return: -112.32 | Avg(10): -16.96 | Length: 52\n",
      "Episode 137/1000 | Return: -5.80 | Avg(10): -15.67 | Length: 100\n",
      "Episode 138/1000 | Return: -5.72 | Avg(10): -15.39 | Length: 100\n",
      "Episode 139/1000 | Return: -11.41 | Avg(10): -16.06 | Length: 100\n",
      "Episode 140/1000 | Return: -8.55 | Avg(10): -16.43 | Length: 100\n",
      "  Losses - World: 0.053, Q1: 6.771, Policy: -2.201, Alpha: 0.013\n",
      "  Losses - World: 13.741, Q1: 2.959, Policy: -5.410, Alpha: 0.013\n",
      "Episode 141/1000 | Return: -11.81 | Avg(10): -17.04 | Length: 100\n",
      "Episode 142/1000 | Return: -5.20 | Avg(10): -17.44 | Length: 100\n",
      "Episode 143/1000 | Return: -4.14 | Avg(10): -16.93 | Length: 100\n",
      "Episode 144/1000 | Return: -122.37 | Avg(10): -28.88 | Length: 44\n",
      "Episode 145/1000 | Return: -7.03 | Avg(10): -29.43 | Length: 100\n",
      "Episode 146/1000 | Return: -3.93 | Avg(10): -18.60 | Length: 100\n",
      "Episode 147/1000 | Return: -3.56 | Avg(10): -18.37 | Length: 100\n",
      "Episode 148/1000 | Return: 0.01 | Avg(10): -17.80 | Length: 100\n",
      "Episode 149/1000 | Return: -5.37 | Avg(10): -17.19 | Length: 100\n",
      "Episode 150/1000 | Return: -9.43 | Avg(10): -17.28 | Length: 100\n",
      "  Losses - World: 61.887, Q1: 18.417, Policy: -2.056, Alpha: 0.013\n",
      "  Losses - World: 0.021, Q1: 3.060, Policy: -7.611, Alpha: 0.013\n",
      "Episode 151/1000 | Return: -4.29 | Avg(10): -16.53 | Length: 100\n",
      "Episode 152/1000 | Return: -3.91 | Avg(10): -16.40 | Length: 100\n",
      "Episode 153/1000 | Return: -3.84 | Avg(10): -16.37 | Length: 100\n",
      "Episode 154/1000 | Return: -1.28 | Avg(10): -4.26 | Length: 100\n",
      "Episode 155/1000 | Return: -123.94 | Avg(10): -15.95 | Length: 56\n",
      "Episode 156/1000 | Return: -6.03 | Avg(10): -16.16 | Length: 100\n",
      "Episode 157/1000 | Return: -11.00 | Avg(10): -16.91 | Length: 100\n",
      "Episode 158/1000 | Return: -119.61 | Avg(10): -28.87 | Length: 46\n",
      "Episode 159/1000 | Return: -5.79 | Avg(10): -28.91 | Length: 100\n",
      "Episode 160/1000 | Return: -109.42 | Avg(10): -38.91 | Length: 76\n",
      "Episode 161/1000 | Return: -111.04 | Avg(10): -49.59 | Length: 41\n",
      "Episode 162/1000 | Return: -7.02 | Avg(10): -49.90 | Length: 100\n",
      "Episode 163/1000 | Return: -7.30 | Avg(10): -50.24 | Length: 100\n",
      "Episode 164/1000 | Return: -11.74 | Avg(10): -51.29 | Length: 100\n",
      "Episode 165/1000 | Return: -97.09 | Avg(10): -48.60 | Length: 96\n",
      "Episode 166/1000 | Return: -3.42 | Avg(10): -48.34 | Length: 100\n",
      "Episode 167/1000 | Return: -3.37 | Avg(10): -47.58 | Length: 100\n",
      "Episode 168/1000 | Return: -5.84 | Avg(10): -36.20 | Length: 100\n",
      "Episode 169/1000 | Return: -2.93 | Avg(10): -35.92 | Length: 100\n",
      "Episode 170/1000 | Return: -0.77 | Avg(10): -25.05 | Length: 100\n",
      "  Losses - World: 0.046, Q1: 5.415, Policy: -2.959, Alpha: 0.012\n",
      "  Losses - World: 0.404, Q1: 10.608, Policy: -2.880, Alpha: 0.012\n",
      "Episode 171/1000 | Return: -108.68 | Avg(10): -24.81 | Length: 63\n",
      "Episode 172/1000 | Return: 2.01 | Avg(10): -23.91 | Length: 100\n",
      "Episode 173/1000 | Return: -6.27 | Avg(10): -23.81 | Length: 100\n",
      "Episode 174/1000 | Return: 0.67 | Avg(10): -22.57 | Length: 100\n",
      "Episode 175/1000 | Return: -97.27 | Avg(10): -22.59 | Length: 65\n",
      "Episode 176/1000 | Return: -109.05 | Avg(10): -33.15 | Length: 53\n",
      "Episode 177/1000 | Return: -12.25 | Avg(10): -34.04 | Length: 100\n",
      "Episode 178/1000 | Return: -110.76 | Avg(10): -44.53 | Length: 46\n",
      "Episode 179/1000 | Return: -110.16 | Avg(10): -55.25 | Length: 44\n",
      "Episode 180/1000 | Return: -5.82 | Avg(10): -55.76 | Length: 100\n",
      "  Losses - World: 1.726, Q1: 5.536, Policy: -0.254, Alpha: 0.011\n",
      "  Losses - World: 1.919, Q1: 12.887, Policy: -3.016, Alpha: 0.011\n",
      "Episode 181/1000 | Return: -3.33 | Avg(10): -45.22 | Length: 100\n",
      "Episode 182/1000 | Return: -99.87 | Avg(10): -55.41 | Length: 83\n",
      "Episode 183/1000 | Return: -3.77 | Avg(10): -55.16 | Length: 100\n",
      "Episode 184/1000 | Return: -5.94 | Avg(10): -55.82 | Length: 100\n",
      "Episode 185/1000 | Return: -6.50 | Avg(10): -46.75 | Length: 100\n",
      "Episode 186/1000 | Return: -111.22 | Avg(10): -46.96 | Length: 60\n",
      "Episode 187/1000 | Return: -101.33 | Avg(10): -55.87 | Length: 94\n",
      "Episode 188/1000 | Return: -13.45 | Avg(10): -46.14 | Length: 100\n",
      "Episode 189/1000 | Return: -4.78 | Avg(10): -35.60 | Length: 100\n",
      "Episode 190/1000 | Return: -104.99 | Avg(10): -45.52 | Length: 86\n",
      "  Losses - World: 0.351, Q1: 9.121, Policy: -3.339, Alpha: 0.011\n",
      "Episode 191/1000 | Return: -107.54 | Avg(10): -55.94 | Length: 51\n",
      "Episode 192/1000 | Return: -10.08 | Avg(10): -46.96 | Length: 100\n",
      "Episode 193/1000 | Return: -12.20 | Avg(10): -47.80 | Length: 100\n",
      "Episode 194/1000 | Return: -109.24 | Avg(10): -58.13 | Length: 93\n",
      "Episode 195/1000 | Return: -10.78 | Avg(10): -58.56 | Length: 100\n",
      "Episode 196/1000 | Return: -116.53 | Avg(10): -59.09 | Length: 62\n",
      "Episode 197/1000 | Return: -107.70 | Avg(10): -59.73 | Length: 60\n",
      "Episode 198/1000 | Return: -109.37 | Avg(10): -69.32 | Length: 74\n",
      "Episode 199/1000 | Return: -5.60 | Avg(10): -69.40 | Length: 100\n",
      "Episode 200/1000 | Return: -2.09 | Avg(10): -59.11 | Length: 100\n",
      "  Losses - World: 0.679, Q1: 9.054, Policy: 0.100, Alpha: 0.011\n",
      "  Losses - World: 0.264, Q1: 7.811, Policy: 0.144, Alpha: 0.012\n",
      "Episode 201/1000 | Return: -11.36 | Avg(10): -49.50 | Length: 100\n",
      "Episode 202/1000 | Return: -110.96 | Avg(10): -59.58 | Length: 42\n",
      "Episode 203/1000 | Return: -110.88 | Avg(10): -69.45 | Length: 49\n",
      "Episode 204/1000 | Return: -106.43 | Avg(10): -69.17 | Length: 71\n",
      "Episode 205/1000 | Return: -102.64 | Avg(10): -78.36 | Length: 59\n",
      "Episode 206/1000 | Return: -105.82 | Avg(10): -77.29 | Length: 64\n",
      "Episode 207/1000 | Return: -4.22 | Avg(10): -66.94 | Length: 100\n",
      "Episode 208/1000 | Return: -2.30 | Avg(10): -56.23 | Length: 100\n",
      "Episode 209/1000 | Return: -3.76 | Avg(10): -56.05 | Length: 100\n",
      "Episode 210/1000 | Return: -7.91 | Avg(10): -56.63 | Length: 100\n",
      "  Losses - World: 72.356, Q1: 8.846, Policy: -2.275, Alpha: 0.013\n",
      "  Losses - World: 0.054, Q1: 7.592, Policy: -4.000, Alpha: 0.013\n",
      "Episode 211/1000 | Return: -9.83 | Avg(10): -56.47 | Length: 100\n",
      "Episode 212/1000 | Return: -108.95 | Avg(10): -56.27 | Length: 76\n",
      "Episode 213/1000 | Return: -107.60 | Avg(10): -55.95 | Length: 83\n",
      "Episode 214/1000 | Return: -7.57 | Avg(10): -46.06 | Length: 100\n",
      "Episode 215/1000 | Return: 1.75 | Avg(10): -35.62 | Length: 100\n",
      "Episode 216/1000 | Return: -15.88 | Avg(10): -26.63 | Length: 100\n",
      "Episode 217/1000 | Return: -106.16 | Avg(10): -36.82 | Length: 90\n",
      "Episode 218/1000 | Return: -4.01 | Avg(10): -36.99 | Length: 100\n",
      "Episode 219/1000 | Return: -110.57 | Avg(10): -47.67 | Length: 67\n",
      "Episode 220/1000 | Return: -119.59 | Avg(10): -58.84 | Length: 85\n",
      "  Losses - World: 0.031, Q1: 4.226, Policy: -4.906, Alpha: 0.013\n",
      "  Losses - World: 0.213, Q1: 14.579, Policy: -3.514, Alpha: 0.013\n",
      "Episode 221/1000 | Return: -13.71 | Avg(10): -59.23 | Length: 100\n",
      "Episode 222/1000 | Return: -108.92 | Avg(10): -59.23 | Length: 79\n",
      "Episode 223/1000 | Return: -4.89 | Avg(10): -48.95 | Length: 100\n",
      "Episode 224/1000 | Return: -106.43 | Avg(10): -58.84 | Length: 76\n",
      "Episode 225/1000 | Return: -1.17 | Avg(10): -59.13 | Length: 100\n",
      "Episode 226/1000 | Return: -5.76 | Avg(10): -58.12 | Length: 100\n",
      "Episode 227/1000 | Return: -5.52 | Avg(10): -48.06 | Length: 100\n",
      "Episode 228/1000 | Return: -105.17 | Avg(10): -58.17 | Length: 67\n",
      "Episode 229/1000 | Return: -1.81 | Avg(10): -47.30 | Length: 100\n",
      "Episode 230/1000 | Return: -9.30 | Avg(10): -36.27 | Length: 100\n",
      "  Losses - World: 0.391, Q1: 18.226, Policy: 0.276, Alpha: 0.013\n",
      "Episode 231/1000 | Return: -111.52 | Avg(10): -46.05 | Length: 57\n",
      "Episode 232/1000 | Return: -112.91 | Avg(10): -46.45 | Length: 58\n",
      "Episode 233/1000 | Return: -13.63 | Avg(10): -47.32 | Length: 100\n",
      "Episode 234/1000 | Return: -5.65 | Avg(10): -37.24 | Length: 100\n",
      "Episode 235/1000 | Return: -96.32 | Avg(10): -46.76 | Length: 73\n",
      "Episode 236/1000 | Return: -7.00 | Avg(10): -46.88 | Length: 100\n",
      "Episode 237/1000 | Return: -7.38 | Avg(10): -47.07 | Length: 100\n",
      "Episode 238/1000 | Return: -1.10 | Avg(10): -36.66 | Length: 100\n",
      "Episode 239/1000 | Return: 3.66 | Avg(10): -36.12 | Length: 100\n",
      "Episode 240/1000 | Return: -105.03 | Avg(10): -45.69 | Length: 51\n",
      "  Losses - World: 0.048, Q1: 4.157, Policy: -4.440, Alpha: 0.011\n",
      "  Losses - World: 0.063, Q1: 13.227, Policy: -2.712, Alpha: 0.011\n",
      "Episode 241/1000 | Return: -7.54 | Avg(10): -35.29 | Length: 100\n",
      "Episode 242/1000 | Return: -6.07 | Avg(10): -24.61 | Length: 100\n",
      "Episode 243/1000 | Return: -3.90 | Avg(10): -23.63 | Length: 100\n",
      "Episode 244/1000 | Return: -100.91 | Avg(10): -33.16 | Length: 62\n",
      "Episode 245/1000 | Return: -4.15 | Avg(10): -23.94 | Length: 100\n",
      "Episode 246/1000 | Return: -107.21 | Avg(10): -33.97 | Length: 56\n",
      "Episode 247/1000 | Return: -7.03 | Avg(10): -33.93 | Length: 100\n",
      "Episode 248/1000 | Return: -7.51 | Avg(10): -34.57 | Length: 100\n",
      "Episode 249/1000 | Return: -7.05 | Avg(10): -35.64 | Length: 100\n",
      "Episode 250/1000 | Return: -15.69 | Avg(10): -26.71 | Length: 100\n",
      "  Losses - World: 0.027, Q1: 27.462, Policy: 2.291, Alpha: 0.012\n",
      "  Losses - World: 0.126, Q1: 6.174, Policy: -1.372, Alpha: 0.012\n",
      "Episode 251/1000 | Return: -103.06 | Avg(10): -36.26 | Length: 87\n",
      "Episode 252/1000 | Return: -3.86 | Avg(10): -36.04 | Length: 100\n",
      "Episode 253/1000 | Return: -5.69 | Avg(10): -36.22 | Length: 100\n",
      "Episode 254/1000 | Return: -4.52 | Avg(10): -26.58 | Length: 100\n",
      "Episode 255/1000 | Return: -1.06 | Avg(10): -26.27 | Length: 100\n",
      "Episode 256/1000 | Return: -5.61 | Avg(10): -16.11 | Length: 100\n",
      "Episode 257/1000 | Return: -115.32 | Avg(10): -26.94 | Length: 74\n",
      "Episode 258/1000 | Return: -108.34 | Avg(10): -37.02 | Length: 91\n",
      "Episode 259/1000 | Return: -3.36 | Avg(10): -36.65 | Length: 100\n",
      "Episode 260/1000 | Return: -2.82 | Avg(10): -35.36 | Length: 100\n",
      "  Losses - World: 0.033, Q1: 6.800, Policy: -0.215, Alpha: 0.012\n",
      "  Losses - World: 0.043, Q1: 6.231, Policy: -1.206, Alpha: 0.012\n",
      "Episode 261/1000 | Return: -16.64 | Avg(10): -26.72 | Length: 100\n",
      "Episode 262/1000 | Return: -4.15 | Avg(10): -26.75 | Length: 100\n",
      "Episode 263/1000 | Return: -13.65 | Avg(10): -27.55 | Length: 100\n",
      "Episode 264/1000 | Return: -11.51 | Avg(10): -28.25 | Length: 100\n",
      "Episode 265/1000 | Return: -1.19 | Avg(10): -28.26 | Length: 100\n",
      "Episode 266/1000 | Return: -15.01 | Avg(10): -29.20 | Length: 100\n",
      "Episode 267/1000 | Return: -5.12 | Avg(10): -18.18 | Length: 100\n",
      "Episode 268/1000 | Return: -17.59 | Avg(10): -9.10 | Length: 100\n",
      "Episode 269/1000 | Return: -101.84 | Avg(10): -18.95 | Length: 100\n",
      "Episode 270/1000 | Return: -0.88 | Avg(10): -18.76 | Length: 100\n",
      "  Losses - World: 0.026, Q1: 9.165, Policy: 4.265, Alpha: 0.011\n",
      "  Losses - World: 0.024, Q1: 14.331, Policy: 3.708, Alpha: 0.011\n",
      "Episode 271/1000 | Return: -13.11 | Avg(10): -18.40 | Length: 100\n",
      "Episode 272/1000 | Return: -10.56 | Avg(10): -19.04 | Length: 100\n",
      "Episode 273/1000 | Return: -107.68 | Avg(10): -28.45 | Length: 54\n",
      "Episode 274/1000 | Return: -12.01 | Avg(10): -28.50 | Length: 100\n",
      "Episode 275/1000 | Return: -10.83 | Avg(10): -29.46 | Length: 100\n",
      "Episode 276/1000 | Return: -6.33 | Avg(10): -28.59 | Length: 100\n",
      "Episode 277/1000 | Return: -13.38 | Avg(10): -29.42 | Length: 100\n",
      "Episode 278/1000 | Return: -103.60 | Avg(10): -38.02 | Length: 97\n",
      "Episode 279/1000 | Return: -2.17 | Avg(10): -28.05 | Length: 100\n",
      "Episode 280/1000 | Return: -8.93 | Avg(10): -28.86 | Length: 100\n",
      "  Losses - World: 3.508, Q1: 11.176, Policy: 3.396, Alpha: 0.009\n",
      "  Losses - World: 0.031, Q1: 12.547, Policy: 6.885, Alpha: 0.009\n",
      "Episode 281/1000 | Return: -16.33 | Avg(10): -29.18 | Length: 100\n",
      "Episode 282/1000 | Return: -17.87 | Avg(10): -29.91 | Length: 100\n",
      "Episode 283/1000 | Return: -14.58 | Avg(10): -20.60 | Length: 100\n",
      "Episode 284/1000 | Return: -114.79 | Avg(10): -30.88 | Length: 62\n",
      "Episode 285/1000 | Return: -121.43 | Avg(10): -41.94 | Length: 53\n",
      "Episode 286/1000 | Return: -13.03 | Avg(10): -42.61 | Length: 100\n",
      "Episode 287/1000 | Return: -4.38 | Avg(10): -41.71 | Length: 100\n",
      "Episode 288/1000 | Return: -12.75 | Avg(10): -32.63 | Length: 100\n",
      "Episode 289/1000 | Return: -16.47 | Avg(10): -34.06 | Length: 100\n",
      "Episode 290/1000 | Return: -5.21 | Avg(10): -33.68 | Length: 100\n",
      "  Losses - World: 0.034, Q1: 7.622, Policy: 7.507, Alpha: 0.009\n",
      "  Losses - World: 76.186, Q1: 19.430, Policy: 3.939, Alpha: 0.009\n",
      "Episode 291/1000 | Return: -17.24 | Avg(10): -33.78 | Length: 100\n",
      "Episode 292/1000 | Return: -8.13 | Avg(10): -32.80 | Length: 100\n",
      "Episode 293/1000 | Return: -11.29 | Avg(10): -32.47 | Length: 100\n",
      "Episode 294/1000 | Return: -12.24 | Avg(10): -22.22 | Length: 100\n",
      "Episode 295/1000 | Return: -5.18 | Avg(10): -10.59 | Length: 100\n",
      "Episode 296/1000 | Return: -9.29 | Avg(10): -10.22 | Length: 100\n",
      "Episode 297/1000 | Return: -12.16 | Avg(10): -11.00 | Length: 100\n",
      "Episode 298/1000 | Return: -113.18 | Avg(10): -21.04 | Length: 64\n",
      "Episode 299/1000 | Return: -6.00 | Avg(10): -19.99 | Length: 100\n",
      "Episode 300/1000 | Return: -3.13 | Avg(10): -19.78 | Length: 100\n",
      "  Losses - World: 0.212, Q1: 7.476, Policy: 4.605, Alpha: 0.008\n",
      "  Losses - World: 0.023, Q1: 3.520, Policy: 0.206, Alpha: 0.008\n",
      "Episode 301/1000 | Return: -1.55 | Avg(10): -18.22 | Length: 100\n",
      "Episode 302/1000 | Return: -8.61 | Avg(10): -18.26 | Length: 100\n",
      "Episode 303/1000 | Return: -7.64 | Avg(10): -17.90 | Length: 100\n",
      "Episode 304/1000 | Return: -5.20 | Avg(10): -17.20 | Length: 100\n",
      "Episode 305/1000 | Return: -3.78 | Avg(10): -17.05 | Length: 100\n",
      "Episode 306/1000 | Return: -4.94 | Avg(10): -16.62 | Length: 100\n",
      "Episode 307/1000 | Return: -11.13 | Avg(10): -16.52 | Length: 100\n",
      "Episode 308/1000 | Return: -104.89 | Avg(10): -15.69 | Length: 66\n",
      "Episode 309/1000 | Return: -16.51 | Avg(10): -16.74 | Length: 100\n",
      "Episode 310/1000 | Return: -103.32 | Avg(10): -26.76 | Length: 58\n",
      "  Losses - World: 0.368, Q1: 10.634, Policy: 5.843, Alpha: 0.008\n",
      "  Losses - World: 0.067, Q1: 13.491, Policy: 3.554, Alpha: 0.008\n",
      "Episode 311/1000 | Return: -6.51 | Avg(10): -27.25 | Length: 100\n",
      "Episode 312/1000 | Return: 1.89 | Avg(10): -26.20 | Length: 100\n",
      "Episode 313/1000 | Return: -101.65 | Avg(10): -35.60 | Length: 84\n",
      "Episode 314/1000 | Return: -104.42 | Avg(10): -45.53 | Length: 86\n",
      "Episode 315/1000 | Return: -103.64 | Avg(10): -55.51 | Length: 77\n",
      "Episode 316/1000 | Return: -11.12 | Avg(10): -56.13 | Length: 100\n",
      "Episode 317/1000 | Return: -12.93 | Avg(10): -56.31 | Length: 100\n",
      "Episode 318/1000 | Return: -6.72 | Avg(10): -46.49 | Length: 100\n",
      "Episode 319/1000 | Return: -4.24 | Avg(10): -45.27 | Length: 100\n",
      "Episode 320/1000 | Return: 0.83 | Avg(10): -34.85 | Length: 100\n",
      "  Losses - World: 12.659, Q1: 5.377, Policy: 2.294, Alpha: 0.007\n",
      "  Losses - World: 4.269, Q1: 2.672, Policy: 7.092, Alpha: 0.007\n",
      "Episode 321/1000 | Return: -6.43 | Avg(10): -34.84 | Length: 100\n",
      "Episode 322/1000 | Return: -4.41 | Avg(10): -35.47 | Length: 100\n",
      "Episode 323/1000 | Return: 0.90 | Avg(10): -25.22 | Length: 100\n",
      "Episode 324/1000 | Return: -101.19 | Avg(10): -24.90 | Length: 64\n",
      "Episode 325/1000 | Return: -13.82 | Avg(10): -15.91 | Length: 100\n",
      "Episode 326/1000 | Return: -7.81 | Avg(10): -15.58 | Length: 100\n",
      "Episode 327/1000 | Return: 2.67 | Avg(10): -14.02 | Length: 100\n",
      "Episode 328/1000 | Return: -104.74 | Avg(10): -23.82 | Length: 79\n",
      "Episode 329/1000 | Return: -19.78 | Avg(10): -25.38 | Length: 100\n",
      "Episode 330/1000 | Return: -100.72 | Avg(10): -35.53 | Length: 84\n",
      "  Losses - World: 82.123, Q1: 6.580, Policy: 4.544, Alpha: 0.006\n",
      "  Losses - World: 0.026, Q1: 9.655, Policy: 0.900, Alpha: 0.006\n",
      "Episode 331/1000 | Return: -11.74 | Avg(10): -36.06 | Length: 100\n",
      "Episode 332/1000 | Return: -99.76 | Avg(10): -45.60 | Length: 99\n",
      "Episode 333/1000 | Return: -115.49 | Avg(10): -57.24 | Length: 76\n",
      "Episode 334/1000 | Return: -111.12 | Avg(10): -58.23 | Length: 70\n",
      "Episode 335/1000 | Return: -118.57 | Avg(10): -68.71 | Length: 67\n",
      "Episode 336/1000 | Return: -6.96 | Avg(10): -68.62 | Length: 100\n",
      "Episode 337/1000 | Return: -3.86 | Avg(10): -69.28 | Length: 100\n",
      "Episode 338/1000 | Return: -103.65 | Avg(10): -69.17 | Length: 79\n",
      "Episode 339/1000 | Return: -14.54 | Avg(10): -68.64 | Length: 100\n",
      "Episode 340/1000 | Return: -15.83 | Avg(10): -60.15 | Length: 100\n",
      "  Losses - World: 7.421, Q1: 9.797, Policy: 1.759, Alpha: 0.006\n",
      "  Losses - World: 6.913, Q1: 16.128, Policy: 1.205, Alpha: 0.006\n",
      "Episode 341/1000 | Return: -104.90 | Avg(10): -69.47 | Length: 100\n",
      "Episode 342/1000 | Return: -15.01 | Avg(10): -60.99 | Length: 100\n",
      "Episode 343/1000 | Return: -108.28 | Avg(10): -60.27 | Length: 76\n",
      "Episode 344/1000 | Return: -102.00 | Avg(10): -59.36 | Length: 94\n",
      "Episode 345/1000 | Return: -5.15 | Avg(10): -48.02 | Length: 100\n",
      "Episode 346/1000 | Return: -110.27 | Avg(10): -58.35 | Length: 48\n",
      "Episode 347/1000 | Return: -8.43 | Avg(10): -58.81 | Length: 100\n",
      "Episode 348/1000 | Return: -4.29 | Avg(10): -48.87 | Length: 100\n",
      "Episode 349/1000 | Return: 0.36 | Avg(10): -47.38 | Length: 100\n",
      "Episode 350/1000 | Return: -111.73 | Avg(10): -56.97 | Length: 78\n",
      "  Losses - World: 0.023, Q1: 3.493, Policy: 1.052, Alpha: 0.006\n",
      "  Losses - World: 1.340, Q1: 2.316, Policy: 2.002, Alpha: 0.006\n",
      "Episode 351/1000 | Return: -10.48 | Avg(10): -47.53 | Length: 100\n",
      "Episode 352/1000 | Return: -6.52 | Avg(10): -46.68 | Length: 100\n",
      "Episode 353/1000 | Return: -6.57 | Avg(10): -36.51 | Length: 100\n",
      "Episode 354/1000 | Return: -116.63 | Avg(10): -37.97 | Length: 77\n",
      "Episode 355/1000 | Return: -14.77 | Avg(10): -38.93 | Length: 100\n",
      "Episode 356/1000 | Return: -15.84 | Avg(10): -29.49 | Length: 100\n",
      "Episode 357/1000 | Return: -117.85 | Avg(10): -40.43 | Length: 63\n",
      "Episode 358/1000 | Return: -1.94 | Avg(10): -40.20 | Length: 100\n",
      "Episode 359/1000 | Return: -116.42 | Avg(10): -51.88 | Length: 56\n",
      "Episode 360/1000 | Return: -123.46 | Avg(10): -53.05 | Length: 83\n",
      "  Losses - World: 28.547, Q1: 16.233, Policy: 6.029, Alpha: 0.006\n",
      "  Losses - World: 0.067, Q1: 8.734, Policy: 4.007, Alpha: 0.006\n",
      "Episode 361/1000 | Return: -11.18 | Avg(10): -53.12 | Length: 100\n",
      "Episode 362/1000 | Return: -112.77 | Avg(10): -63.74 | Length: 48\n",
      "Episode 363/1000 | Return: -15.53 | Avg(10): -64.64 | Length: 100\n",
      "Episode 364/1000 | Return: -113.25 | Avg(10): -64.30 | Length: 99\n",
      "Episode 365/1000 | Return: -5.28 | Avg(10): -63.35 | Length: 100\n",
      "Episode 366/1000 | Return: -110.30 | Avg(10): -72.80 | Length: 71\n",
      "Episode 367/1000 | Return: -9.29 | Avg(10): -61.94 | Length: 100\n",
      "Episode 368/1000 | Return: -6.48 | Avg(10): -62.40 | Length: 100\n",
      "Episode 369/1000 | Return: -11.81 | Avg(10): -51.94 | Length: 100\n",
      "Episode 370/1000 | Return: -125.89 | Avg(10): -52.18 | Length: 94\n",
      "  Losses - World: 0.029, Q1: 2.915, Policy: 4.212, Alpha: 0.006\n",
      "  Losses - World: 2.014, Q1: 2.042, Policy: 3.046, Alpha: 0.005\n",
      "Episode 371/1000 | Return: -4.14 | Avg(10): -51.48 | Length: 100\n",
      "Episode 372/1000 | Return: -106.12 | Avg(10): -50.81 | Length: 51\n",
      "Episode 373/1000 | Return: -9.83 | Avg(10): -50.24 | Length: 100\n",
      "Episode 374/1000 | Return: -10.44 | Avg(10): -39.96 | Length: 100\n",
      "Episode 375/1000 | Return: -2.85 | Avg(10): -39.72 | Length: 100\n",
      "Episode 376/1000 | Return: -23.17 | Avg(10): -31.00 | Length: 100\n",
      "Episode 377/1000 | Return: -110.67 | Avg(10): -41.14 | Length: 49\n",
      "Episode 378/1000 | Return: -0.30 | Avg(10): -40.52 | Length: 100\n",
      "Episode 379/1000 | Return: -98.79 | Avg(10): -49.22 | Length: 74\n",
      "Episode 380/1000 | Return: -10.20 | Avg(10): -37.65 | Length: 100\n",
      "  Losses - World: 0.024, Q1: 8.607, Policy: 1.567, Alpha: 0.006\n",
      "  Losses - World: 0.016, Q1: 1.535, Policy: -0.229, Alpha: 0.006\n",
      "Episode 381/1000 | Return: -12.09 | Avg(10): -38.45 | Length: 100\n",
      "Episode 382/1000 | Return: -109.39 | Avg(10): -38.77 | Length: 60\n",
      "Episode 383/1000 | Return: -9.78 | Avg(10): -38.77 | Length: 100\n",
      "Episode 384/1000 | Return: 1.83 | Avg(10): -37.54 | Length: 100\n",
      "Episode 385/1000 | Return: -4.83 | Avg(10): -37.74 | Length: 100\n",
      "Episode 386/1000 | Return: -102.91 | Avg(10): -45.71 | Length: 90\n",
      "Episode 387/1000 | Return: -2.40 | Avg(10): -34.89 | Length: 100\n",
      "Episode 388/1000 | Return: -116.45 | Avg(10): -46.50 | Length: 96\n",
      "Episode 389/1000 | Return: -15.51 | Avg(10): -38.17 | Length: 100\n",
      "Episode 390/1000 | Return: 1.97 | Avg(10): -36.96 | Length: 100\n",
      "  Losses - World: 1.802, Q1: 8.869, Policy: 7.267, Alpha: 0.007\n",
      "Episode 391/1000 | Return: -122.77 | Avg(10): -48.03 | Length: 78\n",
      "Episode 392/1000 | Return: -16.04 | Avg(10): -38.69 | Length: 100\n",
      "Episode 393/1000 | Return: -11.38 | Avg(10): -38.85 | Length: 100\n",
      "Episode 394/1000 | Return: -16.35 | Avg(10): -40.67 | Length: 100\n",
      "Episode 395/1000 | Return: -0.60 | Avg(10): -40.24 | Length: 100\n",
      "Episode 396/1000 | Return: -109.07 | Avg(10): -40.86 | Length: 50\n",
      "Episode 397/1000 | Return: -9.16 | Avg(10): -41.53 | Length: 100\n",
      "Episode 398/1000 | Return: -9.41 | Avg(10): -30.83 | Length: 100\n",
      "Episode 399/1000 | Return: -5.07 | Avg(10): -29.79 | Length: 100\n",
      "Episode 400/1000 | Return: -102.88 | Avg(10): -40.27 | Length: 61\n",
      "  Losses - World: 72.993, Q1: 24.529, Policy: 7.421, Alpha: 0.008\n",
      "  Losses - World: 0.018, Q1: 8.739, Policy: 2.141, Alpha: 0.008\n",
      "Episode 401/1000 | Return: -10.15 | Avg(10): -29.01 | Length: 100\n",
      "Episode 402/1000 | Return: -15.08 | Avg(10): -28.91 | Length: 100\n",
      "Episode 403/1000 | Return: -11.37 | Avg(10): -28.91 | Length: 100\n",
      "Episode 404/1000 | Return: -107.56 | Avg(10): -38.03 | Length: 72\n",
      "Episode 405/1000 | Return: -115.67 | Avg(10): -49.54 | Length: 69\n",
      "Episode 406/1000 | Return: -4.29 | Avg(10): -39.06 | Length: 100\n",
      "Episode 407/1000 | Return: -14.32 | Avg(10): -39.58 | Length: 100\n",
      "Episode 408/1000 | Return: -6.18 | Avg(10): -39.26 | Length: 100\n",
      "Episode 409/1000 | Return: -110.30 | Avg(10): -49.78 | Length: 63\n",
      "Episode 410/1000 | Return: -13.02 | Avg(10): -40.79 | Length: 100\n",
      "  Losses - World: 154.963, Q1: 8.766, Policy: 5.419, Alpha: 0.008\n",
      "Episode 411/1000 | Return: -112.43 | Avg(10): -51.02 | Length: 84\n",
      "Episode 412/1000 | Return: -102.02 | Avg(10): -59.72 | Length: 50\n",
      "Episode 413/1000 | Return: -101.58 | Avg(10): -68.74 | Length: 56\n",
      "Episode 414/1000 | Return: -112.04 | Avg(10): -69.18 | Length: 92\n",
      "Episode 415/1000 | Return: -102.35 | Avg(10): -67.85 | Length: 100\n",
      "Episode 416/1000 | Return: -9.05 | Avg(10): -68.33 | Length: 100\n",
      "Episode 417/1000 | Return: -5.71 | Avg(10): -67.47 | Length: 100\n",
      "Episode 418/1000 | Return: -14.11 | Avg(10): -68.26 | Length: 100\n",
      "Episode 419/1000 | Return: -8.79 | Avg(10): -58.11 | Length: 100\n",
      "Episode 420/1000 | Return: -121.04 | Avg(10): -68.91 | Length: 88\n",
      "  Losses - World: 5.410, Q1: 18.995, Policy: 6.158, Alpha: 0.008\n",
      "  Losses - World: 1.398, Q1: 20.055, Policy: 2.527, Alpha: 0.008\n",
      "Episode 421/1000 | Return: -3.81 | Avg(10): -58.05 | Length: 100\n",
      "Episode 422/1000 | Return: -102.48 | Avg(10): -58.10 | Length: 62\n",
      "Episode 423/1000 | Return: -6.00 | Avg(10): -48.54 | Length: 100\n",
      "Episode 424/1000 | Return: -31.85 | Avg(10): -40.52 | Length: 100\n",
      "Episode 425/1000 | Return: -115.37 | Avg(10): -41.82 | Length: 71\n",
      "Episode 426/1000 | Return: -122.93 | Avg(10): -53.21 | Length: 99\n",
      "Episode 427/1000 | Return: -120.56 | Avg(10): -64.69 | Length: 98\n",
      "Episode 428/1000 | Return: -10.83 | Avg(10): -64.37 | Length: 100\n",
      "Episode 429/1000 | Return: -12.05 | Avg(10): -64.69 | Length: 100\n",
      "Episode 430/1000 | Return: -15.09 | Avg(10): -54.10 | Length: 100\n",
      "  Losses - World: 0.022, Q1: 5.887, Policy: 6.466, Alpha: 0.008\n",
      "  Losses - World: 0.020, Q1: 5.977, Policy: 5.824, Alpha: 0.008\n",
      "Episode 431/1000 | Return: -4.59 | Avg(10): -54.18 | Length: 100\n",
      "Episode 432/1000 | Return: -6.00 | Avg(10): -44.53 | Length: 100\n",
      "Episode 433/1000 | Return: -5.63 | Avg(10): -44.49 | Length: 100\n",
      "Episode 434/1000 | Return: -118.12 | Avg(10): -53.12 | Length: 86\n",
      "Episode 435/1000 | Return: -6.47 | Avg(10): -42.23 | Length: 100\n",
      "Episode 436/1000 | Return: -112.43 | Avg(10): -41.18 | Length: 38\n",
      "Episode 437/1000 | Return: -11.12 | Avg(10): -30.23 | Length: 100\n",
      "Episode 438/1000 | Return: -1.14 | Avg(10): -29.27 | Length: 100\n",
      "Episode 439/1000 | Return: -100.92 | Avg(10): -38.15 | Length: 98\n",
      "Episode 440/1000 | Return: -118.80 | Avg(10): -48.52 | Length: 49\n",
      "  Losses - World: 0.034, Q1: 9.380, Policy: 11.968, Alpha: 0.008\n",
      "  Losses - World: 18.549, Q1: 14.113, Policy: 8.622, Alpha: 0.008\n",
      "Episode 441/1000 | Return: -124.31 | Avg(10): -60.49 | Length: 94\n",
      "Episode 442/1000 | Return: 0.83 | Avg(10): -59.81 | Length: 100\n",
      "Episode 443/1000 | Return: -107.37 | Avg(10): -69.99 | Length: 58\n",
      "Episode 444/1000 | Return: -113.06 | Avg(10): -69.48 | Length: 38\n",
      "Episode 445/1000 | Return: -99.90 | Avg(10): -78.82 | Length: 56\n",
      "Episode 446/1000 | Return: -101.15 | Avg(10): -77.69 | Length: 49\n",
      "Episode 447/1000 | Return: -102.10 | Avg(10): -86.79 | Length: 51\n",
      "Episode 448/1000 | Return: -100.27 | Avg(10): -96.70 | Length: 52\n",
      "Episode 449/1000 | Return: -106.72 | Avg(10): -97.28 | Length: 57\n",
      "Episode 450/1000 | Return: -15.05 | Avg(10): -86.91 | Length: 100\n",
      "  Losses - World: 0.045, Q1: 8.087, Policy: 9.088, Alpha: 0.008\n",
      "  Losses - World: 31.400, Q1: 12.286, Policy: 8.704, Alpha: 0.008\n",
      "Episode 451/1000 | Return: -125.96 | Avg(10): -87.07 | Length: 70\n",
      "Episode 452/1000 | Return: -106.68 | Avg(10): -97.83 | Length: 62\n",
      "Episode 453/1000 | Return: -15.36 | Avg(10): -88.62 | Length: 100\n",
      "Episode 454/1000 | Return: 3.51 | Avg(10): -76.97 | Length: 100\n",
      "Episode 455/1000 | Return: -7.60 | Avg(10): -67.74 | Length: 100\n",
      "Episode 456/1000 | Return: -118.46 | Avg(10): -69.47 | Length: 68\n",
      "Episode 457/1000 | Return: -5.69 | Avg(10): -59.83 | Length: 100\n",
      "Episode 458/1000 | Return: -13.31 | Avg(10): -51.13 | Length: 100\n",
      "Episode 459/1000 | Return: -119.33 | Avg(10): -52.39 | Length: 81\n",
      "Episode 460/1000 | Return: -34.92 | Avg(10): -54.38 | Length: 100\n",
      "  Losses - World: 0.310, Q1: 12.236, Policy: 10.472, Alpha: 0.008\n",
      "  Losses - World: 3.340, Q1: 18.145, Policy: 7.347, Alpha: 0.008\n",
      "Episode 461/1000 | Return: -4.21 | Avg(10): -42.21 | Length: 100\n",
      "Episode 462/1000 | Return: -4.68 | Avg(10): -32.01 | Length: 100\n",
      "Episode 463/1000 | Return: -5.87 | Avg(10): -31.06 | Length: 100\n",
      "Episode 464/1000 | Return: -106.16 | Avg(10): -42.02 | Length: 99\n",
      "Episode 465/1000 | Return: -1.88 | Avg(10): -41.45 | Length: 100\n",
      "Episode 466/1000 | Return: -17.16 | Avg(10): -31.32 | Length: 100\n",
      "Episode 467/1000 | Return: -6.94 | Avg(10): -31.45 | Length: 100\n",
      "Episode 468/1000 | Return: -6.60 | Avg(10): -30.78 | Length: 100\n",
      "Episode 469/1000 | Return: -5.09 | Avg(10): -19.35 | Length: 100\n",
      "Episode 470/1000 | Return: -11.80 | Avg(10): -17.04 | Length: 100\n",
      "  Losses - World: 0.032, Q1: 17.136, Policy: 9.911, Alpha: 0.009\n",
      "  Losses - World: 0.018, Q1: 8.708, Policy: 6.029, Alpha: 0.009\n",
      "Episode 471/1000 | Return: -106.98 | Avg(10): -27.32 | Length: 100\n",
      "Episode 472/1000 | Return: -7.20 | Avg(10): -27.57 | Length: 100\n",
      "Episode 473/1000 | Return: 2.83 | Avg(10): -26.70 | Length: 100\n",
      "Episode 474/1000 | Return: -100.15 | Avg(10): -26.10 | Length: 56\n",
      "Episode 475/1000 | Return: -110.70 | Avg(10): -36.98 | Length: 65\n",
      "Episode 476/1000 | Return: -108.33 | Avg(10): -46.10 | Length: 69\n",
      "Episode 477/1000 | Return: -105.43 | Avg(10): -55.95 | Length: 75\n",
      "Episode 478/1000 | Return: -1.39 | Avg(10): -55.43 | Length: 100\n",
      "Episode 479/1000 | Return: -102.40 | Avg(10): -65.16 | Length: 98\n",
      "Episode 480/1000 | Return: -102.00 | Avg(10): -74.18 | Length: 86\n",
      "  Losses - World: 0.035, Q1: 7.534, Policy: 13.309, Alpha: 0.008\n",
      "  Losses - World: 71.622, Q1: 11.380, Policy: 11.700, Alpha: 0.008\n",
      "Episode 481/1000 | Return: -101.99 | Avg(10): -73.68 | Length: 81\n",
      "Episode 482/1000 | Return: -96.80 | Avg(10): -82.64 | Length: 85\n",
      "Episode 483/1000 | Return: -7.67 | Avg(10): -83.69 | Length: 100\n",
      "Episode 484/1000 | Return: -101.39 | Avg(10): -83.81 | Length: 88\n",
      "Episode 485/1000 | Return: -2.01 | Avg(10): -72.94 | Length: 100\n",
      "Episode 486/1000 | Return: -19.79 | Avg(10): -64.09 | Length: 100\n",
      "Episode 487/1000 | Return: 8.66 | Avg(10): -52.68 | Length: 100\n",
      "Episode 488/1000 | Return: -100.50 | Avg(10): -62.59 | Length: 53\n",
      "Episode 489/1000 | Return: -5.00 | Avg(10): -52.85 | Length: 100\n",
      "Episode 490/1000 | Return: -99.97 | Avg(10): -52.65 | Length: 70\n",
      "  Losses - World: 46.871, Q1: 16.155, Policy: 12.610, Alpha: 0.008\n",
      "  Losses - World: 2.786, Q1: 6.434, Policy: 10.587, Alpha: 0.008\n",
      "Episode 491/1000 | Return: -100.22 | Avg(10): -52.47 | Length: 72\n",
      "Episode 492/1000 | Return: -105.82 | Avg(10): -53.37 | Length: 70\n",
      "Episode 493/1000 | Return: -0.38 | Avg(10): -52.64 | Length: 100\n",
      "Episode 494/1000 | Return: -105.80 | Avg(10): -53.08 | Length: 72\n",
      "Episode 495/1000 | Return: -99.12 | Avg(10): -62.79 | Length: 99\n",
      "Episode 496/1000 | Return: -11.16 | Avg(10): -61.93 | Length: 100\n",
      "Episode 497/1000 | Return: -9.23 | Avg(10): -63.72 | Length: 100\n",
      "Episode 498/1000 | Return: -104.44 | Avg(10): -64.11 | Length: 88\n",
      "Episode 499/1000 | Return: -99.13 | Avg(10): -73.53 | Length: 71\n",
      "Episode 500/1000 | Return: -16.46 | Avg(10): -65.17 | Length: 100\n",
      "  Losses - World: 0.018, Q1: 18.490, Policy: 13.240, Alpha: 0.007\n",
      "Episode 501/1000 | Return: -99.91 | Avg(10): -65.14 | Length: 75\n",
      "Episode 502/1000 | Return: -99.62 | Avg(10): -64.52 | Length: 86\n",
      "Episode 503/1000 | Return: -102.62 | Avg(10): -74.75 | Length: 72\n",
      "Episode 504/1000 | Return: -102.91 | Avg(10): -74.46 | Length: 63\n",
      "Episode 505/1000 | Return: -17.32 | Avg(10): -66.28 | Length: 100\n",
      "Episode 506/1000 | Return: -100.36 | Avg(10): -75.20 | Length: 83\n",
      "Episode 507/1000 | Return: -100.93 | Avg(10): -84.37 | Length: 98\n",
      "Episode 508/1000 | Return: -99.44 | Avg(10): -83.87 | Length: 80\n",
      "Episode 509/1000 | Return: -103.33 | Avg(10): -84.29 | Length: 75\n",
      "Episode 510/1000 | Return: -13.60 | Avg(10): -84.00 | Length: 100\n",
      "  Losses - World: 0.727, Q1: 13.773, Policy: 10.547, Alpha: 0.008\n",
      "Episode 511/1000 | Return: -106.03 | Avg(10): -84.62 | Length: 52\n",
      "Episode 512/1000 | Return: -8.15 | Avg(10): -75.47 | Length: 100\n",
      "Episode 513/1000 | Return: -109.62 | Avg(10): -76.17 | Length: 78\n",
      "Episode 514/1000 | Return: -103.93 | Avg(10): -76.27 | Length: 66\n",
      "Episode 515/1000 | Return: -26.75 | Avg(10): -77.22 | Length: 100\n",
      "Episode 516/1000 | Return: -107.65 | Avg(10): -77.94 | Length: 93\n",
      "Episode 517/1000 | Return: -112.32 | Avg(10): -79.08 | Length: 75\n",
      "Episode 518/1000 | Return: -104.05 | Avg(10): -79.54 | Length: 56\n",
      "Episode 519/1000 | Return: -1.02 | Avg(10): -69.31 | Length: 100\n",
      "Episode 520/1000 | Return: -9.26 | Avg(10): -68.88 | Length: 100\n",
      "  Losses - World: 0.015, Q1: 12.644, Policy: 9.726, Alpha: 0.008\n",
      "Episode 521/1000 | Return: -98.75 | Avg(10): -68.15 | Length: 68\n",
      "Episode 522/1000 | Return: -99.40 | Avg(10): -77.28 | Length: 51\n",
      "Episode 523/1000 | Return: -100.23 | Avg(10): -76.34 | Length: 54\n",
      "Episode 524/1000 | Return: -105.57 | Avg(10): -76.50 | Length: 59\n",
      "Episode 525/1000 | Return: -9.24 | Avg(10): -74.75 | Length: 100\n",
      "Episode 526/1000 | Return: -103.84 | Avg(10): -74.37 | Length: 42\n",
      "Episode 527/1000 | Return: -106.88 | Avg(10): -73.82 | Length: 54\n",
      "Episode 528/1000 | Return: -7.58 | Avg(10): -64.18 | Length: 100\n",
      "Episode 529/1000 | Return: -106.96 | Avg(10): -74.77 | Length: 59\n",
      "Episode 530/1000 | Return: -6.81 | Avg(10): -74.53 | Length: 100\n",
      "  Losses - World: 24.707, Q1: 15.211, Policy: 13.959, Alpha: 0.009\n",
      "  Losses - World: 1.284, Q1: 13.323, Policy: 18.030, Alpha: 0.009\n",
      "Episode 531/1000 | Return: -2.85 | Avg(10): -64.94 | Length: 100\n",
      "Episode 532/1000 | Return: -4.08 | Avg(10): -55.40 | Length: 100\n",
      "Episode 533/1000 | Return: -6.06 | Avg(10): -45.99 | Length: 100\n",
      "Episode 534/1000 | Return: -15.66 | Avg(10): -37.00 | Length: 100\n",
      "Episode 535/1000 | Return: -95.96 | Avg(10): -45.67 | Length: 77\n",
      "Episode 536/1000 | Return: -7.25 | Avg(10): -36.01 | Length: 100\n",
      "Episode 537/1000 | Return: 1.50 | Avg(10): -25.17 | Length: 100\n",
      "Episode 538/1000 | Return: -2.73 | Avg(10): -24.69 | Length: 100\n",
      "Episode 539/1000 | Return: -13.11 | Avg(10): -15.30 | Length: 100\n",
      "Episode 540/1000 | Return: -24.36 | Avg(10): -17.06 | Length: 100\n",
      "  Losses - World: 1.473, Q1: 6.268, Policy: 6.794, Alpha: 0.010\n",
      "  Losses - World: 0.020, Q1: 7.881, Policy: 10.926, Alpha: 0.010\n",
      "Episode 541/1000 | Return: -2.95 | Avg(10): -17.07 | Length: 100\n",
      "Episode 542/1000 | Return: -103.85 | Avg(10): -27.04 | Length: 52\n",
      "Episode 543/1000 | Return: -8.57 | Avg(10): -27.29 | Length: 100\n",
      "Episode 544/1000 | Return: -102.60 | Avg(10): -35.99 | Length: 66\n",
      "Episode 545/1000 | Return: -99.83 | Avg(10): -36.37 | Length: 73\n",
      "Episode 546/1000 | Return: -96.90 | Avg(10): -45.34 | Length: 69\n",
      "Episode 547/1000 | Return: -5.39 | Avg(10): -46.03 | Length: 100\n",
      "Episode 548/1000 | Return: -13.27 | Avg(10): -47.08 | Length: 100\n",
      "Episode 549/1000 | Return: -104.55 | Avg(10): -56.23 | Length: 62\n",
      "Episode 550/1000 | Return: -3.45 | Avg(10): -54.14 | Length: 100\n",
      "  Losses - World: 1.639, Q1: 7.475, Policy: 14.713, Alpha: 0.009\n",
      "Episode 551/1000 | Return: -106.77 | Avg(10): -64.52 | Length: 68\n",
      "Episode 552/1000 | Return: -3.76 | Avg(10): -54.51 | Length: 100\n",
      "Episode 553/1000 | Return: -101.14 | Avg(10): -63.77 | Length: 83\n",
      "Episode 554/1000 | Return: -108.46 | Avg(10): -64.35 | Length: 54\n",
      "Episode 555/1000 | Return: -104.66 | Avg(10): -64.84 | Length: 77\n",
      "Episode 556/1000 | Return: -1.73 | Avg(10): -55.32 | Length: 100\n",
      "Episode 557/1000 | Return: -3.17 | Avg(10): -55.10 | Length: 100\n",
      "Episode 558/1000 | Return: -110.01 | Avg(10): -64.77 | Length: 44\n",
      "Episode 559/1000 | Return: -131.13 | Avg(10): -67.43 | Length: 82\n",
      "Episode 560/1000 | Return: -129.66 | Avg(10): -80.05 | Length: 97\n",
      "  Losses - World: 1.868, Q1: 5.907, Policy: 14.132, Alpha: 0.010\n",
      "Episode 561/1000 | Return: -98.73 | Avg(10): -79.24 | Length: 58\n",
      "Episode 562/1000 | Return: -4.40 | Avg(10): -79.31 | Length: 100\n",
      "Episode 563/1000 | Return: -120.03 | Avg(10): -81.20 | Length: 79\n",
      "Episode 564/1000 | Return: -10.91 | Avg(10): -71.44 | Length: 100\n",
      "Episode 565/1000 | Return: -20.19 | Avg(10): -63.00 | Length: 100\n",
      "Episode 566/1000 | Return: -10.42 | Avg(10): -63.86 | Length: 100\n",
      "Episode 567/1000 | Return: -109.67 | Avg(10): -74.51 | Length: 95\n",
      "Episode 568/1000 | Return: -6.63 | Avg(10): -64.18 | Length: 100\n",
      "Episode 569/1000 | Return: -9.04 | Avg(10): -51.97 | Length: 100\n",
      "Episode 570/1000 | Return: -8.84 | Avg(10): -39.89 | Length: 100\n",
      "  Losses - World: 1.102, Q1: 6.913, Policy: 16.082, Alpha: 0.010\n",
      "  Losses - World: 0.028, Q1: 14.482, Policy: 16.416, Alpha: 0.010\n",
      "Episode 571/1000 | Return: -3.60 | Avg(10): -30.37 | Length: 100\n",
      "Episode 572/1000 | Return: -120.87 | Avg(10): -42.02 | Length: 68\n",
      "Episode 573/1000 | Return: -3.93 | Avg(10): -30.41 | Length: 100\n",
      "Episode 574/1000 | Return: -118.78 | Avg(10): -41.20 | Length: 95\n",
      "Episode 575/1000 | Return: -2.20 | Avg(10): -39.40 | Length: 100\n",
      "Episode 576/1000 | Return: -111.83 | Avg(10): -49.54 | Length: 49\n",
      "Episode 577/1000 | Return: -118.44 | Avg(10): -50.41 | Length: 78\n",
      "Episode 578/1000 | Return: -118.85 | Avg(10): -61.64 | Length: 50\n",
      "Episode 579/1000 | Return: -118.88 | Avg(10): -72.62 | Length: 58\n",
      "Episode 580/1000 | Return: -113.67 | Avg(10): -83.10 | Length: 68\n",
      "  Losses - World: 0.020, Q1: 17.553, Policy: 17.534, Alpha: 0.010\n",
      "  Losses - World: 0.023, Q1: 11.201, Policy: 17.466, Alpha: 0.010\n",
      "Episode 581/1000 | Return: -7.84 | Avg(10): -83.53 | Length: 100\n",
      "Episode 582/1000 | Return: -11.99 | Avg(10): -72.64 | Length: 100\n",
      "Episode 583/1000 | Return: -5.88 | Avg(10): -72.84 | Length: 100\n",
      "Episode 584/1000 | Return: -10.71 | Avg(10): -62.03 | Length: 100\n",
      "Episode 585/1000 | Return: -1.35 | Avg(10): -61.94 | Length: 100\n",
      "Episode 586/1000 | Return: -5.75 | Avg(10): -51.34 | Length: 100\n",
      "Episode 587/1000 | Return: -16.01 | Avg(10): -41.09 | Length: 100\n",
      "Episode 588/1000 | Return: -15.21 | Avg(10): -30.73 | Length: 100\n",
      "Episode 589/1000 | Return: -0.82 | Avg(10): -18.92 | Length: 100\n",
      "Episode 590/1000 | Return: -7.19 | Avg(10): -8.28 | Length: 100\n",
      "  Losses - World: 0.028, Q1: 4.388, Policy: 13.857, Alpha: 0.010\n",
      "  Losses - World: 0.080, Q1: 5.495, Policy: 8.253, Alpha: 0.010\n",
      "Episode 591/1000 | Return: -3.19 | Avg(10): -7.81 | Length: 100\n",
      "Episode 592/1000 | Return: -12.87 | Avg(10): -7.90 | Length: 100\n",
      "Episode 593/1000 | Return: -8.25 | Avg(10): -8.13 | Length: 100\n",
      "Episode 594/1000 | Return: -103.30 | Avg(10): -17.39 | Length: 68\n",
      "Episode 595/1000 | Return: -8.04 | Avg(10): -18.06 | Length: 100\n",
      "Episode 596/1000 | Return: 0.48 | Avg(10): -17.44 | Length: 100\n",
      "Episode 597/1000 | Return: -8.80 | Avg(10): -16.72 | Length: 100\n",
      "Episode 598/1000 | Return: -0.41 | Avg(10): -15.24 | Length: 100\n",
      "Episode 599/1000 | Return: -7.44 | Avg(10): -15.90 | Length: 100\n",
      "Episode 600/1000 | Return: -2.58 | Avg(10): -15.44 | Length: 100\n",
      "  Losses - World: 31.806, Q1: 6.216, Policy: 20.323, Alpha: 0.011\n",
      "  Losses - World: 0.022, Q1: 4.539, Policy: 17.674, Alpha: 0.011\n",
      "Episode 601/1000 | Return: -7.47 | Avg(10): -15.87 | Length: 100\n",
      "Episode 602/1000 | Return: -4.89 | Avg(10): -15.07 | Length: 100\n",
      "Episode 603/1000 | Return: -12.55 | Avg(10): -15.50 | Length: 100\n",
      "Episode 604/1000 | Return: -4.78 | Avg(10): -5.65 | Length: 100\n",
      "Episode 605/1000 | Return: -1.61 | Avg(10): -5.01 | Length: 100\n",
      "Episode 606/1000 | Return: -12.74 | Avg(10): -6.33 | Length: 100\n",
      "Episode 607/1000 | Return: -0.15 | Avg(10): -5.46 | Length: 100\n",
      "Episode 608/1000 | Return: -7.58 | Avg(10): -6.18 | Length: 100\n",
      "Episode 609/1000 | Return: -10.85 | Avg(10): -6.52 | Length: 100\n",
      "Episode 610/1000 | Return: -9.48 | Avg(10): -7.21 | Length: 100\n",
      "  Losses - World: 59.553, Q1: 10.934, Policy: 21.593, Alpha: 0.010\n",
      "Episode 611/1000 | Return: -105.50 | Avg(10): -17.01 | Length: 62\n",
      "Episode 612/1000 | Return: -103.36 | Avg(10): -26.86 | Length: 96\n",
      "Episode 613/1000 | Return: 1.50 | Avg(10): -25.45 | Length: 100\n",
      "Episode 614/1000 | Return: -104.49 | Avg(10): -35.43 | Length: 84\n",
      "Episode 615/1000 | Return: -7.94 | Avg(10): -36.06 | Length: 100\n",
      "Episode 616/1000 | Return: -3.12 | Avg(10): -35.10 | Length: 100\n",
      "Episode 617/1000 | Return: -22.43 | Avg(10): -37.32 | Length: 100\n",
      "Episode 618/1000 | Return: -10.37 | Avg(10): -37.60 | Length: 100\n",
      "Episode 619/1000 | Return: -3.38 | Avg(10): -36.86 | Length: 100\n",
      "Episode 620/1000 | Return: -2.72 | Avg(10): -36.18 | Length: 100\n",
      "  Losses - World: 0.018, Q1: 8.365, Policy: 17.202, Alpha: 0.011\n",
      "Episode 621/1000 | Return: -112.68 | Avg(10): -36.90 | Length: 51\n",
      "Episode 622/1000 | Return: -117.51 | Avg(10): -38.31 | Length: 68\n",
      "Episode 623/1000 | Return: -14.10 | Avg(10): -39.87 | Length: 100\n",
      "Episode 624/1000 | Return: -4.35 | Avg(10): -29.86 | Length: 100\n",
      "Episode 625/1000 | Return: -9.40 | Avg(10): -30.01 | Length: 100\n",
      "Episode 626/1000 | Return: -6.41 | Avg(10): -30.34 | Length: 100\n",
      "Episode 627/1000 | Return: -8.73 | Avg(10): -28.97 | Length: 100\n",
      "Episode 628/1000 | Return: -14.46 | Avg(10): -29.38 | Length: 100\n",
      "Episode 629/1000 | Return: -8.24 | Avg(10): -29.86 | Length: 100\n",
      "Episode 630/1000 | Return: -11.45 | Avg(10): -30.73 | Length: 100\n",
      "  Losses - World: 29.170, Q1: 18.274, Policy: 20.269, Alpha: 0.010\n",
      "  Losses - World: 0.254, Q1: 17.912, Policy: 21.440, Alpha: 0.010\n",
      "Episode 631/1000 | Return: -9.86 | Avg(10): -20.45 | Length: 100\n",
      "Episode 632/1000 | Return: -8.42 | Avg(10): -9.54 | Length: 100\n",
      "Episode 633/1000 | Return: -10.13 | Avg(10): -9.15 | Length: 100\n",
      "Episode 634/1000 | Return: -8.42 | Avg(10): -9.55 | Length: 100\n",
      "Episode 635/1000 | Return: -101.90 | Avg(10): -18.80 | Length: 75\n",
      "Episode 636/1000 | Return: -100.12 | Avg(10): -28.17 | Length: 73\n",
      "Episode 637/1000 | Return: -101.89 | Avg(10): -37.49 | Length: 82\n",
      "Episode 638/1000 | Return: -103.21 | Avg(10): -46.36 | Length: 90\n",
      "Episode 639/1000 | Return: -7.92 | Avg(10): -46.33 | Length: 100\n",
      "Episode 640/1000 | Return: 0.36 | Avg(10): -45.15 | Length: 100\n",
      "  Losses - World: 4.290, Q1: 9.429, Policy: 22.357, Alpha: 0.011\n",
      "  Losses - World: 0.019, Q1: 11.174, Policy: 20.873, Alpha: 0.011\n",
      "Episode 641/1000 | Return: -4.80 | Avg(10): -44.65 | Length: 100\n",
      "Episode 642/1000 | Return: -5.48 | Avg(10): -44.35 | Length: 100\n",
      "Episode 643/1000 | Return: -106.00 | Avg(10): -53.94 | Length: 84\n",
      "Episode 644/1000 | Return: -9.09 | Avg(10): -54.01 | Length: 100\n",
      "Episode 645/1000 | Return: -7.38 | Avg(10): -44.55 | Length: 100\n",
      "Episode 646/1000 | Return: -4.40 | Avg(10): -34.98 | Length: 100\n",
      "Episode 647/1000 | Return: -106.25 | Avg(10): -35.42 | Length: 92\n",
      "Episode 648/1000 | Return: -103.48 | Avg(10): -35.44 | Length: 70\n",
      "Episode 649/1000 | Return: -112.74 | Avg(10): -45.93 | Length: 65\n",
      "Episode 650/1000 | Return: 4.36 | Avg(10): -45.53 | Length: 100\n",
      "  Losses - World: 0.048, Q1: 17.270, Policy: 22.599, Alpha: 0.012\n",
      "Episode 651/1000 | Return: -115.98 | Avg(10): -56.64 | Length: 59\n",
      "Episode 652/1000 | Return: -12.95 | Avg(10): -57.39 | Length: 100\n",
      "Episode 653/1000 | Return: -120.16 | Avg(10): -58.81 | Length: 94\n",
      "Episode 654/1000 | Return: -108.65 | Avg(10): -68.76 | Length: 68\n",
      "Episode 655/1000 | Return: -102.03 | Avg(10): -78.23 | Length: 62\n",
      "Episode 656/1000 | Return: -118.26 | Avg(10): -89.61 | Length: 72\n",
      "Episode 657/1000 | Return: 1.10 | Avg(10): -78.88 | Length: 100\n",
      "Episode 658/1000 | Return: -9.45 | Avg(10): -69.48 | Length: 100\n",
      "Episode 659/1000 | Return: -8.62 | Avg(10): -59.06 | Length: 100\n",
      "Episode 660/1000 | Return: -3.14 | Avg(10): -59.81 | Length: 100\n",
      "  Losses - World: 0.271, Q1: 12.494, Policy: 28.956, Alpha: 0.014\n",
      "  Losses - World: 1.099, Q1: 13.913, Policy: 31.648, Alpha: 0.014\n",
      "Episode 661/1000 | Return: -10.41 | Avg(10): -49.26 | Length: 100\n",
      "Episode 662/1000 | Return: -9.33 | Avg(10): -48.90 | Length: 100\n",
      "Episode 663/1000 | Return: -12.21 | Avg(10): -38.10 | Length: 100\n",
      "Episode 664/1000 | Return: -99.70 | Avg(10): -37.21 | Length: 83\n",
      "Episode 665/1000 | Return: 3.66 | Avg(10): -26.64 | Length: 100\n",
      "Episode 666/1000 | Return: -12.02 | Avg(10): -16.01 | Length: 100\n",
      "Episode 667/1000 | Return: -2.71 | Avg(10): -16.39 | Length: 100\n",
      "Episode 668/1000 | Return: -0.14 | Avg(10): -15.46 | Length: 100\n",
      "Episode 669/1000 | Return: -7.57 | Avg(10): -15.36 | Length: 100\n",
      "Episode 670/1000 | Return: -0.82 | Avg(10): -15.12 | Length: 100\n",
      "  Losses - World: 0.021, Q1: 23.032, Policy: 31.038, Alpha: 0.017\n",
      "Episode 671/1000 | Return: -105.79 | Avg(10): -24.66 | Length: 60\n",
      "Episode 672/1000 | Return: -111.06 | Avg(10): -34.83 | Length: 72\n",
      "Episode 673/1000 | Return: -15.60 | Avg(10): -35.17 | Length: 100\n",
      "Episode 674/1000 | Return: -7.56 | Avg(10): -25.96 | Length: 100\n",
      "Episode 675/1000 | Return: -12.06 | Avg(10): -27.53 | Length: 100\n",
      "Episode 676/1000 | Return: -4.82 | Avg(10): -26.81 | Length: 100\n",
      "Episode 677/1000 | Return: -101.59 | Avg(10): -36.70 | Length: 78\n",
      "Episode 678/1000 | Return: -10.89 | Avg(10): -37.78 | Length: 100\n",
      "Episode 679/1000 | Return: -105.13 | Avg(10): -47.53 | Length: 65\n",
      "Episode 680/1000 | Return: -2.46 | Avg(10): -47.70 | Length: 100\n",
      "  Losses - World: 0.502, Q1: 17.479, Policy: 26.342, Alpha: 0.017\n",
      "  Losses - World: 0.655, Q1: 16.727, Policy: 32.466, Alpha: 0.017\n",
      "Episode 681/1000 | Return: -12.48 | Avg(10): -38.37 | Length: 100\n",
      "Episode 682/1000 | Return: 0.75 | Avg(10): -27.18 | Length: 100\n",
      "Episode 683/1000 | Return: -119.99 | Avg(10): -37.62 | Length: 92\n",
      "Episode 684/1000 | Return: -132.79 | Avg(10): -50.15 | Length: 81\n",
      "Episode 685/1000 | Return: -11.38 | Avg(10): -50.08 | Length: 100\n",
      "Episode 686/1000 | Return: -104.46 | Avg(10): -60.04 | Length: 59\n",
      "Episode 687/1000 | Return: -8.72 | Avg(10): -50.75 | Length: 100\n",
      "Episode 688/1000 | Return: -7.53 | Avg(10): -50.42 | Length: 100\n",
      "Episode 689/1000 | Return: -107.51 | Avg(10): -50.66 | Length: 47\n",
      "Episode 690/1000 | Return: -0.51 | Avg(10): -50.46 | Length: 100\n",
      "  Losses - World: 3.303, Q1: 10.526, Policy: 37.809, Alpha: 0.017\n",
      "  Losses - World: 0.071, Q1: 13.579, Policy: 42.153, Alpha: 0.017\n",
      "Episode 691/1000 | Return: -8.84 | Avg(10): -50.10 | Length: 100\n",
      "Episode 692/1000 | Return: -8.77 | Avg(10): -51.05 | Length: 100\n",
      "Episode 693/1000 | Return: -3.73 | Avg(10): -39.43 | Length: 100\n",
      "Episode 694/1000 | Return: -104.42 | Avg(10): -36.59 | Length: 74\n",
      "Episode 695/1000 | Return: -98.09 | Avg(10): -45.26 | Length: 71\n",
      "Episode 696/1000 | Return: -105.47 | Avg(10): -45.36 | Length: 60\n",
      "Episode 697/1000 | Return: 0.31 | Avg(10): -44.46 | Length: 100\n",
      "Episode 698/1000 | Return: -102.09 | Avg(10): -53.91 | Length: 94\n",
      "Episode 699/1000 | Return: -5.96 | Avg(10): -43.76 | Length: 100\n",
      "Episode 700/1000 | Return: -0.25 | Avg(10): -43.73 | Length: 100\n",
      "  Losses - World: 0.060, Q1: 8.725, Policy: 31.357, Alpha: 0.019\n",
      "  Losses - World: 0.017, Q1: 9.456, Policy: 36.638, Alpha: 0.019\n",
      "Episode 701/1000 | Return: -4.99 | Avg(10): -43.35 | Length: 100\n",
      "Episode 702/1000 | Return: -102.33 | Avg(10): -52.70 | Length: 83\n",
      "Episode 703/1000 | Return: 1.74 | Avg(10): -52.15 | Length: 100\n",
      "Episode 704/1000 | Return: -108.17 | Avg(10): -52.53 | Length: 78\n",
      "Episode 705/1000 | Return: -112.82 | Avg(10): -54.00 | Length: 62\n",
      "Episode 706/1000 | Return: -0.99 | Avg(10): -43.55 | Length: 100\n",
      "Episode 707/1000 | Return: -108.62 | Avg(10): -54.45 | Length: 83\n",
      "Episode 708/1000 | Return: -15.17 | Avg(10): -45.76 | Length: 100\n",
      "Episode 709/1000 | Return: -12.90 | Avg(10): -46.45 | Length: 100\n",
      "Episode 710/1000 | Return: -117.07 | Avg(10): -58.13 | Length: 78\n",
      "  Losses - World: 0.021, Q1: 14.310, Policy: 48.544, Alpha: 0.020\n",
      "  Losses - World: 0.021, Q1: 7.784, Policy: 42.205, Alpha: 0.020\n",
      "Episode 711/1000 | Return: -118.69 | Avg(10): -69.50 | Length: 65\n",
      "Episode 712/1000 | Return: -11.19 | Avg(10): -60.39 | Length: 100\n",
      "Episode 713/1000 | Return: -7.59 | Avg(10): -61.32 | Length: 100\n",
      "Episode 714/1000 | Return: -10.67 | Avg(10): -51.57 | Length: 100\n",
      "Episode 715/1000 | Return: -121.37 | Avg(10): -52.43 | Length: 93\n",
      "Episode 716/1000 | Return: -8.92 | Avg(10): -53.22 | Length: 100\n",
      "Episode 717/1000 | Return: -9.27 | Avg(10): -43.28 | Length: 100\n",
      "Episode 718/1000 | Return: -109.02 | Avg(10): -52.67 | Length: 80\n",
      "Episode 719/1000 | Return: -6.16 | Avg(10): -51.99 | Length: 100\n",
      "Episode 720/1000 | Return: -8.61 | Avg(10): -41.15 | Length: 100\n",
      "  Losses - World: 10.885, Q1: 10.907, Policy: 32.157, Alpha: 0.017\n",
      "  Losses - World: 0.019, Q1: 16.975, Policy: 38.755, Alpha: 0.017\n",
      "Episode 721/1000 | Return: -9.53 | Avg(10): -30.23 | Length: 100\n",
      "Episode 722/1000 | Return: -4.44 | Avg(10): -29.56 | Length: 100\n",
      "Episode 723/1000 | Return: -10.17 | Avg(10): -29.82 | Length: 100\n",
      "Episode 724/1000 | Return: -8.88 | Avg(10): -29.64 | Length: 100\n",
      "Episode 725/1000 | Return: -104.22 | Avg(10): -27.92 | Length: 83\n",
      "Episode 726/1000 | Return: -100.81 | Avg(10): -37.11 | Length: 82\n",
      "Episode 727/1000 | Return: -10.47 | Avg(10): -37.23 | Length: 100\n",
      "Episode 728/1000 | Return: -103.62 | Avg(10): -36.69 | Length: 55\n",
      "Episode 729/1000 | Return: -118.72 | Avg(10): -47.95 | Length: 99\n",
      "Episode 730/1000 | Return: -114.00 | Avg(10): -58.49 | Length: 60\n",
      "  Losses - World: 1.507, Q1: 12.053, Policy: 40.758, Alpha: 0.017\n",
      "  Losses - World: 0.117, Q1: 8.183, Policy: 40.292, Alpha: 0.017\n",
      "Episode 731/1000 | Return: -5.49 | Avg(10): -58.08 | Length: 100\n",
      "Episode 732/1000 | Return: -97.27 | Avg(10): -67.37 | Length: 69\n",
      "Episode 733/1000 | Return: -108.95 | Avg(10): -77.24 | Length: 69\n",
      "Episode 734/1000 | Return: -112.48 | Avg(10): -87.60 | Length: 90\n",
      "Episode 735/1000 | Return: -105.28 | Avg(10): -87.71 | Length: 70\n",
      "Episode 736/1000 | Return: -105.13 | Avg(10): -88.14 | Length: 51\n",
      "Episode 737/1000 | Return: -107.16 | Avg(10): -97.81 | Length: 91\n",
      "Episode 738/1000 | Return: -101.37 | Avg(10): -97.59 | Length: 63\n",
      "Episode 739/1000 | Return: -106.71 | Avg(10): -96.38 | Length: 83\n",
      "Episode 740/1000 | Return: -8.78 | Avg(10): -85.86 | Length: 100\n",
      "  Losses - World: 0.195, Q1: 6.351, Policy: 41.646, Alpha: 0.019\n",
      "  Losses - World: 0.021, Q1: 3.777, Policy: 41.995, Alpha: 0.019\n",
      "Episode 741/1000 | Return: -9.25 | Avg(10): -86.24 | Length: 100\n",
      "Episode 742/1000 | Return: -12.33 | Avg(10): -77.74 | Length: 100\n",
      "Episode 743/1000 | Return: -123.53 | Avg(10): -79.20 | Length: 94\n",
      "Episode 744/1000 | Return: -112.36 | Avg(10): -79.19 | Length: 70\n",
      "Episode 745/1000 | Return: -120.98 | Avg(10): -80.76 | Length: 71\n",
      "Episode 746/1000 | Return: -102.69 | Avg(10): -80.52 | Length: 79\n",
      "Episode 747/1000 | Return: -4.40 | Avg(10): -70.24 | Length: 100\n",
      "Episode 748/1000 | Return: -3.32 | Avg(10): -60.43 | Length: 100\n",
      "Episode 749/1000 | Return: -13.82 | Avg(10): -51.15 | Length: 100\n",
      "Episode 750/1000 | Return: -3.84 | Avg(10): -50.65 | Length: 100\n",
      "  Losses - World: 0.022, Q1: 4.429, Policy: 43.826, Alpha: 0.021\n",
      "  Losses - World: 0.015, Q1: 4.769, Policy: 41.600, Alpha: 0.022\n",
      "Episode 751/1000 | Return: 1.99 | Avg(10): -49.53 | Length: 100\n",
      "Episode 752/1000 | Return: -102.25 | Avg(10): -58.52 | Length: 96\n",
      "Episode 753/1000 | Return: -104.62 | Avg(10): -56.63 | Length: 69\n",
      "Episode 754/1000 | Return: -106.63 | Avg(10): -56.06 | Length: 73\n",
      "Episode 755/1000 | Return: -3.44 | Avg(10): -44.30 | Length: 100\n",
      "Episode 756/1000 | Return: -104.95 | Avg(10): -44.53 | Length: 66\n",
      "Episode 757/1000 | Return: 0.66 | Avg(10): -44.02 | Length: 100\n",
      "Episode 758/1000 | Return: -14.72 | Avg(10): -45.16 | Length: 100\n",
      "Episode 759/1000 | Return: -101.71 | Avg(10): -53.95 | Length: 95\n",
      "Episode 760/1000 | Return: -106.30 | Avg(10): -64.20 | Length: 52\n",
      "  Losses - World: 0.203, Q1: 7.048, Policy: 38.992, Alpha: 0.023\n",
      "Episode 761/1000 | Return: -116.22 | Avg(10): -76.02 | Length: 62\n",
      "Episode 762/1000 | Return: 1.09 | Avg(10): -65.68 | Length: 100\n",
      "Episode 763/1000 | Return: -100.03 | Avg(10): -65.23 | Length: 67\n",
      "Episode 764/1000 | Return: -109.21 | Avg(10): -65.48 | Length: 67\n",
      "Episode 765/1000 | Return: -7.92 | Avg(10): -65.93 | Length: 100\n",
      "Episode 766/1000 | Return: -102.80 | Avg(10): -65.72 | Length: 50\n",
      "Episode 767/1000 | Return: -107.23 | Avg(10): -76.51 | Length: 62\n",
      "Episode 768/1000 | Return: -108.00 | Avg(10): -85.83 | Length: 84\n",
      "Episode 769/1000 | Return: -19.60 | Avg(10): -77.62 | Length: 100\n",
      "Episode 770/1000 | Return: -101.49 | Avg(10): -77.14 | Length: 56\n",
      "  Losses - World: 0.021, Q1: 4.425, Policy: 39.785, Alpha: 0.020\n",
      "Episode 771/1000 | Return: -102.51 | Avg(10): -75.77 | Length: 82\n",
      "Episode 772/1000 | Return: -100.51 | Avg(10): -85.93 | Length: 71\n",
      "Episode 773/1000 | Return: -4.11 | Avg(10): -76.34 | Length: 100\n",
      "Episode 774/1000 | Return: 2.02 | Avg(10): -65.21 | Length: 100\n",
      "Episode 775/1000 | Return: -3.46 | Avg(10): -64.77 | Length: 100\n",
      "Episode 776/1000 | Return: -5.33 | Avg(10): -55.02 | Length: 100\n",
      "Episode 777/1000 | Return: -4.32 | Avg(10): -44.73 | Length: 100\n",
      "Episode 778/1000 | Return: -8.69 | Avg(10): -34.80 | Length: 100\n",
      "Episode 779/1000 | Return: -2.22 | Avg(10): -33.06 | Length: 100\n",
      "Episode 780/1000 | Return: -5.30 | Avg(10): -23.44 | Length: 100\n",
      "  Losses - World: 45.660, Q1: 6.526, Policy: 34.453, Alpha: 0.017\n",
      "  Losses - World: 0.160, Q1: 6.775, Policy: 37.859, Alpha: 0.017\n",
      "Episode 781/1000 | Return: -5.12 | Avg(10): -13.70 | Length: 100\n",
      "Episode 782/1000 | Return: -4.98 | Avg(10): -4.15 | Length: 100\n",
      "Episode 783/1000 | Return: -5.38 | Avg(10): -4.28 | Length: 100\n",
      "Episode 784/1000 | Return: -4.31 | Avg(10): -4.91 | Length: 100\n",
      "Episode 785/1000 | Return: -8.38 | Avg(10): -5.40 | Length: 100\n",
      "Episode 786/1000 | Return: -10.30 | Avg(10): -5.90 | Length: 100\n",
      "Episode 787/1000 | Return: -7.28 | Avg(10): -6.20 | Length: 100\n",
      "Episode 788/1000 | Return: -8.64 | Avg(10): -6.19 | Length: 100\n",
      "Episode 789/1000 | Return: -4.74 | Avg(10): -6.44 | Length: 100\n",
      "Episode 790/1000 | Return: -7.18 | Avg(10): -6.63 | Length: 100\n",
      "  Losses - World: 0.016, Q1: 5.002, Policy: 38.773, Alpha: 0.015\n",
      "  Losses - World: 0.011, Q1: 4.537, Policy: 36.492, Alpha: 0.015\n",
      "Episode 791/1000 | Return: -13.16 | Avg(10): -7.43 | Length: 100\n",
      "Episode 792/1000 | Return: -7.56 | Avg(10): -7.69 | Length: 100\n",
      "Episode 793/1000 | Return: -11.76 | Avg(10): -8.33 | Length: 100\n",
      "Episode 794/1000 | Return: -5.25 | Avg(10): -8.42 | Length: 100\n",
      "Episode 795/1000 | Return: -9.57 | Avg(10): -8.54 | Length: 100\n",
      "Episode 796/1000 | Return: -4.20 | Avg(10): -7.93 | Length: 100\n",
      "Episode 797/1000 | Return: -4.93 | Avg(10): -7.70 | Length: 100\n",
      "Episode 798/1000 | Return: -2.13 | Avg(10): -7.05 | Length: 100\n",
      "Episode 799/1000 | Return: -110.30 | Avg(10): -17.60 | Length: 95\n",
      "Episode 800/1000 | Return: -5.70 | Avg(10): -17.46 | Length: 100\n",
      "  Losses - World: 2.766, Q1: 10.536, Policy: 38.285, Alpha: 0.015\n",
      "  Losses - World: 0.022, Q1: 4.270, Policy: 34.856, Alpha: 0.015\n",
      "Episode 801/1000 | Return: 0.36 | Avg(10): -16.10 | Length: 100\n",
      "Episode 802/1000 | Return: -105.11 | Avg(10): -25.86 | Length: 66\n",
      "Episode 803/1000 | Return: -115.73 | Avg(10): -36.25 | Length: 71\n",
      "Episode 804/1000 | Return: -113.68 | Avg(10): -47.10 | Length: 74\n",
      "Episode 805/1000 | Return: -106.28 | Avg(10): -56.77 | Length: 54\n",
      "Episode 806/1000 | Return: -126.42 | Avg(10): -68.99 | Length: 93\n",
      "Episode 807/1000 | Return: -112.76 | Avg(10): -79.77 | Length: 90\n",
      "Episode 808/1000 | Return: -109.93 | Avg(10): -90.56 | Length: 57\n",
      "Episode 809/1000 | Return: -9.76 | Avg(10): -80.50 | Length: 100\n",
      "Episode 810/1000 | Return: -114.27 | Avg(10): -91.36 | Length: 83\n",
      "  Losses - World: 71.460, Q1: 8.766, Policy: 37.364, Alpha: 0.015\n",
      "  Losses - World: 0.105, Q1: 8.880, Policy: 39.054, Alpha: 0.015\n",
      "Episode 811/1000 | Return: -11.50 | Avg(10): -92.54 | Length: 100\n",
      "Episode 812/1000 | Return: -9.63 | Avg(10): -83.00 | Length: 100\n",
      "Episode 813/1000 | Return: -11.28 | Avg(10): -72.55 | Length: 100\n",
      "Episode 814/1000 | Return: -12.03 | Avg(10): -62.39 | Length: 100\n",
      "Episode 815/1000 | Return: -105.50 | Avg(10): -62.31 | Length: 69\n",
      "Episode 816/1000 | Return: -13.87 | Avg(10): -51.05 | Length: 100\n",
      "Episode 817/1000 | Return: -8.95 | Avg(10): -40.67 | Length: 100\n",
      "Episode 818/1000 | Return: -7.18 | Avg(10): -30.40 | Length: 100\n",
      "Episode 819/1000 | Return: -2.83 | Avg(10): -29.70 | Length: 100\n",
      "Episode 820/1000 | Return: -8.54 | Avg(10): -19.13 | Length: 100\n",
      "  Losses - World: 0.017, Q1: 3.166, Policy: 33.218, Alpha: 0.013\n",
      "  Losses - World: 0.034, Q1: 4.705, Policy: 37.059, Alpha: 0.013\n",
      "Episode 821/1000 | Return: -2.01 | Avg(10): -18.18 | Length: 100\n",
      "Episode 822/1000 | Return: -37.03 | Avg(10): -20.92 | Length: 100\n",
      "Episode 823/1000 | Return: -104.21 | Avg(10): -30.21 | Length: 82\n",
      "Episode 824/1000 | Return: -6.35 | Avg(10): -29.65 | Length: 100\n",
      "Episode 825/1000 | Return: -127.86 | Avg(10): -31.88 | Length: 100\n",
      "Episode 826/1000 | Return: -104.72 | Avg(10): -40.97 | Length: 75\n",
      "Episode 827/1000 | Return: -6.84 | Avg(10): -40.76 | Length: 100\n",
      "Episode 828/1000 | Return: -118.68 | Avg(10): -51.91 | Length: 90\n",
      "Episode 829/1000 | Return: -7.32 | Avg(10): -52.35 | Length: 100\n",
      "Episode 830/1000 | Return: -104.66 | Avg(10): -61.97 | Length: 96\n",
      "  Losses - World: 29.993, Q1: 19.581, Policy: 37.736, Alpha: 0.011\n",
      "  Losses - World: 43.994, Q1: 6.820, Policy: 36.435, Alpha: 0.011\n",
      "Episode 831/1000 | Return: -114.76 | Avg(10): -73.24 | Length: 99\n",
      "Episode 832/1000 | Return: -6.29 | Avg(10): -70.17 | Length: 100\n",
      "Episode 833/1000 | Return: -16.23 | Avg(10): -61.37 | Length: 100\n",
      "Episode 834/1000 | Return: -115.05 | Avg(10): -72.24 | Length: 88\n",
      "Episode 835/1000 | Return: -113.50 | Avg(10): -70.80 | Length: 50\n",
      "Episode 836/1000 | Return: -113.68 | Avg(10): -71.70 | Length: 46\n",
      "Episode 837/1000 | Return: -13.49 | Avg(10): -72.37 | Length: 100\n",
      "Episode 838/1000 | Return: -111.00 | Avg(10): -71.60 | Length: 42\n",
      "Episode 839/1000 | Return: -112.42 | Avg(10): -82.11 | Length: 92\n",
      "Episode 840/1000 | Return: -115.75 | Avg(10): -83.22 | Length: 79\n",
      "  Losses - World: 0.029, Q1: 10.636, Policy: 42.162, Alpha: 0.014\n",
      "  Losses - World: 0.022, Q1: 8.798, Policy: 46.231, Alpha: 0.014\n",
      "Episode 841/1000 | Return: -16.15 | Avg(10): -73.36 | Length: 100\n",
      "Episode 842/1000 | Return: -106.56 | Avg(10): -83.38 | Length: 48\n",
      "Episode 843/1000 | Return: -100.87 | Avg(10): -91.85 | Length: 65\n",
      "Episode 844/1000 | Return: -5.82 | Avg(10): -80.93 | Length: 100\n",
      "Episode 845/1000 | Return: -102.71 | Avg(10): -79.85 | Length: 66\n",
      "Episode 846/1000 | Return: -105.06 | Avg(10): -78.98 | Length: 60\n",
      "Episode 847/1000 | Return: -110.54 | Avg(10): -88.69 | Length: 63\n",
      "Episode 848/1000 | Return: -13.13 | Avg(10): -78.90 | Length: 100\n",
      "Episode 849/1000 | Return: -103.48 | Avg(10): -78.01 | Length: 66\n",
      "Episode 850/1000 | Return: -107.77 | Avg(10): -77.21 | Length: 64\n",
      "  Losses - World: 0.021, Q1: 4.852, Policy: 38.610, Alpha: 0.015\n",
      "  Losses - World: 0.276, Q1: 4.375, Policy: 35.509, Alpha: 0.015\n",
      "Episode 851/1000 | Return: -16.47 | Avg(10): -77.24 | Length: 100\n",
      "Episode 852/1000 | Return: -10.28 | Avg(10): -67.61 | Length: 100\n",
      "Episode 853/1000 | Return: -107.41 | Avg(10): -68.27 | Length: 63\n",
      "Episode 854/1000 | Return: -4.83 | Avg(10): -68.17 | Length: 100\n",
      "Episode 855/1000 | Return: -15.55 | Avg(10): -59.45 | Length: 100\n",
      "Episode 856/1000 | Return: -6.68 | Avg(10): -49.61 | Length: 100\n",
      "Episode 857/1000 | Return: -105.14 | Avg(10): -49.07 | Length: 68\n",
      "Episode 858/1000 | Return: -100.78 | Avg(10): -57.84 | Length: 64\n",
      "Episode 859/1000 | Return: -15.82 | Avg(10): -49.07 | Length: 100\n",
      "Episode 860/1000 | Return: -106.95 | Avg(10): -48.99 | Length: 80\n",
      "  Losses - World: 72.710, Q1: 4.142, Policy: 36.019, Alpha: 0.013\n",
      "  Losses - World: 0.029, Q1: 19.820, Policy: 39.219, Alpha: 0.013\n",
      "Episode 861/1000 | Return: -9.88 | Avg(10): -48.33 | Length: 100\n",
      "Episode 862/1000 | Return: -114.03 | Avg(10): -58.71 | Length: 60\n",
      "Episode 863/1000 | Return: -104.46 | Avg(10): -58.41 | Length: 64\n",
      "Episode 864/1000 | Return: -4.93 | Avg(10): -58.42 | Length: 100\n",
      "Episode 865/1000 | Return: -8.43 | Avg(10): -57.71 | Length: 100\n",
      "Episode 866/1000 | Return: -9.78 | Avg(10): -58.02 | Length: 100\n",
      "Episode 867/1000 | Return: -8.64 | Avg(10): -48.37 | Length: 100\n",
      "Episode 868/1000 | Return: -10.49 | Avg(10): -39.34 | Length: 100\n",
      "Episode 869/1000 | Return: -122.50 | Avg(10): -50.01 | Length: 94\n",
      "Episode 870/1000 | Return: -119.50 | Avg(10): -51.26 | Length: 46\n",
      "  Losses - World: 13.027, Q1: 11.534, Policy: 38.391, Alpha: 0.012\n",
      "  Losses - World: 0.016, Q1: 6.027, Policy: 40.445, Alpha: 0.012\n",
      "Episode 871/1000 | Return: -0.03 | Avg(10): -50.28 | Length: 100\n",
      "Episode 872/1000 | Return: -25.69 | Avg(10): -41.44 | Length: 100\n",
      "Episode 873/1000 | Return: -113.19 | Avg(10): -42.32 | Length: 34\n",
      "Episode 874/1000 | Return: -10.72 | Avg(10): -42.89 | Length: 100\n",
      "Episode 875/1000 | Return: -108.43 | Avg(10): -52.90 | Length: 83\n",
      "Episode 876/1000 | Return: -8.71 | Avg(10): -52.79 | Length: 100\n",
      "Episode 877/1000 | Return: -10.83 | Avg(10): -53.01 | Length: 100\n",
      "Episode 878/1000 | Return: -4.04 | Avg(10): -52.36 | Length: 100\n",
      "Episode 879/1000 | Return: -102.71 | Avg(10): -50.38 | Length: 62\n",
      "Episode 880/1000 | Return: -21.35 | Avg(10): -40.57 | Length: 100\n",
      "  Losses - World: 39.090, Q1: 8.642, Policy: 42.748, Alpha: 0.011\n",
      "Episode 881/1000 | Return: -107.54 | Avg(10): -51.32 | Length: 58\n",
      "Episode 882/1000 | Return: -101.96 | Avg(10): -58.95 | Length: 66\n",
      "Episode 883/1000 | Return: -104.91 | Avg(10): -58.12 | Length: 50\n",
      "Episode 884/1000 | Return: -105.94 | Avg(10): -67.64 | Length: 49\n",
      "Episode 885/1000 | Return: -109.62 | Avg(10): -67.76 | Length: 78\n",
      "Episode 886/1000 | Return: -5.60 | Avg(10): -67.45 | Length: 100\n",
      "Episode 887/1000 | Return: -106.02 | Avg(10): -76.97 | Length: 73\n",
      "Episode 888/1000 | Return: -3.24 | Avg(10): -76.89 | Length: 100\n",
      "Episode 889/1000 | Return: -7.05 | Avg(10): -67.32 | Length: 100\n",
      "Episode 890/1000 | Return: -7.22 | Avg(10): -65.91 | Length: 100\n",
      "  Losses - World: 0.010, Q1: 6.891, Policy: 38.274, Alpha: 0.010\n",
      "Episode 891/1000 | Return: -102.25 | Avg(10): -65.38 | Length: 58\n",
      "Episode 892/1000 | Return: -102.28 | Avg(10): -65.41 | Length: 86\n",
      "Episode 893/1000 | Return: -104.65 | Avg(10): -65.39 | Length: 53\n",
      "Episode 894/1000 | Return: 1.74 | Avg(10): -54.62 | Length: 100\n",
      "Episode 895/1000 | Return: -107.38 | Avg(10): -54.40 | Length: 54\n",
      "Episode 896/1000 | Return: -15.34 | Avg(10): -55.37 | Length: 100\n",
      "Episode 897/1000 | Return: -3.62 | Avg(10): -45.13 | Length: 100\n",
      "Episode 898/1000 | Return: -4.60 | Avg(10): -45.27 | Length: 100\n",
      "Episode 899/1000 | Return: -106.64 | Avg(10): -55.22 | Length: 60\n",
      "Episode 900/1000 | Return: -7.02 | Avg(10): -55.20 | Length: 100\n",
      "  Losses - World: 0.153, Q1: 6.879, Policy: 40.821, Alpha: 0.010\n",
      "  Losses - World: 0.039, Q1: 5.794, Policy: 43.476, Alpha: 0.010\n",
      "Episode 901/1000 | Return: -12.78 | Avg(10): -46.26 | Length: 100\n",
      "Episode 902/1000 | Return: -100.82 | Avg(10): -46.11 | Length: 78\n",
      "Episode 903/1000 | Return: -24.62 | Avg(10): -38.11 | Length: 100\n",
      "Episode 904/1000 | Return: -116.49 | Avg(10): -49.93 | Length: 85\n",
      "Episode 905/1000 | Return: -11.08 | Avg(10): -40.30 | Length: 100\n",
      "Episode 906/1000 | Return: -12.83 | Avg(10): -40.05 | Length: 100\n",
      "Episode 907/1000 | Return: -109.98 | Avg(10): -50.69 | Length: 70\n",
      "Episode 908/1000 | Return: -122.76 | Avg(10): -62.50 | Length: 97\n",
      "Episode 909/1000 | Return: -107.27 | Avg(10): -62.57 | Length: 89\n",
      "Episode 910/1000 | Return: -8.86 | Avg(10): -62.75 | Length: 100\n",
      "  Losses - World: 0.040, Q1: 4.464, Policy: 35.030, Alpha: 0.010\n",
      "  Losses - World: 1.491, Q1: 11.477, Policy: 39.942, Alpha: 0.010\n",
      "Episode 911/1000 | Return: -5.58 | Avg(10): -62.03 | Length: 100\n",
      "Episode 912/1000 | Return: -6.28 | Avg(10): -52.58 | Length: 100\n",
      "Episode 913/1000 | Return: -2.90 | Avg(10): -50.40 | Length: 100\n",
      "Episode 914/1000 | Return: -103.58 | Avg(10): -49.11 | Length: 93\n",
      "Episode 915/1000 | Return: -11.65 | Avg(10): -49.17 | Length: 100\n",
      "Episode 916/1000 | Return: -2.97 | Avg(10): -48.18 | Length: 100\n",
      "Episode 917/1000 | Return: -4.56 | Avg(10): -37.64 | Length: 100\n",
      "Episode 918/1000 | Return: -5.49 | Avg(10): -25.91 | Length: 100\n",
      "Episode 919/1000 | Return: -7.50 | Avg(10): -15.94 | Length: 100\n",
      "Episode 920/1000 | Return: -0.02 | Avg(10): -15.05 | Length: 100\n",
      "  Losses - World: 12.301, Q1: 8.381, Policy: 41.877, Alpha: 0.011\n",
      "Episode 921/1000 | Return: -107.75 | Avg(10): -25.27 | Length: 97\n",
      "Episode 922/1000 | Return: -103.60 | Avg(10): -35.00 | Length: 56\n",
      "Episode 923/1000 | Return: -8.80 | Avg(10): -35.59 | Length: 100\n",
      "Episode 924/1000 | Return: -7.86 | Avg(10): -26.02 | Length: 100\n",
      "Episode 925/1000 | Return: -115.80 | Avg(10): -36.43 | Length: 82\n",
      "Episode 926/1000 | Return: -12.35 | Avg(10): -37.37 | Length: 100\n",
      "Episode 927/1000 | Return: -3.55 | Avg(10): -37.27 | Length: 100\n",
      "Episode 928/1000 | Return: 6.94 | Avg(10): -36.03 | Length: 100\n",
      "Episode 929/1000 | Return: -6.15 | Avg(10): -35.89 | Length: 100\n",
      "Episode 930/1000 | Return: -11.87 | Avg(10): -37.08 | Length: 100\n",
      "  Losses - World: 0.027, Q1: 4.628, Policy: 39.297, Alpha: 0.012\n",
      "  Losses - World: 0.023, Q1: 3.513, Policy: 40.739, Alpha: 0.012\n",
      "Episode 931/1000 | Return: -12.60 | Avg(10): -27.56 | Length: 100\n",
      "Episode 932/1000 | Return: -108.25 | Avg(10): -28.03 | Length: 84\n",
      "Episode 933/1000 | Return: -3.09 | Avg(10): -27.46 | Length: 100\n",
      "Episode 934/1000 | Return: -9.26 | Avg(10): -27.60 | Length: 100\n",
      "Episode 935/1000 | Return: -3.27 | Avg(10): -16.34 | Length: 100\n",
      "Episode 936/1000 | Return: -107.97 | Avg(10): -25.91 | Length: 68\n",
      "Episode 937/1000 | Return: -5.16 | Avg(10): -26.07 | Length: 100\n",
      "Episode 938/1000 | Return: -8.63 | Avg(10): -27.62 | Length: 100\n",
      "Episode 939/1000 | Return: -16.07 | Avg(10): -28.62 | Length: 100\n",
      "Episode 940/1000 | Return: -121.56 | Avg(10): -39.59 | Length: 65\n",
      "  Losses - World: 0.015, Q1: 5.060, Policy: 37.101, Alpha: 0.011\n",
      "  Losses - World: 13.751, Q1: 8.874, Policy: 33.427, Alpha: 0.011\n",
      "Episode 941/1000 | Return: -3.15 | Avg(10): -38.64 | Length: 100\n",
      "Episode 942/1000 | Return: -1.77 | Avg(10): -27.99 | Length: 100\n",
      "Episode 943/1000 | Return: -4.16 | Avg(10): -28.10 | Length: 100\n",
      "Episode 944/1000 | Return: -100.96 | Avg(10): -37.27 | Length: 70\n",
      "Episode 945/1000 | Return: -7.67 | Avg(10): -37.71 | Length: 100\n",
      "Episode 946/1000 | Return: -3.08 | Avg(10): -27.22 | Length: 100\n",
      "Episode 947/1000 | Return: -2.18 | Avg(10): -26.92 | Length: 100\n",
      "Episode 948/1000 | Return: -104.12 | Avg(10): -36.47 | Length: 77\n",
      "Episode 949/1000 | Return: -106.14 | Avg(10): -45.48 | Length: 72\n",
      "Episode 950/1000 | Return: -15.38 | Avg(10): -34.86 | Length: 100\n",
      "  Losses - World: 0.588, Q1: 5.093, Policy: 39.952, Alpha: 0.012\n",
      "Episode 951/1000 | Return: -115.09 | Avg(10): -46.06 | Length: 55\n",
      "Episode 952/1000 | Return: -103.49 | Avg(10): -56.23 | Length: 67\n",
      "Episode 953/1000 | Return: -16.06 | Avg(10): -57.42 | Length: 100\n",
      "Episode 954/1000 | Return: -2.33 | Avg(10): -47.55 | Length: 100\n",
      "Episode 955/1000 | Return: -5.40 | Avg(10): -47.33 | Length: 100\n",
      "Episode 956/1000 | Return: -7.33 | Avg(10): -47.75 | Length: 100\n",
      "Episode 957/1000 | Return: -5.38 | Avg(10): -48.07 | Length: 100\n",
      "Episode 958/1000 | Return: 4.44 | Avg(10): -37.21 | Length: 100\n",
      "Episode 959/1000 | Return: -122.51 | Avg(10): -38.85 | Length: 87\n",
      "Episode 960/1000 | Return: -9.09 | Avg(10): -38.22 | Length: 100\n",
      "  Losses - World: 0.017, Q1: 3.983, Policy: 39.387, Alpha: 0.010\n",
      "  Losses - World: 0.016, Q1: 5.327, Policy: 38.784, Alpha: 0.010\n",
      "Episode 961/1000 | Return: -122.34 | Avg(10): -38.95 | Length: 73\n",
      "Episode 962/1000 | Return: -1.20 | Avg(10): -28.72 | Length: 100\n",
      "Episode 963/1000 | Return: -116.84 | Avg(10): -38.80 | Length: 95\n",
      "Episode 964/1000 | Return: -9.08 | Avg(10): -39.47 | Length: 100\n",
      "Episode 965/1000 | Return: -112.51 | Avg(10): -50.18 | Length: 52\n",
      "Episode 966/1000 | Return: -98.97 | Avg(10): -59.35 | Length: 93\n",
      "Episode 967/1000 | Return: -21.87 | Avg(10): -61.00 | Length: 100\n",
      "Episode 968/1000 | Return: -9.56 | Avg(10): -62.40 | Length: 100\n",
      "Episode 969/1000 | Return: -13.57 | Avg(10): -51.50 | Length: 100\n",
      "Episode 970/1000 | Return: -114.28 | Avg(10): -62.02 | Length: 100\n",
      "  Losses - World: 14.514, Q1: 10.933, Policy: 42.541, Alpha: 0.013\n",
      "  Losses - World: 0.019, Q1: 10.838, Policy: 40.850, Alpha: 0.013\n",
      "Episode 971/1000 | Return: -117.82 | Avg(10): -61.57 | Length: 73\n",
      "Episode 972/1000 | Return: -121.84 | Avg(10): -73.64 | Length: 92\n",
      "Episode 973/1000 | Return: -116.97 | Avg(10): -73.65 | Length: 65\n",
      "Episode 974/1000 | Return: -100.25 | Avg(10): -82.76 | Length: 97\n",
      "Episode 975/1000 | Return: -101.54 | Avg(10): -81.67 | Length: 71\n",
      "Episode 976/1000 | Return: -7.53 | Avg(10): -72.52 | Length: 100\n",
      "Episode 977/1000 | Return: -9.56 | Avg(10): -71.29 | Length: 100\n",
      "Episode 978/1000 | Return: -102.82 | Avg(10): -80.62 | Length: 48\n",
      "Episode 979/1000 | Return: -108.80 | Avg(10): -90.14 | Length: 56\n",
      "Episode 980/1000 | Return: 8.13 | Avg(10): -77.90 | Length: 100\n",
      "  Losses - World: 0.015, Q1: 6.491, Policy: 41.298, Alpha: 0.014\n",
      "  Losses - World: 0.317, Q1: 7.132, Policy: 43.142, Alpha: 0.014\n",
      "Episode 981/1000 | Return: -7.81 | Avg(10): -66.90 | Length: 100\n",
      "Episode 982/1000 | Return: -4.92 | Avg(10): -55.21 | Length: 100\n",
      "Episode 983/1000 | Return: -6.20 | Avg(10): -44.13 | Length: 100\n",
      "Episode 984/1000 | Return: -13.87 | Avg(10): -35.49 | Length: 100\n",
      "Episode 985/1000 | Return: -6.43 | Avg(10): -25.98 | Length: 100\n",
      "Episode 986/1000 | Return: -7.77 | Avg(10): -26.00 | Length: 100\n",
      "Episode 987/1000 | Return: -4.39 | Avg(10): -25.49 | Length: 100\n",
      "Episode 988/1000 | Return: -101.45 | Avg(10): -25.35 | Length: 74\n",
      "Episode 989/1000 | Return: -6.17 | Avg(10): -15.09 | Length: 100\n",
      "Episode 990/1000 | Return: -4.29 | Avg(10): -16.33 | Length: 100\n",
      "  Losses - World: 0.010, Q1: 4.713, Policy: 37.713, Alpha: 0.014\n",
      "  Losses - World: 0.202, Q1: 5.805, Policy: 37.682, Alpha: 0.014\n",
      "Episode 991/1000 | Return: -23.95 | Avg(10): -17.94 | Length: 100\n",
      "Episode 992/1000 | Return: -19.94 | Avg(10): -19.45 | Length: 100\n",
      "Episode 993/1000 | Return: -130.32 | Avg(10): -31.86 | Length: 89\n",
      "Episode 994/1000 | Return: -129.16 | Avg(10): -43.39 | Length: 93\n",
      "Episode 995/1000 | Return: -32.55 | Avg(10): -46.00 | Length: 100\n",
      "Episode 996/1000 | Return: -104.62 | Avg(10): -55.68 | Length: 76\n",
      "Episode 997/1000 | Return: -0.49 | Avg(10): -55.29 | Length: 100\n",
      "Episode 998/1000 | Return: -16.10 | Avg(10): -46.76 | Length: 100\n",
      "Episode 999/1000 | Return: -105.30 | Avg(10): -56.67 | Length: 49\n",
      "Episode 1000/1000 | Return: -10.54 | Avg(10): -57.30 | Length: 100\n",
      "\n",
      "Training finished!\n",
      "Final average return (last 10 eps): -57.30\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from collections import deque\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, obs_dim, act_dim, size):\n",
    "        self.obs_buf = np.zeros((size, obs_dim), dtype=np.float32)\n",
    "        self.next_obs_buf = np.zeros((size, obs_dim), dtype=np.float32)\n",
    "        self.acts_buf = np.zeros((size, act_dim), dtype=np.float32)\n",
    "        self.rews_buf = np.zeros(size, dtype=np.float32)\n",
    "        self.done_buf = np.zeros(size, dtype=np.float32)\n",
    "        self.ptr, self.size, self.max_size = 0, 0, size\n",
    "\n",
    "    def store(self, obs, act, rew, next_obs, done):\n",
    "        self.obs_buf[self.ptr] = obs\n",
    "        self.acts_buf[self.ptr] = act\n",
    "        self.rews_buf[self.ptr] = rew\n",
    "        self.next_obs_buf[self.ptr] = next_obs\n",
    "        self.done_buf[self.ptr] = done\n",
    "        self.ptr = (self.ptr + 1) % self.max_size\n",
    "        self.size = min(self.size + 1, self.max_size)\n",
    "\n",
    "    def sample_batch(self, batch_size=64):\n",
    "        idxs = np.random.randint(0, self.size, size=batch_size)\n",
    "        batch = dict(\n",
    "            obs=self.obs_buf[idxs],\n",
    "            acts=self.acts_buf[idxs],\n",
    "            rews=self.rews_buf[idxs],\n",
    "            next_obs=self.next_obs_buf[idxs],\n",
    "            done=self.done_buf[idxs],\n",
    "        )\n",
    "        return {k: torch.as_tensor(v, dtype=torch.float32) for k, v in batch.items()}\n",
    "\n",
    "\n",
    "def mlp(in_dim, out_dim, hidden_sizes=(256, 256), activation=nn.ReLU):\n",
    "    layers = []\n",
    "    last_dim = in_dim\n",
    "    for h in hidden_sizes:\n",
    "        layers.append(nn.Linear(last_dim, h))\n",
    "        layers.append(activation())\n",
    "        last_dim = h\n",
    "    layers.append(nn.Linear(last_dim, out_dim))\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "class GaussianPolicy(nn.Module):\n",
    "   \n",
    "    def __init__(self, latent_dim, act_dim, act_low, act_high, hidden_sizes=(256, 256)):\n",
    "        super().__init__()\n",
    "        self.net = mlp(latent_dim, 2 * act_dim, hidden_sizes)\n",
    "        self.act_dim = act_dim\n",
    "\n",
    "        self.register_buffer(\"act_low\", torch.as_tensor(act_low, dtype=torch.float32))\n",
    "        self.register_buffer(\"act_high\", torch.as_tensor(act_high, dtype=torch.float32))\n",
    "\n",
    "        self.LOG_STD_MIN = -20\n",
    "        self.LOG_STD_MAX = 2\n",
    "\n",
    "    def forward(self, z):\n",
    "        mean_logstd = self.net(z)\n",
    "        mean, log_std = torch.chunk(mean_logstd, 2, dim=-1)\n",
    "        log_std = torch.clamp(log_std, self.LOG_STD_MIN, self.LOG_STD_MAX)\n",
    "        std = torch.exp(log_std)\n",
    "        return mean, std\n",
    "\n",
    "    def sample(self, z):\n",
    "        mean, std = self(z)\n",
    "        dist = torch.distributions.Normal(mean, std)\n",
    "        x_t = dist.rsample()\n",
    "        y_t = torch.tanh(x_t)\n",
    "\n",
    "        log_prob = dist.log_prob(x_t).sum(dim=-1, keepdim=True)\n",
    "        log_prob -= torch.sum(torch.log(1 - y_t.pow(2) + 1e-6), dim=-1, keepdim=True)\n",
    "\n",
    "        action = (self.act_high + self.act_low) / 2 + (self.act_high - self.act_low) / 2 * y_t\n",
    "        return action, log_prob\n",
    "\n",
    "    def deterministic(self, z):\n",
    "        mean, _ = self(z)\n",
    "        y_t = torch.tanh(mean)\n",
    "        action = (self.act_high + self.act_low) / 2 + (self.act_high - self.act_low) / 2 * y_t\n",
    "        return action\n",
    "\n",
    "\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, latent_dim, act_dim, hidden_sizes=(256, 256)):\n",
    "        super().__init__()\n",
    "        self.net = mlp(latent_dim + act_dim, 1, hidden_sizes)\n",
    "\n",
    "    def forward(self, z, a):\n",
    "        x = torch.cat([z, a], dim=-1)\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "\n",
    "#### KOOPMAN STUFF ####\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, obs_dim, latent_dim, hidden_sizes=(256, 256)):\n",
    "        super().__init__()\n",
    "        self.net = mlp(obs_dim, latent_dim, hidden_sizes)\n",
    "\n",
    "    def forward(self, obs):\n",
    "        return self.net(obs)\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    \"\"\"Reconstruction decoder for regularization\"\"\"\n",
    "    def __init__(self, latent_dim, obs_dim, hidden_sizes=(256, 256)):\n",
    "        super().__init__()\n",
    "        self.net = mlp(latent_dim, obs_dim, hidden_sizes)\n",
    "\n",
    "    def forward(self, z):\n",
    "        return self.net(z)\n",
    "\n",
    "\n",
    "class KoopmanLinear(nn.Module):\n",
    "  \n",
    "    def __init__(self, latent_dim, act_dim):\n",
    "        super().__init__()\n",
    "        self.K_z = nn.Linear(latent_dim, latent_dim, bias=False)\n",
    "        self.K_a = nn.Linear(act_dim, latent_dim, bias=True)\n",
    "        \n",
    "    def forward(self, z, a):\n",
    "        return self.K_z(z) + self.K_a(a)\n",
    "    \n",
    "    def multi_step(self, z, actions):\n",
    "        for a in actions:\n",
    "            z = self.forward(z, a)\n",
    "        return z\n",
    "\n",
    "\n",
    "# Reward model in the lifted space\n",
    "class RewardModel(nn.Module):\n",
    "    def __init__(self, latent_dim, act_dim, hidden_sizes=(256, 256)):\n",
    "        super().__init__()\n",
    "        self.net = mlp(latent_dim + act_dim, 1, hidden_sizes)\n",
    "\n",
    "    def forward(self, z, a):\n",
    "        x = torch.cat([z, a], dim=-1)\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Koopman + SAC Agent\n",
    "# =========================\n",
    "\n",
    "class KoopmanSACAgent:\n",
    "    def __init__(\n",
    "        self,\n",
    "        obs_dim,\n",
    "        act_dim,\n",
    "        act_low,\n",
    "        act_high,\n",
    "        latent_dim=64,\n",
    "        gamma=0.99,\n",
    "        alpha=0.2,\n",
    "        lr=3e-4,\n",
    "        device=\"cpu\",\n",
    "        H=3,\n",
    "        num_plan_samples=64,\n",
    "        reconstruction_weight=0.1,\n",
    "        koopman_weight=1.0,\n",
    "        reward_weight=1.0,\n",
    "    ):\n",
    "        self.device = device\n",
    "        self.gamma = gamma\n",
    "        self.H = H\n",
    "        self.num_plan_samples = num_plan_samples\n",
    "        self.reconstruction_weight = reconstruction_weight\n",
    "        self.koopman_weight = koopman_weight\n",
    "        self.reward_weight = reward_weight\n",
    "\n",
    "        # Koopman components\n",
    "        self.encoder = Encoder(obs_dim, latent_dim).to(device)\n",
    "        self.decoder = Decoder(latent_dim, obs_dim).to(device)\n",
    "        self.koopman = KoopmanLinear(latent_dim, act_dim).to(device)\n",
    "        self.reward_model = RewardModel(latent_dim, act_dim).to(device)\n",
    "\n",
    "        # SAC in latent space\n",
    "        self.policy = GaussianPolicy(latent_dim, act_dim, act_low, act_high).to(device)\n",
    "        self.q1 = QNetwork(latent_dim, act_dim).to(device)\n",
    "        self.q2 = QNetwork(latent_dim, act_dim).to(device)\n",
    "        self.q1_target = QNetwork(latent_dim, act_dim).to(device)\n",
    "        self.q2_target = QNetwork(latent_dim, act_dim).to(device)\n",
    "        self.q1_target.load_state_dict(self.q1.state_dict())\n",
    "        self.q2_target.load_state_dict(self.q2.state_dict())\n",
    "\n",
    "        # Joint optimizer for world model (encoder, decoder, koopman, reward)\n",
    "        world_model_params = (\n",
    "            list(self.encoder.parameters()) +\n",
    "            list(self.decoder.parameters()) +\n",
    "            list(self.koopman.parameters()) +\n",
    "            list(self.reward_model.parameters())\n",
    "        )\n",
    "        self.opt_world = optim.Adam(world_model_params, lr=lr)\n",
    "        \n",
    "        # SAC optimizers\n",
    "        self.opt_policy = optim.Adam(self.policy.parameters(), lr=lr)\n",
    "        self.opt_q1 = optim.Adam(self.q1.parameters(), lr=lr)\n",
    "        self.opt_q2 = optim.Adam(self.q2.parameters(), lr=lr)\n",
    "\n",
    "        # Entropy temperature\n",
    "        self.target_entropy = -act_dim\n",
    "        self.log_alpha = torch.tensor(np.log(alpha), requires_grad=True, device=device)\n",
    "        self.opt_alpha = optim.Adam([self.log_alpha], lr=lr)\n",
    "\n",
    "    @property\n",
    "    def alpha_val(self):\n",
    "        return self.log_alpha.exp()\n",
    "\n",
    "    # ---------- Planning API ----------\n",
    "\n",
    "    def encode(self, obs_np):\n",
    "        obs_t = torch.as_tensor(obs_np, dtype=torch.float32, device=self.device)\n",
    "        if obs_t.dim() == 1:\n",
    "            obs_t = obs_t.unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            z = self.encoder(obs_t)\n",
    "        return z\n",
    "\n",
    "    def plan_action(self, obs_np):\n",
    "        \"\"\"\n",
    "        Improved H-step lookahead planning with:\n",
    "        - Better action space exploration via adding noise\n",
    "        - More efficient trajectory evaluation\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            z0 = self.encode(obs_np)\n",
    "            z0 = z0.repeat(self.num_plan_samples, 1)\n",
    "\n",
    "            # Generate action sequences with exploration noise\n",
    "            act_seqs = []\n",
    "            for h in range(self.H):\n",
    "                a_h, _ = self.policy.sample(z0)\n",
    "                # Add exploration noise to diversify samples\n",
    "                noise = torch.randn_like(a_h) * 0.1\n",
    "                a_h = torch.clamp(a_h + noise, \n",
    "                                 self.policy.act_low.min(), \n",
    "                                 self.policy.act_high.max())\n",
    "                act_seqs.append(a_h)\n",
    "\n",
    "            # Roll out trajectories and accumulate returns\n",
    "            total_returns = torch.zeros(self.num_plan_samples, 1, device=self.device)\n",
    "            z = z0.clone()\n",
    "\n",
    "            for h in range(self.H):\n",
    "                a_h = act_seqs[h]\n",
    "                r_h = self.reward_model(z, a_h)\n",
    "                total_returns += (self.gamma ** h) * r_h\n",
    "                z = self.koopman(z, a_h)\n",
    "\n",
    "            # Terminal value from SAC\n",
    "            a_terminal, logp_term = self.policy.sample(z)\n",
    "            q1_term = self.q1(z, a_terminal)\n",
    "            q2_term = self.q2(z, a_terminal)\n",
    "            q_min = torch.min(q1_term, q2_term)\n",
    "            v_term = q_min - self.alpha_val * logp_term\n",
    "            total_returns += (self.gamma ** self.H) * v_term\n",
    "\n",
    "            # Select best trajectory\n",
    "            best_idx = torch.argmax(total_returns, dim=0).item()\n",
    "            best_a0 = act_seqs[0][best_idx:best_idx + 1]\n",
    "            return best_a0.cpu().numpy()[0]\n",
    "\n",
    "    # ---------- Training Update ----------\n",
    "\n",
    "    def update(self, batch, tau=0.005):\n",
    "        obs = batch[\"obs\"].to(self.device)\n",
    "        acts = batch[\"acts\"].to(self.device)\n",
    "        rews = batch[\"rews\"].unsqueeze(-1).to(self.device)\n",
    "        next_obs = batch[\"next_obs\"].to(self.device)\n",
    "        done = batch[\"done\"].unsqueeze(-1).to(self.device)\n",
    "\n",
    "        # ---- World Model Update (encoder receives gradients from all losses) ----\n",
    "        \n",
    "        # Encode observations\n",
    "        z = self.encoder(obs)\n",
    "        z_next_true = self.encoder(next_obs)\n",
    "        \n",
    "        # 1. Koopman prediction loss\n",
    "        z_next_pred = self.koopman(z, acts)\n",
    "        koopman_loss = F.mse_loss(z_next_pred, z_next_true)\n",
    "        \n",
    "        # 2. Reward prediction loss\n",
    "        r_pred = self.reward_model(z, acts)\n",
    "        reward_loss = F.mse_loss(r_pred, rews)\n",
    "        \n",
    "        # 3. Reconstruction loss (regularization)\n",
    "        obs_recon = self.decoder(z)\n",
    "        next_obs_recon = self.decoder(z_next_pred)\n",
    "        reconstruction_loss = (\n",
    "            F.mse_loss(obs_recon, obs) + \n",
    "            F.mse_loss(next_obs_recon, next_obs)\n",
    "        ) / 2.0\n",
    "        \n",
    "        # Combined world model loss\n",
    "        world_model_loss = (\n",
    "            self.koopman_weight * koopman_loss +\n",
    "            self.reward_weight * reward_loss +\n",
    "            self.reconstruction_weight * reconstruction_loss\n",
    "        )\n",
    "        \n",
    "        self.opt_world.zero_grad()\n",
    "        world_model_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(\n",
    "            list(self.encoder.parameters()) + \n",
    "            list(self.decoder.parameters()) + \n",
    "            list(self.koopman.parameters()) + \n",
    "            list(self.reward_model.parameters()), \n",
    "            max_norm=10.0\n",
    "        )\n",
    "        self.opt_world.step()\n",
    "\n",
    "        # ---- SAC Updates (detach to prevent backprop through world model) ----\n",
    "        with torch.no_grad():\n",
    "            z_detach = self.encoder(obs)\n",
    "            z_next_detach = self.encoder(next_obs)\n",
    "\n",
    "        # SAC critic update\n",
    "        with torch.no_grad():\n",
    "            a_next, logp_next = self.policy.sample(z_next_detach)\n",
    "            q1_next = self.q1_target(z_next_detach, a_next)\n",
    "            q2_next = self.q2_target(z_next_detach, a_next)\n",
    "            q_next_min = torch.min(q1_next, q2_next)\n",
    "            target_q = rews + (1.0 - done) * self.gamma * (q_next_min - self.alpha_val * logp_next)\n",
    "\n",
    "        q1_pred = self.q1(z_detach, acts)\n",
    "        q2_pred = self.q2(z_detach, acts)\n",
    "        q1_loss = F.mse_loss(q1_pred, target_q)\n",
    "        q2_loss = F.mse_loss(q2_pred, target_q)\n",
    "\n",
    "        self.opt_q1.zero_grad()\n",
    "        q1_loss.backward()\n",
    "        self.opt_q1.step()\n",
    "\n",
    "        self.opt_q2.zero_grad()\n",
    "        q2_loss.backward()\n",
    "        self.opt_q2.step()\n",
    "\n",
    "        # SAC policy update\n",
    "        a_pi, logp_pi = self.policy.sample(z_detach)\n",
    "        q1_pi = self.q1(z_detach, a_pi)\n",
    "        q2_pi = self.q2(z_detach, a_pi)\n",
    "        q_pi_min = torch.min(q1_pi, q2_pi)\n",
    "\n",
    "        policy_loss = (self.alpha_val * logp_pi - q_pi_min).mean()\n",
    "\n",
    "        self.opt_policy.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        self.opt_policy.step()\n",
    "\n",
    "        # Alpha update\n",
    "        alpha_loss = -(self.log_alpha * (logp_pi + self.target_entropy).detach()).mean()\n",
    "        self.opt_alpha.zero_grad()\n",
    "        alpha_loss.backward()\n",
    "        self.opt_alpha.step()\n",
    "\n",
    "        # Soft update targets\n",
    "        with torch.no_grad():\n",
    "            for p, p_targ in zip(self.q1.parameters(), self.q1_target.parameters()):\n",
    "                p_targ.data.copy_(p_targ.data * (1.0 - tau) + p.data * tau)\n",
    "            for p, p_targ in zip(self.q2.parameters(), self.q2_target.parameters()):\n",
    "                p_targ.data.copy_(p_targ.data * (1.0 - tau) + p.data * tau)\n",
    "\n",
    "        return {\n",
    "            \"koopman_loss\": koopman_loss.item(),\n",
    "            \"reward_loss\": reward_loss.item(),\n",
    "            \"reconstruction_loss\": reconstruction_loss.item(),\n",
    "            \"world_model_loss\": world_model_loss.item(),\n",
    "            \"q1_loss\": q1_loss.item(),\n",
    "            \"q2_loss\": q2_loss.item(),\n",
    "            \"policy_loss\": policy_loss.item(),\n",
    "            \"alpha\": self.alpha_val.item(),\n",
    "        }\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Training Loop\n",
    "# =========================\n",
    "\n",
    "def train_koopman_sac(\n",
    "    env_name=\"Pendulum-v1\",\n",
    "    num_episodes=200,\n",
    "    max_steps_per_episode=200,\n",
    "    replay_size=100000,\n",
    "    batch_size=128,\n",
    "    start_random_steps=1000,\n",
    "    update_after=1000,\n",
    "    update_every=50,\n",
    "    H=3,\n",
    "    num_plan_samples=64,\n",
    "    latent_dim=64,\n",
    "    seed=0,\n",
    "):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    env = gym.make(env_name)\n",
    "    assert isinstance(env.action_space, gym.spaces.Box), \"Environment must have continuous actions.\"\n",
    "\n",
    "    obs_dim = env.observation_space.shape[0]\n",
    "    act_dim = env.action_space.shape[0]\n",
    "    act_low = env.action_space.low\n",
    "    act_high = env.action_space.high\n",
    "\n",
    "    # Seeding\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "    env.reset(seed=seed)\n",
    "\n",
    "    replay_buffer = ReplayBuffer(obs_dim, act_dim, replay_size)\n",
    "    agent = KoopmanSACAgent(\n",
    "        obs_dim=obs_dim,\n",
    "        act_dim=act_dim,\n",
    "        act_low=act_low,\n",
    "        act_high=act_high,\n",
    "        latent_dim=latent_dim,\n",
    "        H=H,\n",
    "        num_plan_samples=num_plan_samples,\n",
    "        device=device,\n",
    "    )\n",
    "\n",
    "    total_steps = 0\n",
    "    returns = []\n",
    "    recent_returns = deque(maxlen=10)\n",
    "\n",
    "    for ep in range(num_episodes):\n",
    "        obs, info = env.reset()\n",
    "        ep_ret = 0.0\n",
    "        ep_len = 0\n",
    "\n",
    "        for t in range(max_steps_per_episode):\n",
    "            total_steps += 1\n",
    "            ep_len += 1\n",
    "\n",
    "            # Action selection\n",
    "            if total_steps < start_random_steps:\n",
    "                act = env.action_space.sample()\n",
    "            else:\n",
    "                act = agent.plan_action(obs)\n",
    "            act = np.asarray(act, dtype=np.float32)\n",
    "\n",
    "            next_obs, rew, terminated, truncated, info = env.step(act)\n",
    "            done = terminated or truncated\n",
    "            ep_ret += rew\n",
    "\n",
    "            replay_buffer.store(obs, act, rew, next_obs, float(done))\n",
    "            obs = next_obs\n",
    "\n",
    "            # Training updates\n",
    "            if total_steps >= update_after and total_steps % update_every == 0:\n",
    "                for _ in range(update_every):\n",
    "                    batch = replay_buffer.sample_batch(batch_size)\n",
    "                    loss_dict = agent.update(batch)\n",
    "                \n",
    "                # Print loss every 10 episodes\n",
    "                if ep % 10 == 0:\n",
    "                    print(f\"  Losses - World: {loss_dict['world_model_loss']:.3f}, \"\n",
    "                          f\"Q1: {loss_dict['q1_loss']:.3f}, \"\n",
    "                          f\"Policy: {loss_dict['policy_loss']:.3f}, \"\n",
    "                          f\"Alpha: {loss_dict['alpha']:.3f}\")\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        returns.append(ep_ret)\n",
    "        recent_returns.append(ep_ret)\n",
    "        avg_return = np.mean(recent_returns)\n",
    "        \n",
    "        print(f\"Episode {ep+1}/{num_episodes} | Return: {ep_ret:.2f} | \"\n",
    "              f\"Avg(10): {avg_return:.2f} | Length: {ep_len}\")\n",
    "\n",
    "    env.close()\n",
    "    return returns, agent\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # ENV_NAME = \"Pendulum-v1\" \n",
    "    ENV_NAME = \"BipedalWalker-v3\"  # Uncomment for harder task\n",
    "\n",
    "    returns, agent = train_koopman_sac(\n",
    "        env_name=ENV_NAME,\n",
    "        num_episodes=1000,\n",
    "        max_steps_per_episode=100 if \"Pendulum\" in ENV_NAME else 100,\n",
    "        H=3,\n",
    "        num_plan_samples=32,\n",
    "        latent_dim=64,\n",
    "        seed=42,\n",
    "    )\n",
    "    print(\"\\nTraining finished!\")\n",
    "    print(f\"Final average return (last 10 eps): {np.mean(returns[-10:]):.2f}\")\n",
    "    plt.plot(np.linspace(0, len(returns), len(returns)), returns)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d5d0878a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x21eaf4beb00>]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(np.linspace(0, len(returns), len(returns)), returns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfea38e2",
   "metadata": {},
   "source": [
    "KOOPMAN + RL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "91a2cda0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1 | Return: -1302.57 | Length: 200\n",
      "Episode 2 | Return: -919.56 | Length: 200\n",
      "Episode 3 | Return: -996.86 | Length: 200\n",
      "Episode 4 | Return: -1065.56 | Length: 200\n",
      "[Step 1000] Koopman mean prediction error: 0.022667841985821724\n",
      "Episode 5 | Return: -1743.88 | Length: 200\n",
      "Episode 6 | Return: -1454.82 | Length: 200\n",
      "Episode 7 | Return: -1718.97 | Length: 200\n",
      "Episode 8 | Return: -1764.99 | Length: 200\n",
      "Episode 9 | Return: -1649.08 | Length: 200\n",
      "[Step 2000] Koopman mean prediction error: 0.013409366831183434\n",
      "Episode 10 | Return: -1500.61 | Length: 200\n",
      "Episode 11 | Return: -1349.43 | Length: 200\n",
      "Episode 12 | Return: -1307.13 | Length: 200\n",
      "Episode 13 | Return: -1312.08 | Length: 200\n",
      "Episode 14 | Return: -1342.38 | Length: 200\n",
      "[Step 3000] Koopman mean prediction error: 0.012944946065545082\n",
      "Episode 15 | Return: -794.21 | Length: 200\n",
      "Episode 16 | Return: -1191.66 | Length: 200\n",
      "Episode 17 | Return: -1007.00 | Length: 200\n",
      "Episode 18 | Return: -889.48 | Length: 200\n",
      "Episode 19 | Return: -872.41 | Length: 200\n",
      "[Step 4000] Koopman mean prediction error: 0.014136050827801228\n",
      "Episode 20 | Return: -503.94 | Length: 200\n",
      "Episode 21 | Return: -779.48 | Length: 200\n",
      "Episode 22 | Return: -787.80 | Length: 200\n",
      "Episode 23 | Return: -915.29 | Length: 200\n",
      "Episode 24 | Return: -1062.03 | Length: 200\n",
      "[Step 5000] Koopman mean prediction error: 0.01706569269299507\n",
      "Episode 25 | Return: -1187.03 | Length: 200\n",
      "Episode 26 | Return: -579.93 | Length: 200\n",
      "Episode 27 | Return: -984.66 | Length: 200\n",
      "Episode 28 | Return: -386.70 | Length: 200\n",
      "Episode 29 | Return: -1.62 | Length: 200\n",
      "[Step 6000] Koopman mean prediction error: 0.017295286059379578\n",
      "Episode 30 | Return: -394.36 | Length: 200\n",
      "Episode 31 | Return: -2.25 | Length: 200\n",
      "Episode 32 | Return: -415.12 | Length: 200\n",
      "Episode 33 | Return: -500.84 | Length: 200\n",
      "Episode 34 | Return: -252.78 | Length: 200\n",
      "[Step 7000] Koopman mean prediction error: 0.01429002359509468\n",
      "Episode 35 | Return: -402.52 | Length: 200\n",
      "Episode 36 | Return: -250.59 | Length: 200\n",
      "Episode 37 | Return: -407.50 | Length: 200\n",
      "Episode 38 | Return: -126.91 | Length: 200\n",
      "Episode 39 | Return: -122.12 | Length: 200\n",
      "[Step 8000] Koopman mean prediction error: 0.01224492583423853\n",
      "Episode 40 | Return: -248.61 | Length: 200\n",
      "Episode 41 | Return: -122.20 | Length: 200\n",
      "Episode 42 | Return: -249.13 | Length: 200\n",
      "Episode 43 | Return: -126.19 | Length: 200\n",
      "Episode 44 | Return: -122.19 | Length: 200\n",
      "[Step 9000] Koopman mean prediction error: 0.014442226849496365\n",
      "Episode 45 | Return: -122.54 | Length: 200\n",
      "Episode 46 | Return: -122.95 | Length: 200\n",
      "Episode 47 | Return: -244.81 | Length: 200\n",
      "Episode 48 | Return: -122.66 | Length: 200\n",
      "Episode 49 | Return: -243.18 | Length: 200\n",
      "[Step 10000] Koopman mean prediction error: 0.010029003955423832\n",
      "Episode 50 | Return: -472.24 | Length: 200\n",
      "Episode 51 | Return: -119.84 | Length: 200\n",
      "Episode 52 | Return: -119.56 | Length: 200\n",
      "Episode 53 | Return: -1.11 | Length: 200\n",
      "Episode 54 | Return: -121.08 | Length: 200\n",
      "[Step 11000] Koopman mean prediction error: 0.013969753868877888\n",
      "Episode 55 | Return: -258.87 | Length: 200\n",
      "Episode 56 | Return: -0.84 | Length: 200\n",
      "Episode 57 | Return: -120.51 | Length: 200\n",
      "Episode 58 | Return: -366.88 | Length: 200\n",
      "Episode 59 | Return: -126.28 | Length: 200\n",
      "[Step 12000] Koopman mean prediction error: 0.0097689563408494\n",
      "Episode 60 | Return: -1.33 | Length: 200\n",
      "Episode 61 | Return: -124.81 | Length: 200\n",
      "Episode 62 | Return: -382.45 | Length: 200\n",
      "Episode 63 | Return: -284.41 | Length: 200\n",
      "Episode 64 | Return: -121.20 | Length: 200\n",
      "[Step 13000] Koopman mean prediction error: 0.014140608720481396\n",
      "Episode 65 | Return: -3.91 | Length: 200\n",
      "Episode 66 | Return: -125.93 | Length: 200\n",
      "Episode 67 | Return: -122.78 | Length: 200\n",
      "Episode 68 | Return: -1.80 | Length: 200\n",
      "Episode 69 | Return: -121.89 | Length: 200\n",
      "[Step 14000] Koopman mean prediction error: 0.009151249192655087\n",
      "Episode 70 | Return: -126.76 | Length: 200\n",
      "Episode 71 | Return: -122.25 | Length: 200\n",
      "Episode 72 | Return: -0.89 | Length: 200\n",
      "Episode 73 | Return: -116.92 | Length: 200\n",
      "Episode 74 | Return: -248.61 | Length: 200\n",
      "[Step 15000] Koopman mean prediction error: 0.009005347266793251\n",
      "Episode 75 | Return: -125.10 | Length: 200\n",
      "Episode 76 | Return: -248.21 | Length: 200\n",
      "Episode 77 | Return: -127.07 | Length: 200\n",
      "Episode 78 | Return: -122.07 | Length: 200\n",
      "Episode 79 | Return: -419.14 | Length: 200\n",
      "[Step 16000] Koopman mean prediction error: 0.012407000176608562\n",
      "Episode 80 | Return: -372.40 | Length: 200\n",
      "Episode 81 | Return: -494.84 | Length: 200\n",
      "Episode 82 | Return: -119.08 | Length: 200\n",
      "Episode 83 | Return: -114.91 | Length: 200\n",
      "Episode 84 | Return: -2.22 | Length: 200\n",
      "[Step 17000] Koopman mean prediction error: 0.010641622357070446\n",
      "Episode 85 | Return: -123.80 | Length: 200\n",
      "Episode 86 | Return: -233.39 | Length: 200\n",
      "Episode 87 | Return: -388.63 | Length: 200\n",
      "Episode 88 | Return: -329.11 | Length: 200\n",
      "Episode 89 | Return: -125.24 | Length: 200\n",
      "[Step 18000] Koopman mean prediction error: 0.010863175615668297\n",
      "Episode 90 | Return: -1.99 | Length: 200\n",
      "Episode 91 | Return: -127.74 | Length: 200\n",
      "Episode 92 | Return: -372.63 | Length: 200\n",
      "Episode 93 | Return: -231.39 | Length: 200\n",
      "Episode 94 | Return: -1.52 | Length: 200\n",
      "[Step 19000] Koopman mean prediction error: 0.014840280637145042\n",
      "Episode 95 | Return: -453.37 | Length: 200\n",
      "Episode 96 | Return: -231.35 | Length: 200\n",
      "Episode 97 | Return: -120.31 | Length: 200\n",
      "Episode 98 | Return: -4.34 | Length: 200\n",
      "Episode 99 | Return: -2.30 | Length: 200\n",
      "[Step 20000] Koopman mean prediction error: 0.010482298210263252\n",
      "Episode 100 | Return: -128.12 | Length: 200\n",
      "Training complete.\n",
      "Saved trained SAC weights!\n",
      "\n",
      "=== Running Episode 1/3 ===\n",
      "Episode 1 finished: return = -125.79, steps = 200\n",
      "\n",
      "=== Running Episode 2/3 ===\n",
      "Episode 2 finished: return = -258.19, steps = 200\n",
      "\n",
      "=== Running Episode 3/3 ===\n",
      "Episode 3 finished: return = -122.43, steps = 200\n",
      "\n",
      "Done running the agent!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Normal\n",
    "import gymnasium as gym\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Replay Buffer\n",
    "# ============================================================\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, obs_dim, act_dim, size):\n",
    "        self.obs_buf      = np.zeros((size, obs_dim), dtype=np.float32)\n",
    "        self.next_obs_buf = np.zeros((size, obs_dim), dtype=np.float32)\n",
    "        self.acts_buf     = np.zeros((size, act_dim), dtype=np.float32)\n",
    "        self.rews_buf     = np.zeros(size, dtype=np.float32)\n",
    "        self.done_buf     = np.zeros(size, dtype=np.float32)\n",
    "        self.ptr, self.size, self.max_size = 0, 0, size\n",
    "\n",
    "    def store(self, obs, act, rew, next_obs, done):\n",
    "        self.obs_buf[self.ptr]      = obs\n",
    "        self.acts_buf[self.ptr]     = act\n",
    "        self.rews_buf[self.ptr]     = rew\n",
    "        self.next_obs_buf[self.ptr] = next_obs\n",
    "        self.done_buf[self.ptr]     = done\n",
    "\n",
    "        self.ptr = (self.ptr + 1) % self.max_size\n",
    "        self.size = min(self.size + 1, self.max_size)\n",
    "\n",
    "    def sample_batch(self, batch_size):\n",
    "        idxs = np.random.randint(0, self.size, size=batch_size)\n",
    "        batch = dict(\n",
    "            obs      = self.obs_buf[idxs],\n",
    "            acts     = self.acts_buf[idxs],\n",
    "            rews     = self.rews_buf[idxs],\n",
    "            next_obs = self.next_obs_buf[idxs],\n",
    "            done     = self.done_buf[idxs],\n",
    "        )\n",
    "        return batch\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# CUSTOMIZABLE LIFTER (s)\n",
    "# For Pendulum-v1: s = [cos(theta), sin(theta), theta_dot]\n",
    "# ============================================================\n",
    "def lifter(s):\n",
    "    c, sn, w = s\n",
    "    return np.array([\n",
    "        c,\n",
    "        sn,\n",
    "        w,\n",
    "        c * w,\n",
    "        sn * w,\n",
    "        c**2,\n",
    "        sn**2,\n",
    "        w**2,\n",
    "    ], dtype=np.float32)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# EDMD / DMDc TRAINING (Koopman K and B)\n",
    "# ============================================================\n",
    "def train_koopman_EDMD(buffer, max_samples=None):\n",
    "    N = buffer.size\n",
    "    if N < 2:\n",
    "        return None, None\n",
    "\n",
    "    if max_samples is None or max_samples > N:\n",
    "        max_samples = N\n",
    "\n",
    "    idxs = np.random.choice(N, size=max_samples, replace=False)\n",
    "\n",
    "    Z_list, Zp_list, U_list = [], [], []\n",
    "\n",
    "    for i in idxs:\n",
    "        z  = lifter(buffer.obs_buf[i])\n",
    "        zp = lifter(buffer.next_obs_buf[i])\n",
    "        u  = buffer.acts_buf[i]\n",
    "\n",
    "        Z_list.append(z)\n",
    "        Zp_list.append(zp)\n",
    "        U_list.append(u)\n",
    "\n",
    "    Z  = np.array(Z_list).T       # (lift_dim, N)\n",
    "    Zp = np.array(Zp_list).T      # (lift_dim, N)\n",
    "    U  = np.array(U_list).T       # (act_dim, N)\n",
    "\n",
    "    XU = np.vstack([Z, U])        # (lift_dim + act_dim, N)\n",
    "\n",
    "    # Solve [K B] = Zp * pinv([Z; U])\n",
    "    A = Zp @ np.linalg.pinv(XU)\n",
    "\n",
    "    lift_dim = Z.shape[0]\n",
    "    act_dim  = U.shape[0]\n",
    "\n",
    "    K = A[:, :lift_dim]\n",
    "    B = A[:, lift_dim:lift_dim+act_dim]\n",
    "\n",
    "    return K, B\n",
    "\n",
    "\n",
    "def koopman_predict_lifted(s, a, K, B):\n",
    "    z  = lifter(s)\n",
    "    zp = K @ z + B @ a\n",
    "    return zp\n",
    "\n",
    "\n",
    "def decode_from_lifted(z):\n",
    "    # first 3 entries correspond to [cos(theta), sin(theta), theta_dot]\n",
    "    return z[:3]\n",
    "\n",
    "\n",
    "def test_koopman_model(buffer, K, B, num_tests=20):\n",
    "    if K is None or B is None:\n",
    "        return None\n",
    "    errors = []\n",
    "    for _ in range(num_tests):\n",
    "        idx = np.random.randint(0, buffer.size)\n",
    "        s = buffer.obs_buf[idx]\n",
    "        a = buffer.acts_buf[idx]\n",
    "        true_next = buffer.next_obs_buf[idx]\n",
    "\n",
    "        z_next = koopman_predict_lifted(s, a, K, B)\n",
    "        pred_next = decode_from_lifted(z_next)\n",
    "\n",
    "        err = np.linalg.norm(pred_next - true_next)\n",
    "        errors.append(err)\n",
    "    return float(np.mean(errors))\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Neural Network helpers\n",
    "# ============================================================\n",
    "def mlp(sizes, activation=nn.ReLU, output_activation=nn.Identity):\n",
    "    layers = []\n",
    "    for j in range(len(sizes)-1):\n",
    "        act = activation if j < len(sizes)-2 else output_activation\n",
    "        layers += [nn.Linear(sizes[j], sizes[j+1]), act()]\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# SAC Actor (Gaussian policy with Tanh squashing)\n",
    "# ============================================================\n",
    "class GaussianPolicy(nn.Module):\n",
    "    def __init__(self, obs_dim, act_dim, hidden_sizes, action_limit):\n",
    "        super().__init__()\n",
    "        self.net = mlp([obs_dim] + hidden_sizes,\n",
    "                       activation=nn.ReLU,\n",
    "                       output_activation=nn.ReLU)\n",
    "        self.mean_layer = nn.Linear(hidden_sizes[-1], act_dim)\n",
    "        self.log_std_layer = nn.Linear(hidden_sizes[-1], act_dim)\n",
    "        self.action_limit = action_limit\n",
    "        self.LOG_STD_MIN = -20\n",
    "        self.LOG_STD_MAX =  2\n",
    "\n",
    "    def forward(self, obs):\n",
    "        x = self.net(obs)\n",
    "        mean = self.mean_layer(x)\n",
    "        log_std = self.log_std_layer(x)\n",
    "        log_std = torch.clamp(log_std, self.LOG_STD_MIN, self.LOG_STD_MAX)\n",
    "        std = log_std.exp()\n",
    "        return mean, std\n",
    "\n",
    "    def sample(self, obs):\n",
    "        mean, std = self.forward(obs)\n",
    "        normal = Normal(mean, std)\n",
    "        z = normal.rsample()               # reparameterization trick\n",
    "        a = torch.tanh(z)\n",
    "        action = self.action_limit * a\n",
    "\n",
    "        # log_prob with Tanh correction\n",
    "        log_prob = normal.log_prob(z) - torch.log(1 - a.pow(2) + 1e-7)\n",
    "        log_prob = log_prob.sum(dim=-1, keepdim=True)\n",
    "\n",
    "        mean_action = self.action_limit * torch.tanh(mean)\n",
    "        return action, log_prob, mean_action\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# SAC Critics (Q-functions)\n",
    "# ============================================================\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, obs_dim, act_dim, hidden_sizes):\n",
    "        super().__init__()\n",
    "        self.q = mlp([obs_dim + act_dim] + hidden_sizes + [1],\n",
    "                     activation=nn.ReLU,\n",
    "                     output_activation=nn.Identity)\n",
    "\n",
    "    def forward(self, obs, act):\n",
    "        x = torch.cat([obs, act], dim=-1)\n",
    "        return self.q(x)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Soft Actor-Critic Agent\n",
    "# ============================================================\n",
    "class SACAgent:\n",
    "    def __init__(self, obs_dim, act_dim, action_limit,\n",
    "                 gamma=0.99, tau=0.005, alpha=0.2,\n",
    "                 actor_lr=3e-4, critic_lr=3e-4,\n",
    "                 hidden_sizes=[256, 256]):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.obs_dim = obs_dim\n",
    "        self.act_dim = act_dim\n",
    "        self.action_limit = action_limit\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "        self.alpha = alpha  # fixed entropy coefficient\n",
    "\n",
    "        # Actor\n",
    "        self.actor = GaussianPolicy(obs_dim, act_dim, hidden_sizes, action_limit).to(self.device)\n",
    "        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=actor_lr)\n",
    "\n",
    "        # Critics\n",
    "        self.q1 = QNetwork(obs_dim, act_dim, hidden_sizes).to(self.device)\n",
    "        self.q2 = QNetwork(obs_dim, act_dim, hidden_sizes).to(self.device)\n",
    "        self.q1_target = QNetwork(obs_dim, act_dim, hidden_sizes).to(self.device)\n",
    "        self.q2_target = QNetwork(obs_dim, act_dim, hidden_sizes).to(self.device)\n",
    "        self.q1_target.load_state_dict(self.q1.state_dict())\n",
    "        self.q2_target.load_state_dict(self.q2.state_dict())\n",
    "\n",
    "        self.q1_optimizer = optim.Adam(self.q1.parameters(), lr=critic_lr)\n",
    "        self.q2_optimizer = optim.Adam(self.q2.parameters(), lr=critic_lr)\n",
    "\n",
    "    def select_action(self, obs, deterministic=False):\n",
    "        obs = torch.as_tensor(obs, dtype=torch.float32).unsqueeze(0).to(self.device)\n",
    "        with torch.no_grad():\n",
    "            if deterministic:\n",
    "                _, _, act = self.actor.sample(obs)\n",
    "            else:\n",
    "                act, _, _ = self.actor.sample(obs)\n",
    "        return act.cpu().numpy()[0]\n",
    "\n",
    "    def update(self, batch):\n",
    "        obs      = torch.as_tensor(batch['obs'], dtype=torch.float32, device=self.device)\n",
    "        acts     = torch.as_tensor(batch['acts'], dtype=torch.float32, device=self.device)\n",
    "        rews     = torch.as_tensor(batch['rews'], dtype=torch.float32, device=self.device).unsqueeze(-1)\n",
    "        next_obs = torch.as_tensor(batch['next_obs'], dtype=torch.float32, device=self.device)\n",
    "        done     = torch.as_tensor(batch['done'], dtype=torch.float32, device=self.device).unsqueeze(-1)\n",
    "\n",
    "        # ------------------ Critic update ------------------ #\n",
    "        with torch.no_grad():\n",
    "            next_action, next_log_prob, _ = self.actor.sample(next_obs)\n",
    "            q1_next = self.q1_target(next_obs, next_action)\n",
    "            q2_next = self.q2_target(next_obs, next_action)\n",
    "            q_next = torch.min(q1_next, q2_next) - self.alpha * next_log_prob\n",
    "            target_q = rews + self.gamma * (1 - done) * q_next\n",
    "\n",
    "        q1 = self.q1(obs, acts)\n",
    "        q2 = self.q2(obs, acts)\n",
    "\n",
    "        q1_loss = F.mse_loss(q1, target_q)\n",
    "        q2_loss = F.mse_loss(q2, target_q)\n",
    "\n",
    "        self.q1_optimizer.zero_grad()\n",
    "        q1_loss.backward()\n",
    "        self.q1_optimizer.step()\n",
    "\n",
    "        self.q2_optimizer.zero_grad()\n",
    "        q2_loss.backward()\n",
    "        self.q2_optimizer.step()\n",
    "\n",
    "        # ------------------ Actor update ------------------ #\n",
    "        new_actions, log_prob, _ = self.actor.sample(obs)\n",
    "        q1_new = self.q1(obs, new_actions)\n",
    "        q2_new = self.q2(obs, new_actions)\n",
    "        q_new = torch.min(q1_new, q2_new)\n",
    "\n",
    "        actor_loss = (self.alpha * log_prob - q_new).mean()\n",
    "\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "\n",
    "        # ------------------ Target network update ------------------ #\n",
    "        with torch.no_grad():\n",
    "            for param, target_param in zip(self.q1.parameters(), self.q1_target.parameters()):\n",
    "                target_param.data.mul_(1 - self.tau)\n",
    "                target_param.data.add_(self.tau * param.data)\n",
    "            for param, target_param in zip(self.q2.parameters(), self.q2_target.parameters()):\n",
    "                target_param.data.mul_(1 - self.tau)\n",
    "                target_param.data.add_(self.tau * param.data)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# RUN TRAINING + KOOPMAN\n",
    "# ============================================================\n",
    "def train_sac_with_koopman():\n",
    "    env = gym.make(\"Pendulum-v1\")\n",
    "    obs_dim = env.observation_space.shape[0]\n",
    "    act_dim = env.action_space.shape[0]\n",
    "    action_limit = float(env.action_space.high[0])\n",
    "\n",
    "    buffer_size = 200000 \n",
    "    batch_size = 256\n",
    "    start_steps = 1000\n",
    "    update_after = 1000\n",
    "    update_every = 50\n",
    "    koopman_train_every = 1000   # environment steps\n",
    "    koopman_max_samples = 5000\n",
    "\n",
    "    max_env_steps = 20000  # give it some more room to actually update\n",
    "    max_ep_len = 200\n",
    "\n",
    "    replay_buffer = ReplayBuffer(obs_dim, act_dim, buffer_size)\n",
    "    agent = SACAgent(obs_dim, act_dim, action_limit)\n",
    "\n",
    "    total_steps = 0\n",
    "    episode = 0\n",
    "    K, B = None, None\n",
    "\n",
    "    all_steps = 0\n",
    "    all_ep_returns = []\n",
    "\n",
    "    while total_steps < max_env_steps:\n",
    "        obs, _ = env.reset()\n",
    "        ep_ret = 0.0\n",
    "        ep_len = 0\n",
    "        episode += 1\n",
    "\n",
    "        for t in range(max_ep_len):\n",
    "            if total_steps < start_steps:\n",
    "                act = env.action_space.sample()\n",
    "            else:\n",
    "                act = agent.select_action(obs)\n",
    "\n",
    "            # ensure np.float32\n",
    "            act = np.asarray(act, dtype=np.float32)\n",
    "\n",
    "            next_obs, rew, terminated, truncated, _ = env.step(act)\n",
    "            done = terminated or truncated\n",
    "            ep_ret += rew\n",
    "            ep_len += 1\n",
    "\n",
    "            replay_buffer.store(obs, act, rew, next_obs, float(done))\n",
    "\n",
    "            obs = next_obs\n",
    "            total_steps += 1\n",
    "\n",
    "            # SAC updates\n",
    "            if total_steps >= update_after and total_steps % update_every == 0:\n",
    "                for _ in range(update_every):\n",
    "                    batch = replay_buffer.sample_batch(batch_size)\n",
    "                    agent.update(batch)\n",
    "\n",
    "            # Koopman training + test\n",
    "            if total_steps >= update_after and total_steps % koopman_train_every == 0:\n",
    "                K, B = train_koopman_EDMD(replay_buffer, max_samples=koopman_max_samples)\n",
    "                koop_err = test_koopman_model(replay_buffer, K, B, num_tests=50)\n",
    "                print(f\"[Step {total_steps}] Koopman mean prediction error: {koop_err}\")\n",
    "\n",
    "            if done or total_steps >= max_env_steps:\n",
    "                print(f\"Episode {episode} | Return: {ep_ret:.2f} | Length: {ep_len}\")\n",
    "                all_steps += ep_len\n",
    "                all_ep_returns.append((all_steps, ep_ret))\n",
    "                break\n",
    "\n",
    "    env.close()\n",
    "    print(\"Training complete.\")\n",
    "\n",
    "    torch.save(agent.actor.state_dict(), \"actor_final_koopman.pt\")\n",
    "    torch.save(agent.q1.state_dict(), \"q1_final_koopman.pt\")\n",
    "    torch.save(agent.q2.state_dict(), \"q2_final_koopman.pt\")\n",
    "    print(\"Saved trained SAC weights!\")\n",
    "\n",
    "    return agent, K, B, all_steps, all_ep_returns\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# GUI ROLLOUT OF TRAINED AGENT\n",
    "# ============================================================\n",
    "def run_agent(agent, env_name=\"Pendulum-v1\", episodes=3, max_steps=200, deterministic=True):\n",
    "    env = gym.make(env_name, render_mode=\"human\")\n",
    "\n",
    "    for ep in range(episodes):\n",
    "        state, _ = env.reset()\n",
    "        ep_return = 0.0\n",
    "        print(f\"\\n=== Running Episode {ep+1}/{episodes} ===\")\n",
    "\n",
    "        for step in range(max_steps):\n",
    "            action = agent.select_action(state, deterministic=deterministic)\n",
    "            action = np.asarray(action, dtype=np.float32)\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "\n",
    "            state = next_state\n",
    "            ep_return += reward\n",
    "\n",
    "            if terminated or truncated:\n",
    "                break\n",
    "\n",
    "        print(f\"Episode {ep+1} finished: return = {ep_return:.2f}, steps = {step+1}\")\n",
    "\n",
    "    env.close()\n",
    "    print(\"\\nDone running the agent!\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# MAIN\n",
    "# ============================================================\n",
    "if __name__ == \"__main__\":\n",
    "    trained_agent_w_koopman, K, B, all_steps1, all_ep_returns1 = train_sac_with_koopman()\n",
    "    # After training, visualize the agent with GUI\n",
    "    run_agent(trained_agent_w_koopman, env_name=\"Pendulum-v1\", episodes=3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d9f015",
   "metadata": {},
   "source": [
    "STRAIGHT SAC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9a56c73d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1 | Return: -1383.57 | Length: 200\n",
      "Episode 2 | Return: -870.99 | Length: 200\n",
      "Episode 3 | Return: -1164.39 | Length: 200\n",
      "Episode 4 | Return: -1636.19 | Length: 200\n",
      "Episode 5 | Return: -845.22 | Length: 200\n",
      "Episode 6 | Return: -1221.91 | Length: 200\n",
      "Episode 7 | Return: -1644.56 | Length: 200\n",
      "Episode 8 | Return: -1911.90 | Length: 200\n",
      "Episode 9 | Return: -1602.54 | Length: 200\n",
      "Episode 10 | Return: -1774.16 | Length: 200\n",
      "Episode 11 | Return: -1522.48 | Length: 200\n",
      "Episode 12 | Return: -1417.63 | Length: 200\n",
      "Episode 13 | Return: -1392.05 | Length: 200\n",
      "Episode 14 | Return: -1239.53 | Length: 200\n",
      "Episode 15 | Return: -1251.55 | Length: 200\n",
      "Episode 16 | Return: -1223.22 | Length: 200\n",
      "Episode 17 | Return: -887.54 | Length: 200\n",
      "Episode 18 | Return: -911.34 | Length: 200\n",
      "Episode 19 | Return: -1188.24 | Length: 200\n",
      "Episode 20 | Return: -525.94 | Length: 200\n",
      "Episode 21 | Return: -411.33 | Length: 200\n",
      "Episode 22 | Return: -1047.89 | Length: 200\n",
      "Episode 23 | Return: -387.48 | Length: 200\n",
      "Episode 24 | Return: -1340.39 | Length: 200\n",
      "Episode 25 | Return: -1186.95 | Length: 200\n",
      "Episode 26 | Return: -126.22 | Length: 200\n",
      "Episode 27 | Return: -1235.34 | Length: 200\n",
      "Episode 28 | Return: -1139.87 | Length: 200\n",
      "Episode 29 | Return: -1017.74 | Length: 200\n",
      "Episode 30 | Return: -128.61 | Length: 200\n",
      "Episode 31 | Return: -127.33 | Length: 200\n",
      "Episode 32 | Return: -0.58 | Length: 200\n",
      "Episode 33 | Return: -368.00 | Length: 200\n",
      "Episode 34 | Return: -121.23 | Length: 200\n",
      "Episode 35 | Return: -0.39 | Length: 200\n",
      "Episode 36 | Return: -386.01 | Length: 200\n",
      "Episode 37 | Return: -262.93 | Length: 200\n",
      "Episode 38 | Return: -253.09 | Length: 200\n",
      "Episode 39 | Return: -486.86 | Length: 200\n",
      "Episode 40 | Return: -257.69 | Length: 200\n",
      "Episode 41 | Return: -495.44 | Length: 200\n",
      "Episode 42 | Return: -126.37 | Length: 200\n",
      "Episode 43 | Return: -250.40 | Length: 200\n",
      "Episode 44 | Return: -237.14 | Length: 200\n",
      "Episode 45 | Return: -122.33 | Length: 200\n",
      "Episode 46 | Return: -253.11 | Length: 200\n",
      "Episode 47 | Return: -125.78 | Length: 200\n",
      "Episode 48 | Return: -244.98 | Length: 200\n",
      "Episode 49 | Return: -399.65 | Length: 200\n",
      "Episode 50 | Return: -267.80 | Length: 200\n",
      "Episode 51 | Return: -120.11 | Length: 200\n",
      "Episode 52 | Return: -123.00 | Length: 200\n",
      "Episode 53 | Return: -126.72 | Length: 200\n",
      "Episode 54 | Return: -251.14 | Length: 200\n",
      "Episode 55 | Return: -276.12 | Length: 200\n",
      "Episode 56 | Return: -241.95 | Length: 200\n",
      "Episode 57 | Return: -537.59 | Length: 200\n",
      "Episode 58 | Return: -125.74 | Length: 200\n",
      "Episode 59 | Return: -125.33 | Length: 200\n",
      "Episode 60 | Return: -126.86 | Length: 200\n",
      "Episode 61 | Return: -264.23 | Length: 200\n",
      "Episode 62 | Return: -125.09 | Length: 200\n",
      "Episode 63 | Return: -127.20 | Length: 200\n",
      "Episode 64 | Return: -122.23 | Length: 200\n",
      "Episode 65 | Return: -124.49 | Length: 200\n",
      "Episode 66 | Return: -127.57 | Length: 200\n",
      "Episode 67 | Return: -376.01 | Length: 200\n",
      "Episode 68 | Return: -516.99 | Length: 200\n",
      "Episode 69 | Return: -0.59 | Length: 200\n",
      "Episode 70 | Return: -121.27 | Length: 200\n",
      "Episode 71 | Return: -124.38 | Length: 200\n",
      "Episode 72 | Return: -122.77 | Length: 200\n",
      "Episode 73 | Return: -123.90 | Length: 200\n",
      "Episode 74 | Return: -124.80 | Length: 200\n",
      "Episode 75 | Return: -252.22 | Length: 200\n",
      "Episode 76 | Return: -124.83 | Length: 200\n",
      "Episode 77 | Return: -248.54 | Length: 200\n",
      "Episode 78 | Return: -1.86 | Length: 200\n",
      "Episode 79 | Return: -281.71 | Length: 200\n",
      "Episode 80 | Return: -1.59 | Length: 200\n",
      "Episode 81 | Return: -354.04 | Length: 200\n",
      "Episode 82 | Return: -123.81 | Length: 200\n",
      "Episode 83 | Return: -0.66 | Length: 200\n",
      "Episode 84 | Return: -120.40 | Length: 200\n",
      "Episode 85 | Return: -119.39 | Length: 200\n",
      "Episode 86 | Return: -245.69 | Length: 200\n",
      "Episode 87 | Return: -123.55 | Length: 200\n",
      "Episode 88 | Return: -0.43 | Length: 200\n",
      "Episode 89 | Return: -122.08 | Length: 200\n",
      "Episode 90 | Return: -115.88 | Length: 200\n",
      "Episode 91 | Return: -237.10 | Length: 200\n",
      "Episode 92 | Return: -233.72 | Length: 200\n",
      "Episode 93 | Return: -247.10 | Length: 200\n",
      "Episode 94 | Return: -373.54 | Length: 200\n",
      "Episode 95 | Return: -400.16 | Length: 200\n",
      "Episode 96 | Return: -120.81 | Length: 200\n",
      "Episode 97 | Return: -119.90 | Length: 200\n",
      "Episode 98 | Return: -1.77 | Length: 200\n",
      "Episode 99 | Return: -245.98 | Length: 200\n",
      "Episode 100 | Return: -118.75 | Length: 200\n",
      "Finished training SAC.\n",
      "Saved SAC weights!\n",
      "\n",
      "=== Running Episode 1/3 ===\n",
      "Episode 1 Return = -116.66\n",
      "\n",
      "=== Running Episode 2/3 ===\n",
      "Episode 2 Return = -118.46\n",
      "\n",
      "=== Running Episode 3/3 ===\n",
      "Episode 3 Return = -121.62\n",
      "Training complete.\n",
      "Saved trained SAC weights!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Normal\n",
    "import gymnasium as gym\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Replay Buffer\n",
    "# ============================================================\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, obs_dim, act_dim, size):\n",
    "        self.obs_buf      = np.zeros((size, obs_dim), dtype=np.float32)\n",
    "        self.next_obs_buf = np.zeros((size, obs_dim), dtype=np.float32)\n",
    "        self.acts_buf     = np.zeros((size, act_dim), dtype=np.float32)\n",
    "        self.rews_buf     = np.zeros(size, dtype=np.float32)\n",
    "        self.done_buf     = np.zeros(size, dtype=np.float32)\n",
    "        self.ptr, self.size, self.max_size = 0, 0, size\n",
    "\n",
    "    def store(self, obs, act, rew, next_obs, done):\n",
    "        self.obs_buf[self.ptr]      = obs\n",
    "        self.acts_buf[self.ptr]     = act\n",
    "        self.rews_buf[self.ptr]     = rew\n",
    "        self.next_obs_buf[self.ptr] = next_obs\n",
    "        self.done_buf[self.ptr]     = done\n",
    "\n",
    "        self.ptr = (self.ptr + 1) % self.max_size\n",
    "        self.size = min(self.size + 1, self.max_size)\n",
    "\n",
    "    def sample_batch(self, batch_size):\n",
    "        idxs = np.random.randint(0, self.size, size=batch_size)\n",
    "        batch = dict(\n",
    "            obs      = self.obs_buf[idxs],\n",
    "            acts     = self.acts_buf[idxs],\n",
    "            rews     = self.rews_buf[idxs],\n",
    "            next_obs = self.next_obs_buf[idxs],\n",
    "            done     = self.done_buf[idxs],\n",
    "        )\n",
    "        return batch\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Neural Network: simple MLP builder\n",
    "# ============================================================\n",
    "def mlp(sizes, activation=nn.ReLU, output_activation=nn.Identity):\n",
    "    layers = []\n",
    "    for j in range(len(sizes) - 1):\n",
    "        act = activation if j < len(sizes) - 2 else output_activation\n",
    "        layers += [nn.Linear(sizes[j], sizes[j+1]), act()]\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Gaussian Policy (Actor)\n",
    "# ============================================================\n",
    "class GaussianPolicy(nn.Module):\n",
    "    def __init__(self, obs_dim, act_dim, hidden_sizes, action_limit):\n",
    "        super().__init__()\n",
    "        self.net = mlp([obs_dim] + hidden_sizes,\n",
    "                       activation=nn.ReLU,\n",
    "                       output_activation=nn.ReLU)\n",
    "        \n",
    "        self.mean_layer = nn.Linear(hidden_sizes[-1], act_dim)\n",
    "        self.log_std_layer = nn.Linear(hidden_sizes[-1], act_dim)\n",
    "\n",
    "        self.action_limit = action_limit\n",
    "        self.LOG_STD_MIN = -20\n",
    "        self.LOG_STD_MAX =  2\n",
    "\n",
    "    def forward(self, obs):\n",
    "        x = self.net(obs)\n",
    "        mean = self.mean_layer(x)\n",
    "        log_std = self.log_std_layer(x)\n",
    "        log_std = torch.clamp(log_std, self.LOG_STD_MIN, self.LOG_STD_MAX)\n",
    "        return mean, log_std.exp()\n",
    "\n",
    "    def sample(self, obs):\n",
    "        mean, std = self.forward(obs)\n",
    "        normal = Normal(mean, std)\n",
    "\n",
    "        z = normal.rsample()               # reparameterization\n",
    "        a = torch.tanh(z)\n",
    "        action = self.action_limit * a\n",
    "\n",
    "        # log (a|s)\n",
    "        log_prob = normal.log_prob(z) - torch.log(1 - a.pow(2) + 1e-7)\n",
    "        log_prob = log_prob.sum(axis=-1, keepdim=True)\n",
    "\n",
    "        mean_action = self.action_limit * torch.tanh(mean)\n",
    "        return action, log_prob, mean_action\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Q-value Networks (Critics)\n",
    "# ============================================================\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, obs_dim, act_dim, hidden_sizes):\n",
    "        super().__init__()\n",
    "        self.q = mlp([obs_dim + act_dim] + hidden_sizes + [1],\n",
    "                     activation=nn.ReLU,\n",
    "                     output_activation=nn.Identity)\n",
    "\n",
    "    def forward(self, obs, act):\n",
    "        x = torch.cat([obs, act], dim=-1)\n",
    "        return self.q(x)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Soft Actor-Critic Agent\n",
    "# ============================================================\n",
    "class SACAgent:\n",
    "    def __init__(self, obs_dim, act_dim, action_limit,\n",
    "                 gamma=0.99, tau=0.005, alpha=0.2,\n",
    "                 actor_lr=3e-4, critic_lr=3e-4,\n",
    "                 hidden_sizes=[256, 256]):\n",
    "\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.obs_dim = obs_dim\n",
    "        self.act_dim = act_dim\n",
    "        self.action_limit = action_limit\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "        self.alpha = alpha   # entropy coefficient\n",
    "\n",
    "        # Actor\n",
    "        self.actor = GaussianPolicy(obs_dim, act_dim, hidden_sizes, action_limit).to(self.device)\n",
    "        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=actor_lr)\n",
    "\n",
    "        # Critics\n",
    "        self.q1 = QNetwork(obs_dim, act_dim, hidden_sizes).to(self.device)\n",
    "        self.q2 = QNetwork(obs_dim, act_dim, hidden_sizes).to(self.device)\n",
    "        self.q1_target = QNetwork(obs_dim, act_dim, hidden_sizes).to(self.device)\n",
    "        self.q2_target = QNetwork(obs_dim, act_dim, hidden_sizes).to(self.device)\n",
    "\n",
    "        self.q1_target.load_state_dict(self.q1.state_dict())\n",
    "        self.q2_target.load_state_dict(self.q2.state_dict())\n",
    "\n",
    "        self.q1_optimizer = optim.Adam(self.q1.parameters(), lr=critic_lr)\n",
    "        self.q2_optimizer = optim.Adam(self.q2.parameters(), lr=critic_lr)\n",
    "\n",
    "    # Select greedy or exploratory\n",
    "    def select_action(self, obs, deterministic=False):\n",
    "        obs = torch.as_tensor(obs, dtype=torch.float32).unsqueeze(0).to(self.device)\n",
    "        with torch.no_grad():\n",
    "            if deterministic:\n",
    "                _, _, action = self.actor.sample(obs)\n",
    "            else:\n",
    "                action, _, _ = self.actor.sample(obs)\n",
    "        return action.cpu().numpy()[0]\n",
    "\n",
    "    # Gradient update\n",
    "    def update(self, batch):\n",
    "        obs = torch.as_tensor(batch['obs'], dtype=torch.float32, device=self.device)\n",
    "        acts = torch.as_tensor(batch['acts'], dtype=torch.float32, device=self.device)\n",
    "        rews = torch.as_tensor(batch['rews'], dtype=torch.float32, device=self.device).unsqueeze(-1)\n",
    "        next_obs = torch.as_tensor(batch['next_obs'], dtype=torch.float32, device=self.device)\n",
    "        done = torch.as_tensor(batch['done'], dtype=torch.float32, device=self.device).unsqueeze(-1)\n",
    "\n",
    "        # -------- Target Q -------- #\n",
    "        with torch.no_grad():\n",
    "            next_action, next_log_prob, _ = self.actor.sample(next_obs)\n",
    "            q1_next = self.q1_target(next_obs, next_action)\n",
    "            q2_next = self.q2_target(next_obs, next_action)\n",
    "            q_target = torch.min(q1_next, q2_next) - self.alpha * next_log_prob\n",
    "            target_value = rews + self.gamma * (1 - done) * q_target\n",
    "\n",
    "        # -------- Update Q1 -------- #\n",
    "        q1 = self.q1(obs, acts)\n",
    "        q1_loss = F.mse_loss(q1, target_value)\n",
    "        self.q1_optimizer.zero_grad()\n",
    "        q1_loss.backward()\n",
    "        self.q1_optimizer.step()\n",
    "\n",
    "        # -------- Update Q2 -------- #\n",
    "        q2 = self.q2(obs, acts)\n",
    "        q2_loss = F.mse_loss(q2, target_value)\n",
    "        self.q2_optimizer.zero_grad()\n",
    "        q2_loss.backward()\n",
    "        self.q2_optimizer.step()\n",
    "\n",
    "        # -------- Update Actor -------- #\n",
    "        new_actions, log_prob, _ = self.actor.sample(obs)\n",
    "        q1_new = self.q1(obs, new_actions)\n",
    "        q2_new = self.q2(obs, new_actions)\n",
    "        q_new = torch.min(q1_new, q2_new)\n",
    "\n",
    "        actor_loss = (self.alpha * log_prob - q_new).mean()\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "\n",
    "        # -------- Soft update targets -------- #\n",
    "        with torch.no_grad():\n",
    "            for p, tp in zip(self.q1.parameters(), self.q1_target.parameters()):\n",
    "                tp.data.mul_(1 - self.tau)\n",
    "                tp.data.add_(self.tau * p.data)\n",
    "            for p, tp in zip(self.q2.parameters(), self.q2_target.parameters()):\n",
    "                tp.data.mul_(1 - self.tau)\n",
    "                tp.data.add_(self.tau * p.data)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# TRAIN SAC\n",
    "# ============================================================\n",
    "def train_sac():\n",
    "    env = gym.make(\"Pendulum-v1\")\n",
    "    obs_dim = env.observation_space.shape[0]\n",
    "    act_dim = env.action_space.shape[0]\n",
    "    action_limit = float(env.action_space.high[0])\n",
    "\n",
    "    agent = SACAgent(obs_dim, act_dim, action_limit)\n",
    "\n",
    "    replay_buffer = ReplayBuffer(obs_dim, act_dim, size=200000)\n",
    "\n",
    "    max_steps = 20000\n",
    "    start_steps = 1000\n",
    "    update_after = 1000\n",
    "    update_every = 50\n",
    "    batch_size = 256\n",
    "    max_ep_len = 200\n",
    "\n",
    "    total_steps = 0\n",
    "    ep = 0\n",
    "\n",
    "    all_steps = 0\n",
    "    all_ep_returns = []\n",
    "\n",
    "    while total_steps < max_steps:\n",
    "        state, _ = env.reset()\n",
    "        ep += 1\n",
    "        ep_return = 0\n",
    "        ep_len = 0\n",
    "\n",
    "        for t in range(max_ep_len):\n",
    "            if total_steps < start_steps:\n",
    "                action = env.action_space.sample()\n",
    "            else:\n",
    "                action = agent.select_action(state, deterministic=False)\n",
    "\n",
    "            action = np.asarray(action, dtype=np.float32)\n",
    "\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "\n",
    "            replay_buffer.store(state, action, reward, next_state, float(done))\n",
    "            state = next_state\n",
    "\n",
    "            ep_return += reward\n",
    "            ep_len += 1\n",
    "            total_steps += 1\n",
    "\n",
    "            # Learn\n",
    "            if total_steps >= update_after and total_steps % update_every == 0:\n",
    "                for _ in range(update_every):\n",
    "                    batch = replay_buffer.sample_batch(batch_size)\n",
    "                    agent.update(batch)\n",
    "\n",
    "            if done or total_steps >= max_steps:\n",
    "                print(f\"Episode {ep} | Return: {ep_return:.2f} | Length: {ep_len}\")\n",
    "                all_steps += ep_len\n",
    "                all_ep_returns.append((all_steps, ep_return))\n",
    "                break\n",
    "\n",
    "    env.close()\n",
    "    print(\"Finished training SAC.\")\n",
    "\n",
    "    # Save trained policy\n",
    "    torch.save(agent.actor.state_dict(), \"actor_final.pt\")\n",
    "    torch.save(agent.q1.state_dict(),   \"q1_final.pt\")\n",
    "    torch.save(agent.q2.state_dict(),   \"q2_final.pt\")\n",
    "    print(\"Saved SAC weights!\")\n",
    "\n",
    "    return agent, all_steps, all_ep_returns\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# RUN TRAINED AGENT WITH GUI\n",
    "# ============================================================\n",
    "def run_agent(agent, env_name=\"Pendulum-v1\", episodes=3, max_steps=200, deterministic=True):\n",
    "    env = gym.make(env_name, render_mode=\"human\")\n",
    "\n",
    "    for ep in range(episodes):\n",
    "        state, _ = env.reset()\n",
    "        ep_return = 0\n",
    "\n",
    "        print(f\"\\n=== Running Episode {ep+1}/{episodes} ===\")\n",
    "\n",
    "        for step in range(max_steps):\n",
    "            action = agent.select_action(state, deterministic=True)\n",
    "            action = np.asarray(action, dtype=np.float32)\n",
    "\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            state = next_state\n",
    "\n",
    "            ep_return += reward\n",
    "            if terminated or truncated:\n",
    "                break\n",
    "\n",
    "        print(f\"Episode {ep+1} Return = {ep_return:.2f}\")\n",
    "\n",
    "    env.close()\n",
    "    print(\"Training complete.\")\n",
    "\n",
    "    torch.save(agent.actor.state_dict(), \"actor_final_SAC.pt\")\n",
    "    torch.save(agent.q1.state_dict(), \"q1_final_SAC.pt\")\n",
    "    torch.save(agent.q2.state_dict(), \"q2_final_SAC.pt\")\n",
    "    print(\"Saved trained SAC weights!\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# MAIN\n",
    "# ============================================================\n",
    "if __name__ == \"__main__\":\n",
    "    trained_agent_SAC, all_steps2, all_ep_returns2 = train_sac()\n",
    "    run_agent(trained_agent_SAC, env_name=\"Pendulum-v1\", episodes=3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "513a036b",
   "metadata": {},
   "source": [
    "KOOPMAN + PLANNING + RL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e785f6d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1 | Return: -1705.59 | Length: 200\n",
      "Episode 2 | Return: -1516.45 | Length: 200\n",
      "Episode 3 | Return: -1562.14 | Length: 200\n",
      "Episode 4 | Return: -1371.16 | Length: 200\n",
      "[Step 1000] Training dynamics models...\n",
      "[Step 1000 ] Dynamics training done.\n",
      "Episode 5 | Return: -1454.47 | Length: 200\n",
      "Episode 6 | Return: -1755.47 | Length: 200\n",
      "Episode 7 | Return: -1742.90 | Length: 200\n",
      "Episode 8 | Return: -1553.84 | Length: 200\n",
      "Episode 9 | Return: -1327.00 | Length: 200\n",
      "[Step 2000] Training dynamics models...\n",
      "[Step 2000 ] Dynamics training done.\n",
      "Episode 10 | Return: -1622.10 | Length: 200\n",
      "Episode 11 | Return: -1155.56 | Length: 200\n",
      "Episode 12 | Return: -922.99 | Length: 200\n",
      "Episode 13 | Return: -907.31 | Length: 200\n",
      "Episode 14 | Return: -1213.40 | Length: 200\n",
      "[Step 3000] Training dynamics models...\n",
      "[Step 3000 ] Dynamics training done.\n",
      "Episode 15 | Return: -943.40 | Length: 200\n",
      "Episode 16 | Return: -753.13 | Length: 200\n",
      "Episode 17 | Return: -668.70 | Length: 200\n",
      "Episode 18 | Return: -555.37 | Length: 200\n",
      "Episode 19 | Return: -643.42 | Length: 200\n",
      "[Step 4000] Training dynamics models...\n",
      "[Step 4000 ] Dynamics training done.\n",
      "Episode 20 | Return: -662.34 | Length: 200\n",
      "Episode 21 | Return: -122.46 | Length: 200\n",
      "Episode 22 | Return: -126.97 | Length: 200\n",
      "Episode 23 | Return: -254.53 | Length: 200\n",
      "Episode 24 | Return: -247.59 | Length: 200\n",
      "[Step 5000] Training dynamics models...\n",
      "[Step 5000 ] Dynamics training done.\n",
      "Episode 25 | Return: -363.69 | Length: 200\n",
      "Episode 26 | Return: -127.03 | Length: 200\n",
      "Episode 27 | Return: -129.92 | Length: 200\n",
      "Episode 28 | Return: -1.32 | Length: 200\n",
      "Episode 29 | Return: -251.63 | Length: 200\n",
      "[Step 6000] Training dynamics models...\n",
      "[Step 6000 ] Dynamics training done.\n",
      "Episode 30 | Return: -119.90 | Length: 200\n",
      "Episode 31 | Return: -127.34 | Length: 200\n",
      "Episode 32 | Return: -126.55 | Length: 200\n",
      "Episode 33 | Return: -126.13 | Length: 200\n",
      "Episode 34 | Return: -0.51 | Length: 200\n",
      "[Step 7000] Training dynamics models...\n",
      "[Step 7000 ] Dynamics training done.\n",
      "Episode 35 | Return: -245.77 | Length: 200\n",
      "Episode 36 | Return: -126.19 | Length: 200\n",
      "Episode 37 | Return: -0.61 | Length: 200\n",
      "Episode 38 | Return: -120.88 | Length: 200\n",
      "Episode 39 | Return: -1.95 | Length: 200\n",
      "[Step 8000] Training dynamics models...\n",
      "[Step 8000 ] Dynamics training done.\n",
      "Episode 40 | Return: -124.85 | Length: 200\n",
      "Episode 41 | Return: -123.51 | Length: 200\n",
      "Episode 42 | Return: -127.26 | Length: 200\n",
      "Episode 43 | Return: -370.70 | Length: 200\n",
      "Episode 44 | Return: -126.97 | Length: 200\n",
      "[Step 9000] Training dynamics models...\n",
      "[Step 9000 ] Dynamics training done.\n",
      "Episode 45 | Return: -117.73 | Length: 200\n",
      "Episode 46 | Return: -244.37 | Length: 200\n",
      "Episode 47 | Return: -122.82 | Length: 200\n",
      "Episode 48 | Return: -118.18 | Length: 200\n",
      "Episode 49 | Return: -242.81 | Length: 200\n",
      "[Step 10000] Training dynamics models...\n",
      "[Step 10000 ] Dynamics training done.\n",
      "Episode 50 | Return: -116.78 | Length: 200\n",
      "Episode 51 | Return: -124.80 | Length: 200\n",
      "Episode 52 | Return: -0.96 | Length: 200\n",
      "Episode 53 | Return: -122.05 | Length: 200\n",
      "Episode 54 | Return: -127.71 | Length: 200\n",
      "[Step 11000] Training dynamics models...\n",
      "[Step 11000 ] Dynamics training done.\n",
      "Episode 55 | Return: -224.99 | Length: 200\n",
      "Episode 56 | Return: -1.53 | Length: 200\n",
      "Episode 57 | Return: -119.97 | Length: 200\n",
      "Episode 58 | Return: -236.71 | Length: 200\n",
      "Episode 59 | Return: -231.33 | Length: 200\n",
      "[Step 12000] Training dynamics models...\n",
      "[Step 12000 ] Dynamics training done.\n",
      "Episode 60 | Return: -228.91 | Length: 200\n",
      "Episode 61 | Return: -124.31 | Length: 200\n",
      "Episode 62 | Return: -122.68 | Length: 200\n",
      "Episode 63 | Return: -118.22 | Length: 200\n",
      "Episode 64 | Return: -371.19 | Length: 200\n",
      "[Step 13000] Training dynamics models...\n",
      "[Step 13000 ] Dynamics training done.\n",
      "Episode 65 | Return: -124.21 | Length: 200\n",
      "Episode 66 | Return: -122.53 | Length: 200\n",
      "Episode 67 | Return: -2.06 | Length: 200\n",
      "Episode 68 | Return: -124.25 | Length: 200\n",
      "Episode 69 | Return: -123.96 | Length: 200\n",
      "[Step 14000] Training dynamics models...\n",
      "[Step 14000 ] Dynamics training done.\n",
      "Episode 70 | Return: -235.64 | Length: 200\n",
      "Episode 71 | Return: -122.94 | Length: 200\n",
      "Episode 72 | Return: -115.07 | Length: 200\n",
      "Episode 73 | Return: -116.57 | Length: 200\n",
      "Episode 74 | Return: -115.65 | Length: 200\n",
      "[Step 15000] Training dynamics models...\n",
      "[Step 15000 ] Dynamics training done.\n",
      "Episode 75 | Return: -117.05 | Length: 200\n",
      "Episode 76 | Return: -1.06 | Length: 200\n",
      "Episode 77 | Return: -243.03 | Length: 200\n",
      "Episode 78 | Return: -119.77 | Length: 200\n",
      "Episode 79 | Return: -118.40 | Length: 200\n",
      "[Step 16000] Training dynamics models...\n",
      "[Step 16000 ] Dynamics training done.\n",
      "Episode 80 | Return: -121.80 | Length: 200\n",
      "Episode 81 | Return: -344.35 | Length: 200\n",
      "Episode 82 | Return: -235.80 | Length: 200\n",
      "Episode 83 | Return: -243.24 | Length: 200\n",
      "Episode 84 | Return: -352.99 | Length: 200\n",
      "[Step 17000] Training dynamics models...\n",
      "[Step 17000 ] Dynamics training done.\n",
      "Episode 85 | Return: -284.88 | Length: 200\n",
      "Episode 86 | Return: -234.16 | Length: 200\n",
      "Episode 87 | Return: -235.74 | Length: 200\n",
      "Episode 88 | Return: -231.64 | Length: 200\n",
      "Episode 89 | Return: -124.95 | Length: 200\n",
      "[Step 18000] Training dynamics models...\n",
      "[Step 18000 ] Dynamics training done.\n",
      "Episode 90 | Return: -116.85 | Length: 200\n",
      "Episode 91 | Return: -122.48 | Length: 200\n",
      "Episode 92 | Return: -124.18 | Length: 200\n",
      "Episode 93 | Return: -122.67 | Length: 200\n",
      "Episode 94 | Return: -222.78 | Length: 200\n",
      "[Step 19000] Training dynamics models...\n",
      "[Step 19000 ] Dynamics training done.\n",
      "Episode 95 | Return: -244.40 | Length: 200\n",
      "Episode 96 | Return: -269.27 | Length: 200\n",
      "Episode 97 | Return: -121.25 | Length: 200\n",
      "Episode 98 | Return: -119.68 | Length: 200\n",
      "Episode 99 | Return: -120.36 | Length: 200\n",
      "[Step 20000] Training dynamics models...\n",
      "[Step 20000 ] Dynamics training done.\n",
      "Episode 100 | Return: -122.17 | Length: 200\n",
      "Finished training SAC + planner.\n",
      "Saved SAC weights!\n",
      "\n",
      "=== Running Episode 1/3 ===\n",
      "Episode 1 Return = -118.08\n",
      "\n",
      "=== Running Episode 2/3 ===\n",
      "Episode 2 Return = -240.40\n",
      "\n",
      "=== Running Episode 3/3 ===\n",
      "Episode 3 Return = -118.79\n",
      "\n",
      "Done displaying agent!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Normal\n",
    "import gymnasium as gym\n",
    "\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, obs_dim, act_dim, size):\n",
    "        self.obs_buf      = np.zeros((size, obs_dim), dtype=np.float32)\n",
    "        self.next_obs_buf = np.zeros((size, obs_dim), dtype=np.float32)\n",
    "        self.acts_buf     = np.zeros((size, act_dim), dtype=np.float32)\n",
    "        self.rews_buf     = np.zeros(size, dtype=np.float32)\n",
    "        self.done_buf     = np.zeros(size, dtype=np.float32)\n",
    "        self.ptr, self.size, self.max_size = 0, 0, size\n",
    "\n",
    "    def store(self, obs, act, rew, next_obs, done):\n",
    "        self.obs_buf[self.ptr]      = obs\n",
    "        self.acts_buf[self.ptr]     = act\n",
    "        self.rews_buf[self.ptr]     = rew\n",
    "        self.next_obs_buf[self.ptr] = next_obs\n",
    "        self.done_buf[self.ptr]     = done\n",
    "\n",
    "        self.ptr = (self.ptr + 1) % self.max_size\n",
    "        self.size = min(self.size + 1, self.max_size)\n",
    "\n",
    "    def sample_batch(self, batch_size):\n",
    "        idxs = np.random.randint(0, self.size, size=batch_size)\n",
    "        batch = dict(\n",
    "            obs      = self.obs_buf[idxs],\n",
    "            acts     = self.acts_buf[idxs],\n",
    "            rews     = self.rews_buf[idxs],\n",
    "            next_obs = self.next_obs_buf[idxs],\n",
    "            done     = self.done_buf[idxs],\n",
    "        )\n",
    "        return batch\n",
    "\n",
    "\n",
    "def mlp(sizes, activation=nn.ReLU, output_activation=nn.Identity):\n",
    "    layers = []\n",
    "    for j in range(len(sizes) - 1):\n",
    "        act = activation if j < len(sizes) - 2 else output_activation\n",
    "        layers += [nn.Linear(sizes[j], sizes[j+1]), act()]\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "class GaussianPolicy(nn.Module):\n",
    "    def __init__(self, obs_dim, act_dim, hidden_sizes, action_limit):\n",
    "        super().__init__()\n",
    "        self.net = mlp([obs_dim] + hidden_sizes,\n",
    "                       activation=nn.ReLU,\n",
    "                       output_activation=nn.ReLU)\n",
    "        \n",
    "        self.mean_layer = nn.Linear(hidden_sizes[-1], act_dim)\n",
    "        self.log_std_layer = nn.Linear(hidden_sizes[-1], act_dim)\n",
    "\n",
    "        self.action_limit = action_limit\n",
    "        self.LOG_STD_MIN = -20\n",
    "        self.LOG_STD_MAX =  2\n",
    "\n",
    "    def forward(self, obs):\n",
    "        x = self.net(obs)\n",
    "        mean = self.mean_layer(x)\n",
    "        log_std = self.log_std_layer(x)\n",
    "        log_std = torch.clamp(log_std, self.LOG_STD_MIN, self.LOG_STD_MAX)\n",
    "        return mean, log_std.exp()\n",
    "\n",
    "    def sample(self, obs):\n",
    "        mean, std = self.forward(obs)\n",
    "        normal = Normal(mean, std)\n",
    "\n",
    "        z = normal.rsample()               # reparameterization\n",
    "        a = torch.tanh(z)\n",
    "        action = self.action_limit * a\n",
    "\n",
    "        # log (a|s)\n",
    "        log_prob = normal.log_prob(z) - torch.log(1 - a.pow(2) + 1e-7)\n",
    "        log_prob = log_prob.sum(axis=-1, keepdim=True)\n",
    "\n",
    "        mean_action = self.action_limit * torch.tanh(mean)\n",
    "        return action, log_prob, mean_action\n",
    "\n",
    "\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, obs_dim, act_dim, hidden_sizes):\n",
    "        super().__init__()\n",
    "        self.q = mlp([obs_dim + act_dim] + hidden_sizes + [1],\n",
    "                     activation=nn.ReLU,\n",
    "                     output_activation=nn.Identity)\n",
    "\n",
    "    def forward(self, obs, act):\n",
    "        x = torch.cat([obs, act], dim=-1)\n",
    "        return self.q(x)\n",
    "\n",
    "\n",
    "class DynamicsModel(nn.Module):\n",
    "    def __init__(self, obs_dim, act_dim, hidden_sizes=[256, 256]):\n",
    "        super().__init__()\n",
    "        self.net = mlp([obs_dim + act_dim] + hidden_sizes + [obs_dim + 1],\n",
    "                       activation=nn.ReLU,\n",
    "                       output_activation=nn.Identity)\n",
    "\n",
    "    def forward(self, obs, act):\n",
    "        x = torch.cat([obs, act], dim=-1)\n",
    "        out = self.net(x)\n",
    "        delta = out[..., :-1]\n",
    "        reward = out[..., -1:]\n",
    "        return delta, reward\n",
    "\n",
    "\n",
    "class SACAgent:\n",
    "    def __init__(self, obs_dim, act_dim, action_limit,\n",
    "                 gamma=0.99, tau=0.005, alpha=0.2,\n",
    "                 actor_lr=3e-4, critic_lr=3e-4,\n",
    "                 hidden_sizes=[256, 256]):\n",
    "\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.obs_dim = obs_dim\n",
    "        self.act_dim = act_dim\n",
    "        self.action_limit = action_limit\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "        self.alpha = alpha   # entropy coefficient\n",
    "\n",
    "        # Actor\n",
    "        self.actor = GaussianPolicy(obs_dim, act_dim, hidden_sizes, action_limit).to(self.device)\n",
    "        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=actor_lr)\n",
    "\n",
    "        # Critics\n",
    "        self.q1 = QNetwork(obs_dim, act_dim, hidden_sizes).to(self.device)\n",
    "        self.q2 = QNetwork(obs_dim, act_dim, hidden_sizes).to(self.device)\n",
    "        self.q1_target = QNetwork(obs_dim, act_dim, hidden_sizes).to(self.device)\n",
    "        self.q2_target = QNetwork(obs_dim, act_dim, hidden_sizes).to(self.device)\n",
    "\n",
    "        self.q1_target.load_state_dict(self.q1.state_dict())\n",
    "        self.q2_target.load_state_dict(self.q2.state_dict())\n",
    "\n",
    "        self.q1_optimizer = optim.Adam(self.q1.parameters(), lr=critic_lr)\n",
    "        self.q2_optimizer = optim.Adam(self.q2.parameters(), lr=critic_lr)\n",
    "\n",
    "    def select_action(self, obs, deterministic=False):\n",
    "        obs = torch.as_tensor(obs, dtype=torch.float32).unsqueeze(0).to(self.device)\n",
    "        with torch.no_grad():\n",
    "            if deterministic:\n",
    "                _, _, action = self.actor.sample(obs)\n",
    "            else:\n",
    "                action, _, _ = self.actor.sample(obs)\n",
    "        return action.cpu().numpy()[0]\n",
    "\n",
    "    def update(self, batch):\n",
    "        obs = torch.as_tensor(batch['obs'], dtype=torch.float32, device=self.device)\n",
    "        acts = torch.as_tensor(batch['acts'], dtype=torch.float32, device=self.device)\n",
    "        rews = torch.as_tensor(batch['rews'], dtype=torch.float32, device=self.device).unsqueeze(-1)\n",
    "        next_obs = torch.as_tensor(batch['next_obs'], dtype=torch.float32, device=self.device)\n",
    "        done = torch.as_tensor(batch['done'], dtype=torch.float32, device=self.device).unsqueeze(-1)\n",
    "\n",
    "        # -------- Target Q -------- #\n",
    "        with torch.no_grad():\n",
    "            next_action, next_log_prob, _ = self.actor.sample(next_obs)\n",
    "            q1_next = self.q1_target(next_obs, next_action)\n",
    "            q2_next = self.q2_target(next_obs, next_action)\n",
    "            q_target = torch.min(q1_next, q2_next) - self.alpha * next_log_prob\n",
    "            target_value = rews + self.gamma * (1 - done) * q_target\n",
    "\n",
    "        # -------- Update Q1 -------- #\n",
    "        q1 = self.q1(obs, acts)\n",
    "        q1_loss = F.mse_loss(q1, target_value)\n",
    "        self.q1_optimizer.zero_grad()\n",
    "        q1_loss.backward()\n",
    "        self.q1_optimizer.step()\n",
    "\n",
    "        # -------- Update Q2 -------- #\n",
    "        q2 = self.q2(obs, acts)\n",
    "        q2_loss = F.mse_loss(q2, target_value)\n",
    "        self.q2_optimizer.zero_grad()\n",
    "        q2_loss.backward()\n",
    "        self.q2_optimizer.step()\n",
    "\n",
    "        # -------- Update Actor -------- #\n",
    "        new_actions, log_prob, _ = self.actor.sample(obs)\n",
    "        q1_new = self.q1(obs, new_actions)\n",
    "        q2_new = self.q2(obs, new_actions)\n",
    "        q_new = torch.min(q1_new, q2_new)\n",
    "\n",
    "        actor_loss = (self.alpha * log_prob - q_new).mean()\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "\n",
    "        # -------- Soft update targets -------- #\n",
    "        with torch.no_grad():\n",
    "            for p, tp in zip(self.q1.parameters(), self.q1_target.parameters()):\n",
    "                tp.data.mul_(1 - self.tau)\n",
    "                tp.data.add_(self.tau * p.data)\n",
    "            for p, tp in zip(self.q2.parameters(), self.q2_target.parameters()):\n",
    "                tp.data.mul_(1 - self.tau)\n",
    "                tp.data.add_(self.tau * p.data)\n",
    "\n",
    "\n",
    "def train_dynamics_ensemble(models, buffer, device,\n",
    "                            batch_size=256, updates=200):\n",
    "    if buffer.size < batch_size:\n",
    "        return\n",
    "\n",
    "    for _ in range(updates):\n",
    "        batch = buffer.sample_batch(batch_size)\n",
    "        obs = torch.as_tensor(batch['obs'], dtype=torch.float32, device=device)\n",
    "        acts = torch.as_tensor(batch['acts'], dtype=torch.float32, device=device)\n",
    "        next_obs = torch.as_tensor(batch['next_obs'], dtype=torch.float32, device=device)\n",
    "        rews = torch.as_tensor(batch['rews'], dtype=torch.float32, device=device).unsqueeze(-1)\n",
    "\n",
    "        for model, opt in models:\n",
    "            delta_pred, rew_pred = model(obs, acts)\n",
    "            delta_true = next_obs - obs\n",
    "\n",
    "            loss_delta = F.mse_loss(delta_pred, delta_true)\n",
    "            loss_rew   = F.mse_loss(rew_pred, rews)\n",
    "            loss = loss_delta + loss_rew\n",
    "\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "\n",
    "def plan_action_with_model(state, agent, models, action_limit,\n",
    "                           H=3, N=64, iters=3, gamma=0.99, beta=0.7):\n",
    "    \"\"\"\n",
    "    state: np array (obs_dim,)\n",
    "    agent: SACAgent\n",
    "    models: list of (DynamicsModel, optimizer)\n",
    "    Returns: np array action (act_dim,)\n",
    "    \"\"\"\n",
    "\n",
    "    device = agent.device\n",
    "    obs_dim = agent.obs_dim\n",
    "    act_dim = agent.act_dim\n",
    "\n",
    "    if len(models) == 0:\n",
    "        # Fallback to actor if no models\n",
    "        return agent.select_action(state, deterministic=False)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        s0 = torch.as_tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "\n",
    "        # Get actor's mean action as prior\n",
    "        mean_action, _, _ = agent.actor.sample(s0)   # (1, act_dim)\n",
    "        mean_action = mean_action.squeeze(0)         # (act_dim,)\n",
    "\n",
    "        # Init sequence mean & std (H, act_dim)\n",
    "        seq_mean = mean_action.unsqueeze(0).repeat(H, 1)  # same mean for all steps\n",
    "        seq_std  = (0.5 * action_limit) * torch.ones_like(seq_mean)\n",
    "\n",
    "        for _ in range(iters):\n",
    "            # Sample N action sequences: (N, H, act_dim)\n",
    "            eps = torch.randn(N, H, act_dim, device=device)\n",
    "            actions = seq_mean.unsqueeze(0) + seq_std.unsqueeze(0) * eps\n",
    "            actions = torch.clamp(actions, -action_limit, action_limit)\n",
    "\n",
    "            # Rollout in model\n",
    "            s = s0.unsqueeze(0).repeat(N, 1, 1)  # (N,1,obs_dim)\n",
    "            s = s[:, 0, :]                       # (N, obs_dim)\n",
    "\n",
    "            returns = torch.zeros(N, 1, device=device)\n",
    "\n",
    "            for t in range(H):\n",
    "                a_t = actions[:, t, :]\n",
    "                # Pick a random model per sequence for diversity\n",
    "                idxs = torch.randint(0, len(models), (N,), device=device)\n",
    "                delta_list = []\n",
    "                rew_list = []\n",
    "                for m_i in range(len(models)):\n",
    "                    mask = (idxs == m_i)\n",
    "                    if mask.sum() == 0:\n",
    "                        continue\n",
    "                    model = models[m_i][0]\n",
    "                    delta_pred, rew_pred = model(s[mask], a_t[mask])\n",
    "                    # store back\n",
    "                    delta_list.append((mask, delta_pred, rew_pred))\n",
    "                # aggregate\n",
    "                new_s = s.clone()\n",
    "                rew_t = torch.zeros_like(returns)\n",
    "                for mask, delta_pred, rew_pred in delta_list:\n",
    "                    new_s[mask] = s[mask] + delta_pred\n",
    "                    rew_t[mask] = rew_pred\n",
    "                s = new_s\n",
    "                returns += (gamma**t) * rew_t\n",
    "\n",
    "            # Add bootstrap value from critic at s_H\n",
    "            a_H, _, _ = agent.actor.sample(s)\n",
    "            q1_H = agent.q1(s, a_H)\n",
    "            q2_H = agent.q2(s, a_H)\n",
    "            q_H = torch.min(q1_H, q2_H)\n",
    "            returns += (gamma**H) * q_H\n",
    "\n",
    "            # Softmax weights over sequences\n",
    "            scores = returns.squeeze(1)\n",
    "            scores = scores - scores.max()     # numerical stability\n",
    "            weights = torch.softmax(scores, dim=0)  # (N,)\n",
    "\n",
    "            # Update mean and std per time-step (CEM-like)\n",
    "            w = weights.view(N, 1, 1)\n",
    "            seq_mean = (w * actions).sum(dim=0)           # (H, act_dim)\n",
    "            diff = actions - seq_mean.unsqueeze(0)\n",
    "            seq_std = torch.sqrt((w * diff**2).sum(dim=0) + 1e-6)\n",
    "\n",
    "        # Use mean at time 0\n",
    "        a0 = seq_mean[0]\n",
    "        return a0.cpu().numpy()\n",
    "\n",
    "\n",
    "def train_sac_with_planning():\n",
    "    env = gym.make(\"Pendulum-v1\")\n",
    "    obs_dim = env.observation_space.shape[0]\n",
    "    act_dim = env.action_space.shape[0]\n",
    "    action_limit = float(env.action_space.high[0])\n",
    "\n",
    "    agent = SACAgent(obs_dim, act_dim, action_limit)\n",
    "    replay_buffer = ReplayBuffer(obs_dim, act_dim, size=200000)\n",
    "\n",
    "    # Dynamics ensemble\n",
    "    ensemble_size = 5\n",
    "    models = []\n",
    "    for _ in range(ensemble_size):\n",
    "        m = DynamicsModel(obs_dim, act_dim).to(agent.device)\n",
    "        opt = optim.Adam(m.parameters(), lr=1e-3)\n",
    "        models.append((m, opt))\n",
    "\n",
    "    max_steps = 20000\n",
    "    start_steps = 1000          \n",
    "    update_after = 1000\n",
    "    update_every = 50\n",
    "    batch_size = 256\n",
    "    max_ep_len = 200\n",
    "\n",
    "    model_train_every = 1000\n",
    "    model_updates = 200\n",
    "\n",
    "    total_steps = 0\n",
    "    ep = 0\n",
    "\n",
    "    all_steps = 0\n",
    "    all_ep_returns = []\n",
    "\n",
    "    while total_steps < max_steps:\n",
    "        state, _ = env.reset()\n",
    "        ep += 1\n",
    "        ep_return = 0\n",
    "        ep_len = 0\n",
    "\n",
    "        for t in range(max_ep_len):\n",
    "            if total_steps < start_steps:\n",
    "                action = env.action_space.sample()\n",
    "            else:\n",
    "                # Use planner instead of raw actor\n",
    "                action = plan_action_with_model(\n",
    "                    state, agent, models, action_limit,\n",
    "                    H=3, N=64, iters=3, gamma=agent.gamma, beta=0.7\n",
    "                )\n",
    "\n",
    "            action = np.asarray(action, dtype=np.float32)\n",
    "\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "\n",
    "            replay_buffer.store(state, action, reward, next_state, float(done))\n",
    "            state = next_state\n",
    "\n",
    "            ep_return += reward\n",
    "            ep_len += 1\n",
    "            total_steps += 1\n",
    "\n",
    "            # SAC updates\n",
    "            if total_steps >= update_after and total_steps % update_every == 0:\n",
    "                for _ in range(update_every):\n",
    "                    batch = replay_buffer.sample_batch(batch_size)\n",
    "                    agent.update(batch)\n",
    "\n",
    "            # Train dynamics ensemble\n",
    "            if total_steps >= update_after and total_steps % model_train_every == 0:\n",
    "                print(f\"[Step {total_steps}] Training dynamics models...\")\n",
    "                train_dynamics_ensemble(models, replay_buffer, agent.device,\n",
    "                                        batch_size=batch_size, updates=model_updates)\n",
    "                print(\"[Step\", total_steps, \"] Dynamics training done.\")\n",
    "\n",
    "            if done or total_steps >= max_steps:\n",
    "                print(f\"Episode {ep} | Return: {ep_return:.2f} | Length: {ep_len}\")\n",
    "                all_steps += ep_len\n",
    "                all_ep_returns.append((all_steps, ep_return))\n",
    "                break\n",
    "\n",
    "    env.close()\n",
    "    print(\"Finished training SAC + planner.\")\n",
    "\n",
    "    # Save trained policy\n",
    "    torch.save(agent.actor.state_dict(), \"actor_final_plan.pt\")\n",
    "    torch.save(agent.q1.state_dict(),   \"q1_final_plan.pt\")\n",
    "    torch.save(agent.q2.state_dict(),   \"q2_final_plan.pt\")\n",
    "    print(\"Saved SAC weights!\")\n",
    "\n",
    "    return agent, all_steps, all_ep_returns\n",
    "\n",
    "\n",
    "def run_agent(agent, env_name=\"Pendulum-v1\", episodes=3, max_steps=200, deterministic=True):\n",
    "    env = gym.make(env_name, render_mode=\"human\")\n",
    "\n",
    "    for ep in range(episodes):\n",
    "        state, _ = env.reset()\n",
    "        ep_return = 0\n",
    "\n",
    "        print(f\"\\n=== Running Episode {ep+1}/{episodes} ===\")\n",
    "\n",
    "        for step in range(max_steps):\n",
    "            action = agent.select_action(state, deterministic=deterministic)\n",
    "            action = np.asarray(action, dtype=np.float32)\n",
    "\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            state = next_state\n",
    "\n",
    "            ep_return += reward\n",
    "            if terminated or truncated:\n",
    "                break\n",
    "\n",
    "        print(f\"Episode {ep+1} Return = {ep_return:.2f}\")\n",
    "\n",
    "    env.close()\n",
    "    print(\"\\nDone displaying agent!\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    trained_agent_SAC_w_planning, all_steps3, all_ep_returns3 = train_sac_with_planning()\n",
    "    run_agent(trained_agent_SAC_w_planning, env_name=\"Pendulum-v1\", episodes=3, deterministic=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "2ed996cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_evaluated_return(all_ep_returns):\n",
    "    avg_ret = []\n",
    "    steps = []\n",
    "    all_returns = []\n",
    "    mod = len(avg_ret)%10\n",
    "    all_ep_returns = all_ep_returns[:len(all_ep_returns)-mod]\n",
    "    for a in all_ep_returns:\n",
    "        all_returns.append(a[1])\n",
    "    \n",
    "    for r in range(len(all_ep_returns)):\n",
    "        avg_ret.append(np.mean(all_returns[r:r+10]))\n",
    "        steps.append(all_ep_returns[r][0])\n",
    "\n",
    "    plt.plot(steps, avg_ret)\n",
    "    plt.show\n",
    "\n",
    "# plot_evaluated_return(all_ep_returns)\n",
    "plot_evaluated_return(all_ep_returns2)\n",
    "plot_evaluated_return(all_ep_returns3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a956eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Normal\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pendulum = gym.make(\"Pendulum-v1\")\n",
    "acrobot = gym.make('Acrobot-v1', render_mode=\"rgb_array\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda7feb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, nO, nA, max_size):\n",
    "        self.obs = np.zeros((max_size, nO))\n",
    "        self.actions = np.zeros((max_size, nA))\n",
    "        self.rewards = np.zeros((max_size))\n",
    "        self.next_obs = np.zeros((max_size, nO))\n",
    "        self.done = np.zeros(max_size)\n",
    "\n",
    "        self.curr_size = 0\n",
    "        self.max_size = max_size\n",
    "        self.idx = 0\n",
    "\n",
    "    def store(self, o, a, r, next_o, done):\n",
    "        self.obs[self.idx] = o\n",
    "        self.actions[self.idx] = a\n",
    "        self.rewards[self.idx] = r\n",
    "        self.next_obs[self.idx] = next_o\n",
    "        self.done[self.idx] = done\n",
    "\n",
    "        self.idx = (self.idx + 1) % self.max_size\n",
    "        self.curr_size = min(self.curr_size + 1, self.max_size)\n",
    "\n",
    "    def sample_batch(self, batch_size):\n",
    "        batch_idxs = batch_idxs = np.random.randint(0, self.curr_size, size=batch_size)\n",
    "        batch = {\"obs\": self.obs[batch_idxs],\n",
    "                     \"actions\": self.actions[batch_idxs],\n",
    "                     \"rewards\": self.rewards[batch_idxs],\n",
    "                     \"next_obs\": self.next_obs[batch_idxs],\n",
    "                     \"done\": self.done[batch_idxs]\n",
    "        }\n",
    "        return batch\n",
    "\n",
    "def mlp(sizes, activation=nn.ReLU, output_activation=nn.Identity):\n",
    "    layers = []\n",
    "    for j in range(len(sizes) - 1):\n",
    "        act = activation if j < len(sizes) - 2 else output_activation\n",
    "        layers += [nn.Linear(sizes[j], sizes[j+1]), act()]\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "class GaussianPolicy(nn.Module):\n",
    "    def __init__(self, obs_dim, act_dim, hidden_sizes, action_limit):\n",
    "        super().__init__()\n",
    "        self.net = mlp([obs_dim] + hidden_sizes,\n",
    "                       activation=nn.ReLU,\n",
    "                       output_activation=nn.ReLU)\n",
    "        \n",
    "        self.mean_layer = nn.Linear(hidden_sizes[-1], act_dim)\n",
    "        self.log_std_layer = nn.Linear(hidden_sizes[-1], act_dim)\n",
    "\n",
    "        self.action_limit = action_limit\n",
    "        self.LOG_STD_MIN = -20\n",
    "        self.LOG_STD_MAX =  2\n",
    "\n",
    "    def forward(self, obs):\n",
    "        x = self.net(obs)\n",
    "        mean = self.mean_layer(x)\n",
    "        log_std = self.log_std_layer(x)\n",
    "        log_std = torch.clamp(log_std, self.LOG_STD_MIN, self.LOG_STD_MAX)\n",
    "        return mean, log_std.exp()\n",
    "\n",
    "    def sample(self, obs):\n",
    "        mean, std = self.forward(obs)\n",
    "        normal = Normal(mean, std)\n",
    "\n",
    "        z = normal.rsample()\n",
    "        a = torch.tanh(z)\n",
    "        action = self.action_limit * a\n",
    "\n",
    "        log_prob = normal.log_prob(z) - torch.log(1 - a.pow(2) + 1e-7)\n",
    "        log_prob = log_prob.sum(axis=-1, keepdim=True)\n",
    "\n",
    "        mean_action = self.action_limit * torch.tanh(mean)\n",
    "        return action, log_prob, mean_action\n",
    "\n",
    "\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, obs_dim, act_dim, hidden_sizes):\n",
    "        super().__init__()\n",
    "        self.q = mlp([obs_dim + act_dim] + hidden_sizes + [1],\n",
    "                     activation=nn.ReLU,\n",
    "                     output_activation=nn.Identity)\n",
    "\n",
    "    def forward(self, obs, act):\n",
    "        x = torch.cat([obs, act], dim=-1)\n",
    "        return self.q(x)\n",
    "    \n",
    "\n",
    "\n",
    "class KoopmanDynamics:\n",
    "    def __init__(self, env, lifter_fn, state_dim, act_dim):\n",
    "    \n",
    "        if env.spec.id.startswith(\"Pendulum\"):\n",
    "            self.env_name = \"Pendulum\"\n",
    "        elif env.spec.id.startswith(\"Acrobot\"):\n",
    "            self.env_name = \"Acrobot\"\n",
    "        elif env.spec.id.startswith(\"CartPole\"):\n",
    "            self.env_name = \"CartPole\"\n",
    "\n",
    "        self.lifter_fn = lifter_fn\n",
    "        self.state_dim = state_dim\n",
    "        self.act_dim = act_dim\n",
    "        self.K = None\n",
    "        self.B = None\n",
    "\n",
    "\n",
    "    def lift(self, s):\n",
    "\n",
    "        if self.env_name == \"Pendulum\":\n",
    "            cs, sn, w = s\n",
    "            return [cs, sn, w, cs*w, sn*w, cs**2, sn**2, w**2]\n",
    "        \n",
    "        elif self.env_name == \"Acrobot\":\n",
    "            cs1, sn1, cs2, sn2, w1, w2 = s\n",
    "            return [cs1, sn1, cs2, sn2, w1, w2,\n",
    "                    cs1*w1, cs1*w2, cs1*sn1, cs1*sn2, cs1*cs2, cs1*cs1,\n",
    "                    sn1*w1, sn1*w2, sn1*sn1, sn1*sn2, sn1*cs2, sn1*cs1,\n",
    "                    cs2*w1, cs2*w2, cs2*sn1, cs2*sn2, cs2*cs2, cs2*cs1,\n",
    "                    sn2*w1, sn2*w2, sn2*sn2, sn2*sn2, sn2*cs2, sn2*cs1,\n",
    "                    w1*w1, w1*w2, w1*sn1, w1*sn2, w1*cs2, w1*cs1,\n",
    "                    w2*w1, w2*w2, w2*sn1, w2*sn2, w2*cs2, w2*cs1]\n",
    "\n",
    "        elif self.env_name == \"CartPole\":\n",
    "            x, dx, th, dth = s\n",
    "            return [\n",
    "                x, dx, th, dth,\n",
    "                np.sin(th), np.cos(th),\n",
    "                x*th, dx*dth,\n",
    "                th**2, dth**2\n",
    "            ]\n",
    "\n",
    "\n",
    "    def fit(self, states, actions, next_states):\n",
    "        \"\"\"\n",
    "        Fit Koopman operator from a batch of transitions.\n",
    "        states:      (N, state_dim)\n",
    "        actions:     (N, act_dim)\n",
    "        next_states: (N, state_dim)\n",
    "        \"\"\"\n",
    "        N = states.shape[0]\n",
    "\n",
    "        # ---- Lift states ---- #\n",
    "        Z = np.array([self.lift(s) for s in states])         # (N, lift_dim)\n",
    "        Zp = np.array([self.lift(sp) for sp in next_states]) # (N, lift_dim)\n",
    "        U = actions                                          # (N, act_dim)\n",
    "\n",
    "        # Transpose to match EDMD math:\n",
    "        # Z:  (lift_dim, N)\n",
    "        # Zp: (lift_dim, N)\n",
    "        # U:  (act_dim, N)\n",
    "        Z  = Z.T\n",
    "        Zp = Zp.T\n",
    "        U  = U.T\n",
    "\n",
    "        lift_dim = Z.shape[0]\n",
    "\n",
    "        # ---- Build regression matrix ---- #\n",
    "        XU = np.vstack([Z, U])   # shape (lift_dim + act_dim, N)\n",
    "\n",
    "        # ---- Solve [K B] = Zp * pinv([Z;U]) ---- #\n",
    "        A = Zp @ np.linalg.pinv(XU)\n",
    "\n",
    "        self.K = A[:, :lift_dim]\n",
    "        self.B = A[:, lift_dim : lift_dim + self.act_dim]\n",
    "\n",
    "        return self.K, self.B\n",
    "\n",
    "    def predict_lifted(self, s, a):\n",
    "        \"\"\"Predict z_{t+1} in lifted space.\"\"\"\n",
    "        z = self.lift(s)\n",
    "        return self.K @ z + self.B @ a\n",
    "\n",
    "    def decode(self, z):\n",
    "        \"\"\"Decode z back to the original state (assumes first state_dim entries).\"\"\"\n",
    "        return z[:self.state_dim]\n",
    "\n",
    "    def predict_state(self, s, a):\n",
    "        \"\"\"Predict next original state.\"\"\"\n",
    "        z_next = self.predict_lifted(s, a)\n",
    "        return self.decode(z_next)\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "class SACAgent:\n",
    "    def __init__(self, obs_dim, act_dim, action_limit,\n",
    "                 gamma=0.99, tau=0.005, alpha=0.2,\n",
    "                 actor_lr=3e-4, critic_lr=3e-4,\n",
    "                 hidden_sizes=[256, 256]):\n",
    "\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.obs_dim = obs_dim\n",
    "        self.act_dim = act_dim\n",
    "        self.action_limit = action_limit\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "        self.alpha = alpha   # entropy coefficient\n",
    "\n",
    "        # Actor\n",
    "        self.actor = GaussianPolicy(obs_dim, act_dim, hidden_sizes, action_limit).to(self.device)\n",
    "        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=actor_lr)\n",
    "\n",
    "        # Critics\n",
    "        self.q1 = QNetwork(obs_dim, act_dim, hidden_sizes).to(self.device)\n",
    "        self.q2 = QNetwork(obs_dim, act_dim, hidden_sizes).to(self.device)\n",
    "        self.q1_target = QNetwork(obs_dim, act_dim, hidden_sizes).to(self.device)\n",
    "        self.q2_target = QNetwork(obs_dim, act_dim, hidden_sizes).to(self.device)\n",
    "\n",
    "        self.q1_target.load_state_dict(self.q1.state_dict())\n",
    "        self.q2_target.load_state_dict(self.q2.state_dict())\n",
    "\n",
    "        self.q1_optimizer = optim.Adam(self.q1.parameters(), lr=critic_lr)\n",
    "        self.q2_optimizer = optim.Adam(self.q2.parameters(), lr=critic_lr)\n",
    "\n",
    "    def select_action(self, obs, deterministic=False):\n",
    "        obs = torch.as_tensor(obs, dtype=torch.float32).unsqueeze(0).to(self.device)\n",
    "        with torch.no_grad():\n",
    "            if deterministic:\n",
    "                _, _, action = self.actor.sample(obs)\n",
    "            else:\n",
    "                action, _, _ = self.actor.sample(obs)\n",
    "        return action.cpu().numpy()[0]\n",
    "\n",
    "    def update(self, batch):\n",
    "        obs = torch.as_tensor(batch['obs'], dtype=torch.float32, device=self.device)\n",
    "        acts = torch.as_tensor(batch['actions'], dtype=torch.float32, device=self.device)\n",
    "        rews = torch.as_tensor(batch['rewards'], dtype=torch.float32, device=self.device).unsqueeze(-1)\n",
    "        next_obs = torch.as_tensor(batch['next_obs'], dtype=torch.float32, device=self.device)\n",
    "        done = torch.as_tensor(batch['done'], dtype=torch.float32, device=self.device).unsqueeze(-1)\n",
    "\n",
    "        # -------- Target Q -------- #\n",
    "        with torch.no_grad():\n",
    "            next_action, next_log_prob, _ = self.actor.sample(next_obs)\n",
    "            q1_next = self.q1_target(next_obs, next_action)\n",
    "            q2_next = self.q2_target(next_obs, next_action)\n",
    "            q_target = torch.min(q1_next, q2_next) - self.alpha * next_log_prob\n",
    "            target_value = rews + self.gamma * (1 - done) * q_target\n",
    "\n",
    "        # -------- Update Q1 -------- #\n",
    "        q1 = self.q1(obs, acts)\n",
    "        q1_loss = F.mse_loss(q1, target_value)\n",
    "        self.q1_optimizer.zero_grad()\n",
    "        q1_loss.backward()\n",
    "        self.q1_optimizer.step()\n",
    "\n",
    "        # -------- Update Q2 -------- #\n",
    "        q2 = self.q2(obs, acts)\n",
    "        q2_loss = F.mse_loss(q2, target_value)\n",
    "        self.q2_optimizer.zero_grad()\n",
    "        q2_loss.backward()\n",
    "        self.q2_optimizer.step()\n",
    "\n",
    "        # -------- Update Actor -------- #\n",
    "        new_actions, log_prob, _ = self.actor.sample(obs)\n",
    "        q1_new = self.q1(obs, new_actions)\n",
    "        q2_new = self.q2(obs, new_actions)\n",
    "        q_new = torch.min(q1_new, q2_new)\n",
    "\n",
    "        actor_loss = (self.alpha * log_prob - q_new).mean()\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "\n",
    "        # -------- Soft update targets -------- #\n",
    "        with torch.no_grad():\n",
    "            for p, tp in zip(self.q1.parameters(), self.q1_target.parameters()):\n",
    "                tp.data.mul_(1 - self.tau)\n",
    "                tp.data.add_(self.tau * p.data)\n",
    "            for p, tp in zip(self.q2.parameters(), self.q2_target.parameters()):\n",
    "                tp.data.mul_(1 - self.tau)\n",
    "                tp.data.add_(self.tau * p.data)\n",
    "\n",
    "\n",
    "\n",
    "def koopman_reward(koopman_model, s, a):\n",
    "    \"\"\"\n",
    "    Approximate immediate reward r(s,a) using env-specific formula.\n",
    "    Currently implemented for Pendulum.\n",
    "    \"\"\"\n",
    "    if koopman_model.env_name == \"Pendulum\":\n",
    "        cs, sn, w = s\n",
    "        theta = np.arctan2(sn, cs)\n",
    "        u = float(a[0])\n",
    "        cost = theta**2 + 0.1 * (w**2) + 0.001 * (u**2)\n",
    "        return -cost\n",
    "    else:\n",
    "        # Fallback: no reward model  0\n",
    "        # (You can add Acrobot/CartPole versions later)\n",
    "        return 0.0\n",
    "\n",
    "\n",
    "def plan_action_with_model(state, agent, koopman_model, action_limit,\n",
    "                           H=5, N=64, iters=3, gamma=0.99, beta=0.7):\n",
    "    \"\"\"\n",
    "    Koopman-MPC style planner:\n",
    "    - state: np array (obs_dim,)\n",
    "    - agent: SACAgent\n",
    "    - koopman_model: KoopmanDynamics (with K,B already fitted)\n",
    "    Returns: np array action (act_dim,)\n",
    "    \"\"\"\n",
    "\n",
    "    # If Koopman not ready, just use SAC\n",
    "    if koopman_model.K is None or koopman_model.B is None:\n",
    "        return agent.select_action(state, deterministic=False)\n",
    "\n",
    "    device = agent.device\n",
    "    obs_dim = agent.obs_dim\n",
    "    act_dim = agent.act_dim\n",
    "\n",
    "    # Initial lifted state\n",
    "    z0 = np.asarray(koopman_model.lift(state), dtype=np.float32)\n",
    "    lift_dim = z0.shape[0]\n",
    "\n",
    "    # Get actor's mean action as prior (PyTorch  numpy)\n",
    "    with torch.no_grad():\n",
    "        s0_t = torch.as_tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "        mean_action, _, _ = agent.actor.sample(s0_t)\n",
    "        mean_action = mean_action.squeeze(0).cpu().numpy()  # (act_dim,)\n",
    "\n",
    "    # Init mean & std of action sequence: (H, act_dim)\n",
    "    seq_mean = np.tile(mean_action[None, :], (H, 1))              # (H, act_dim)\n",
    "    seq_std  = 0.5 * action_limit * np.ones_like(seq_mean)        # (H, act_dim)\n",
    "\n",
    "    for _ in range(iters):\n",
    "        # Sample N action sequences: (N, H, act_dim)\n",
    "        eps = np.random.randn(N, H, act_dim).astype(np.float32)\n",
    "        actions = seq_mean[None, :, :] + seq_std[None, :, :] * eps\n",
    "        actions = np.clip(actions, -action_limit, action_limit)\n",
    "\n",
    "        # Roll out each sequence using Koopman\n",
    "        returns = np.zeros((N,), dtype=np.float32)\n",
    "\n",
    "        for i in range(N):\n",
    "            z = z0.copy()\n",
    "            s = state.copy()\n",
    "            G = 0.0\n",
    "            discount = 1.0\n",
    "\n",
    "            for t in range(H):\n",
    "                a_t = actions[i, t, :]  # (act_dim,)\n",
    "                r_t = koopman_reward(koopman_model, s, a_t)\n",
    "                G += discount * r_t\n",
    "\n",
    "                # Koopman one-step in z-space\n",
    "                z = koopman_model.K @ z + koopman_model.B @ a_t\n",
    "                # Decode to state (first state_dim components)\n",
    "                s = koopman_model.decode(z)\n",
    "\n",
    "                discount *= gamma\n",
    "\n",
    "            # Bootstrap with SAC critic at s_H\n",
    "            with torch.no_grad():\n",
    "                s_t = torch.as_tensor(s, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "                a_boot, _, _ = agent.actor.sample(s_t)\n",
    "                q1_H = agent.q1(s_t, a_boot)\n",
    "                q2_H = agent.q2(s_t, a_boot)\n",
    "                q_H = torch.min(q1_H, q2_H).item()\n",
    "            G += discount * q_H\n",
    "\n",
    "            returns[i] = G\n",
    "\n",
    "        # Softmax weights over sequences (CEM-like)\n",
    "        returns_max = np.max(returns)\n",
    "        scores = returns - returns_max\n",
    "        weights = np.exp(beta * scores)\n",
    "        weights /= np.sum(weights) + 1e-8    # (N,)\n",
    "\n",
    "        # Update mean and std per time-step\n",
    "        # actions: (N, H, act_dim), weights: (N,)\n",
    "        w = weights[:, None, None]           # (N, 1, 1)\n",
    "        new_mean = np.sum(w * actions, axis=0)                 # (H, act_dim)\n",
    "        diff = actions - new_mean[None, :, :]\n",
    "        new_std = np.sqrt(np.sum(w * diff**2, axis=0) + 1e-6)  # (H, act_dim)\n",
    "\n",
    "        seq_mean = new_mean\n",
    "        seq_std  = new_std\n",
    "\n",
    "    # Use mean action at time t=0\n",
    "    a0 = seq_mean[0]\n",
    "    return a0.astype(np.float32)\n",
    "\n",
    "\n",
    "def train_sac_with_planning(env):\n",
    "    # env = gym.make(\"Pendulum-v1\")\n",
    "    obs_dim = env.observation_space.shape[0]\n",
    "    act_dim = env.action_space.shape[0]\n",
    "    action_limit = float(env.action_space.high[0])\n",
    "\n",
    "    # Agents + replay\n",
    "    agent = SACAgent(obs_dim, act_dim, action_limit)\n",
    "    replay_buffer = ReplayBuffer(obs_dim, act_dim, max_size=200000)\n",
    "\n",
    "    # Koopman model\n",
    "    koopman_model = KoopmanDynamics(env, lifter_fn=None,\n",
    "                                    state_dim=obs_dim, act_dim=act_dim)\n",
    "\n",
    "    # ==== Hyperparameters ====\n",
    "    MAX_EPISODES = 3        # <---- STOP after this many episodes\n",
    "    max_ep_len   = 200        # max length of each episode\n",
    "    start_steps  = 1000       # random exploration steps\n",
    "    update_after = 1000       # start updating SAC after this many steps total\n",
    "    update_every = 50         # update SAC every X steps\n",
    "    batch_size   = 256\n",
    "\n",
    "    koopman_train_every = 5   # <---- fit Koopman every 5 episodes\n",
    "    koopman_max_samples = 5000\n",
    "\n",
    "    total_steps = 0\n",
    "    all_steps = 0\n",
    "    all_ep_returns = []\n",
    "\n",
    "    # ========== EPISODE LOOP (MAIN CHANGE) ==========\n",
    "    for episode in range(1, MAX_EPISODES + 1):\n",
    "\n",
    "        state, _ = env.reset()\n",
    "        ep_return = 0.0\n",
    "        ep_len = 0\n",
    "\n",
    "        for t in range(max_ep_len):\n",
    "            # ---- ACTION SELECTION ----\n",
    "            if total_steps < start_steps or koopman_model.K is None:\n",
    "                if total_steps < start_steps:\n",
    "                    action = env.action_space.sample()\n",
    "                else:\n",
    "                    action = agent.select_action(state, deterministic=False)\n",
    "            else:\n",
    "                action = plan_action_with_model(\n",
    "                    state, agent, koopman_model, action_limit,\n",
    "                    H=5, N=64, iters=3, gamma=agent.gamma, beta=0.7\n",
    "                )\n",
    "\n",
    "            action = np.asarray(action, dtype=np.float32)\n",
    "\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "\n",
    "            # ---- Store in replay ----\n",
    "            replay_buffer.store(state, action, reward, next_state, float(done))\n",
    "\n",
    "            state = next_state\n",
    "            ep_return += reward\n",
    "            ep_len += 1\n",
    "            total_steps += 1\n",
    "\n",
    "            # ---- SAC updates ----\n",
    "            if total_steps >= update_after and total_steps % update_every == 0:\n",
    "                for _ in range(update_every):\n",
    "                    batch = replay_buffer.sample_batch(batch_size)\n",
    "                    agent.update(batch)\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        # ===== END EPISODE =====\n",
    "        print(f\"Episode {episode} | Return {ep_return:.2f} | Length {ep_len}\")\n",
    "        all_steps += ep_len\n",
    "        all_ep_returns.append((all_steps, ep_return))\n",
    "\n",
    "        # ===== FIT KOOPMAN EVERY X EPISODES =====\n",
    "        if episode % koopman_train_every == 0 and replay_buffer.curr_size > 1000:\n",
    "            N = min(replay_buffer.curr_size, koopman_max_samples)\n",
    "            idxs = np.random.choice(replay_buffer.curr_size, size=N, replace=False)\n",
    "            states = replay_buffer.obs[idxs]\n",
    "            actions = replay_buffer.actions[idxs]\n",
    "            next_states = replay_buffer.next_obs[idxs]\n",
    "            K, B = koopman_model.fit(states, actions, next_states)\n",
    "            print(f\"[Episode {episode}] Fitted Koopman: K {K.shape}, B {B.shape}\")\n",
    "\n",
    "    env.close()\n",
    "    print(\"Training complete (episode-based).\")\n",
    "\n",
    "    # Save trained SAC policy\n",
    "    torch.save(agent.actor.state_dict(), \"actor_final_plan.pt\")\n",
    "    torch.save(agent.q1.state_dict(),   \"q1_final_plan_q1.pt\")\n",
    "    torch.save(agent.q2.state_dict(),   \"q2_final_plan_q2.pt\")\n",
    "    print(\"Saved SAC weights!\")\n",
    "\n",
    "    return agent, koopman_model, all_steps, all_ep_returns\n",
    "\n",
    "\n",
    "def run_agent(agent, env_name=\"Pendulum-v1\", episodes=3, max_steps=200, deterministic=True):\n",
    "    env = gym.make(env_name, render_mode=\"human\")\n",
    "\n",
    "    for ep in range(episodes):\n",
    "        state, _ = env.reset()\n",
    "        ep_return = 0\n",
    "\n",
    "        print(f\"\\n=== Running Episode {ep+1}/{episodes} ===\")\n",
    "\n",
    "        for step in range(max_steps):\n",
    "            action = agent.select_action(state, deterministic=deterministic)\n",
    "            action = np.asarray(action, dtype=np.float32)\n",
    "\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            state = next_state\n",
    "\n",
    "            ep_return += reward\n",
    "            if terminated or truncated:\n",
    "                break\n",
    "\n",
    "        print(f\"Episode {ep+1} Return = {ep_return:.2f}\")\n",
    "\n",
    "    env.close()\n",
    "    print(\"\\nDone displaying agent!\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(1)\n",
    "    # agent, koopman_model, all_steps3, all_ep_returns3 = train_sac_with_planning(pendulum)\n",
    "    # run_agent(agent, env_name=\"Pendulum-v1\", episodes=3, deterministic=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27401e2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "978958fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RL_ENV (3.10.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
